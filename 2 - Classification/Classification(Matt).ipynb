{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# this import allows you train and test you test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# this import allows you to standardize your data, scaling so that all features have a mean of zero and a standard deviation of 1. \n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "# this import allows you to create a logistic regression model; type of machine learning model that can be used for classification tasks \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# this import allows you to create a support vector machine SVM model, a type of ML model that can be used for classification tasks. \n",
    "from sklearn.svm import SVC\n",
    "# this import allows you to perform CV on your model, a technique for evaluating the performance of a ML on unseen data\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# these imports allow you to calculate various evaluation metrics for your ML model. Eval metrics are used to asses the performance of a ML on held-out test set. \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "# for testing differences with 95% confidence\n",
    "from scipy.stats import ttest_rel\n",
    "# for RandomForest models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# for KNN models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# # for dimensionality reduction using PCA\n",
    "# from sklearn.decomposition import PCA\n",
    "# for feature selection\n",
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_name</th>\n",
       "      <th>date</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>news_category</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>log_shares</th>\n",
       "      <th>log_n_tokens_content</th>\n",
       "      <th>log_num_hrefs</th>\n",
       "      <th>log_num_self_hrefs</th>\n",
       "      <th>log_num_imgs</th>\n",
       "      <th>log_num_videos</th>\n",
       "      <th>log_kw_max_min</th>\n",
       "      <th>log_kw_min_max</th>\n",
       "      <th>log_kw_avg_avg</th>\n",
       "      <th>log_self_reference_min_shares</th>\n",
       "      <th>log_self_reference_max_shares</th>\n",
       "      <th>log_self_reference_avg_sharess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon-instant-video-browser/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>593</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.386879</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reeddit-reddit/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>4.546154</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.022276</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.681548</td>\n",
       "      <td>0.381987</td>\n",
       "      <td>0.152189</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.353939</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>1300</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rage-comics-dying/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>4.759494</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.199626</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.714611</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.542580</td>\n",
       "      <td>0.122370</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357269</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.338889</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1100</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.003974</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>power-matters-alliance-organization/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.535390</td>\n",
       "      <td>5.147748</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.117255</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>0.425089</td>\n",
       "      <td>0.128515</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.337965</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.225794</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1600</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.378384</td>\n",
       "      <td>6.320768</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>polaroid-android-camera/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.424132</td>\n",
       "      <td>4.631390</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.327017</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>0.506520</td>\n",
       "      <td>0.279769</td>\n",
       "      <td>0.071749</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.417055</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.212354</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2400</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.783641</td>\n",
       "      <td>7.017506</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.302619</td>\n",
       "      <td>9.680406</td>\n",
       "      <td>8.140199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               url_name        date  timedelta  \\\n",
       "0         amazon-instant-video-browser/  2013-01-07      731.0   \n",
       "1                       reeddit-reddit/  2013-01-07      731.0   \n",
       "2                    rage-comics-dying/  2013-01-07      731.0   \n",
       "3  power-matters-alliance-organization/  2013-01-07      731.0   \n",
       "4              polaroid-android-camera/  2013-01-07      731.0   \n",
       "\n",
       "   n_tokens_title  n_unique_tokens  average_token_length  num_keywords  \\\n",
       "0            12.0         0.663594              4.680365           5.0   \n",
       "1             8.0         0.821705              4.546154           9.0   \n",
       "2             9.0         0.608602              4.759494           7.0   \n",
       "3            10.0         0.535390              5.147748          10.0   \n",
       "4             9.0         0.424132              4.631390           8.0   \n",
       "\n",
       "   kw_min_min  kw_avg_min  kw_max_max  kw_avg_max  kw_min_avg  kw_max_avg  \\\n",
       "0         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   is_weekend    LDA_00    LDA_01    LDA_02    LDA_03    LDA_04  \\\n",
       "0         0.0  0.500331  0.378279  0.040005  0.041263  0.040123   \n",
       "1         0.0  0.022265  0.022446  0.022276  0.251465  0.681548   \n",
       "2         0.0  0.028575  0.199626  0.028615  0.714611  0.028572   \n",
       "3         0.0  0.020011  0.020317  0.117255  0.020007  0.822410   \n",
       "4         0.0  0.025001  0.327017  0.025001  0.025001  0.597981   \n",
       "\n",
       "   global_subjectivity  global_sentiment_polarity  global_rate_positive_words  \\\n",
       "0             0.521617                   0.092562                    0.045662   \n",
       "1             0.381987                   0.152189                    0.038462   \n",
       "2             0.542580                   0.122370                    0.063291   \n",
       "3             0.425089                   0.128515                    0.039640   \n",
       "4             0.506520                   0.279769                    0.071749   \n",
       "\n",
       "   global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "0                    0.013699             0.769231             0.230769   \n",
       "1                    0.007692             0.833333             0.166667   \n",
       "2                    0.025316             0.714286             0.285714   \n",
       "3                    0.012613             0.758621             0.241379   \n",
       "4                    0.013453             0.842105             0.157895   \n",
       "\n",
       "   avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "0               0.378636               0.100000                    0.7   \n",
       "1               0.353939               0.033333                    0.7   \n",
       "2               0.357269               0.050000                    0.6   \n",
       "3               0.337965               0.050000                    0.7   \n",
       "4               0.417055               0.100000                    1.0   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.350000                   -0.6                 -0.200   \n",
       "1              -0.400000                   -0.4                 -0.400   \n",
       "2              -0.338889                   -1.0                 -0.050   \n",
       "3              -0.225794                   -0.4                 -0.125   \n",
       "4              -0.212354                   -0.5                 -0.050   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.500000                   -0.1875                0.000000   \n",
       "1            0.250000                    0.2000                0.250000   \n",
       "2            0.650000                   -0.5000                0.150000   \n",
       "3            0.500000                   -0.1000                0.000000   \n",
       "4            0.333333                    0.2500                0.166667   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares day_of_week  news_category  year  \\\n",
       "0                        0.1875     593      Monday  Entertainment  2013   \n",
       "1                        0.2000    1300      Monday           Tech  2013   \n",
       "2                        0.5000    1100      Monday  Uncategorized  2013   \n",
       "3                        0.1000    1600      Monday           Tech  2013   \n",
       "4                        0.2500    2400      Monday           Tech  2013   \n",
       "\n",
       "   month  log_shares  log_n_tokens_content  log_num_hrefs  log_num_self_hrefs  \\\n",
       "0      1    6.386879              5.393628       1.609438            1.098612   \n",
       "1      1    7.170888              4.875197       2.079442            1.609438   \n",
       "2      1    7.003974              6.163315       2.484907            0.000000   \n",
       "3      1    7.378384              6.320768       2.079442            1.945910   \n",
       "4      1    7.783641              7.017506       3.091042            3.091042   \n",
       "\n",
       "   log_num_imgs  log_num_videos  log_kw_max_min  log_kw_min_max  \\\n",
       "0      0.693147             0.0             0.0             0.0   \n",
       "1      0.000000             0.0             0.0             0.0   \n",
       "2      0.693147             0.0             0.0             0.0   \n",
       "3      0.693147             0.0             0.0             0.0   \n",
       "4      3.044522             0.0             0.0             0.0   \n",
       "\n",
       "   log_kw_avg_avg  log_self_reference_min_shares  \\\n",
       "0             0.0                       6.208590   \n",
       "1             0.0                       7.170888   \n",
       "2             0.0                       0.000000   \n",
       "3             0.0                       7.550135   \n",
       "4             0.0                       6.302619   \n",
       "\n",
       "   log_self_reference_max_shares  log_self_reference_avg_sharess  \n",
       "0                       6.208590                        6.208590  \n",
       "1                       7.170888                        7.170888  \n",
       "2                       0.000000                        0.000000  \n",
       "3                       7.550135                        7.550135  \n",
       "4                       9.680406                        8.140199  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file path\n",
    "file_path = \"../1 - Visualization and Data Preprocessing/Data/ONPClean2.csv\" # previously cleaned\n",
    "# file_path = '../1 - Visualization and Data Preprocessing/Data/OnlineNewsPopularity.csv' # unclean\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Set the maximum number of columns to display to None\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation Part 1 [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and prepare your class variables.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use proper variable representations (int, float, one-hot, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39644 entries, 0 to 39643\n",
      "Data columns (total 52 columns):\n",
      " #   Column                          Non-Null Count  Dtype  \n",
      "---  ------                          --------------  -----  \n",
      " 0   url_name                        39644 non-null  object \n",
      " 1   date                            39644 non-null  object \n",
      " 2   timedelta                       39644 non-null  float64\n",
      " 3   n_tokens_title                  39644 non-null  float64\n",
      " 4   n_unique_tokens                 39644 non-null  float64\n",
      " 5   average_token_length            39644 non-null  float64\n",
      " 6   num_keywords                    39644 non-null  float64\n",
      " 7   kw_min_min                      39644 non-null  float64\n",
      " 8   kw_avg_min                      39644 non-null  float64\n",
      " 9   kw_max_max                      39644 non-null  float64\n",
      " 10  kw_avg_max                      39644 non-null  float64\n",
      " 11  kw_min_avg                      39644 non-null  float64\n",
      " 12  kw_max_avg                      39644 non-null  float64\n",
      " 13  is_weekend                      39644 non-null  float64\n",
      " 14  LDA_00                          39644 non-null  float64\n",
      " 15  LDA_01                          39644 non-null  float64\n",
      " 16  LDA_02                          39644 non-null  float64\n",
      " 17  LDA_03                          39644 non-null  float64\n",
      " 18  LDA_04                          39644 non-null  float64\n",
      " 19  global_subjectivity             39644 non-null  float64\n",
      " 20  global_sentiment_polarity       39644 non-null  float64\n",
      " 21  global_rate_positive_words      39644 non-null  float64\n",
      " 22  global_rate_negative_words      39644 non-null  float64\n",
      " 23  rate_positive_words             39644 non-null  float64\n",
      " 24  rate_negative_words             39644 non-null  float64\n",
      " 25  avg_positive_polarity           39644 non-null  float64\n",
      " 26  min_positive_polarity           39644 non-null  float64\n",
      " 27  max_positive_polarity           39644 non-null  float64\n",
      " 28  avg_negative_polarity           39644 non-null  float64\n",
      " 29  min_negative_polarity           39644 non-null  float64\n",
      " 30  max_negative_polarity           39644 non-null  float64\n",
      " 31  title_subjectivity              39644 non-null  float64\n",
      " 32  title_sentiment_polarity        39644 non-null  float64\n",
      " 33  abs_title_subjectivity          39644 non-null  float64\n",
      " 34  abs_title_sentiment_polarity    39644 non-null  float64\n",
      " 35  shares                          39644 non-null  int64  \n",
      " 36  day_of_week                     39644 non-null  object \n",
      " 37  news_category                   39644 non-null  object \n",
      " 38  year                            39644 non-null  int64  \n",
      " 39  month                           39644 non-null  int64  \n",
      " 40  log_shares                      39644 non-null  float64\n",
      " 41  log_n_tokens_content            39644 non-null  float64\n",
      " 42  log_num_hrefs                   39644 non-null  float64\n",
      " 43  log_num_self_hrefs              39644 non-null  float64\n",
      " 44  log_num_imgs                    39644 non-null  float64\n",
      " 45  log_num_videos                  39644 non-null  float64\n",
      " 46  log_kw_max_min                  39644 non-null  float64\n",
      " 47  log_kw_min_max                  39644 non-null  float64\n",
      " 48  log_kw_avg_avg                  39644 non-null  float64\n",
      " 49  log_self_reference_min_shares   39644 non-null  float64\n",
      " 50  log_self_reference_max_shares   39644 non-null  float64\n",
      " 51  log_self_reference_avg_sharess  39644 non-null  float64\n",
      "dtypes: float64(45), int64(3), object(4)\n",
      "memory usage: 15.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original (before any cleaning):\n",
    "url:        \n",
    "    Containes the url of the article with the date      \n",
    "    Object\n",
    "\n",
    "timedelta:               \n",
    "    Days between the article publication and the dataset acquisition (non-predictive)               \n",
    "    float64\n",
    "\n",
    "n_tokens_title:               \n",
    "    Number of words in the title               \n",
    "    float64\n",
    "\n",
    "n_tokens_content:               \n",
    "    Number of words in the content               \n",
    "    float64\n",
    "\n",
    "n_unique_tokens:               \n",
    "    Rate of unique words in the content               \n",
    "    float64\n",
    "\n",
    "n_non_stop_words:           \n",
    "    Rate of non-stop words in the content           \n",
    "    float64\n",
    "\n",
    "n_non_stop_unique_tokens:      \n",
    "    Rate of unique non-stop words in the content      \n",
    "    float64\n",
    "\n",
    "num_hrefs:                    \n",
    "    Number of links                 \n",
    "    float64\n",
    "\n",
    "num_self_hrefs:               \n",
    "    Number of links to other articles published by Mashable            \n",
    "    float64\n",
    "\n",
    "num_imgs:                      \n",
    "    Number of images        \n",
    "    float64\n",
    "\n",
    "num_videos:                    \n",
    "    Number of videos            \n",
    "    float64\n",
    "    \n",
    "average_token_length:               \n",
    "    Average length of the words in the content               \n",
    "    Float64\n",
    "\n",
    "num_keywords:               \n",
    "    Number of keywords in the metadata               \n",
    "    float64\n",
    "\n",
    "data_channel_is_lifestyle:     \n",
    "    Is data channel 'Lifestyle'?            \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "data_channel_is_entertainment:          \n",
    "    Is data channel 'Entertainment'?            \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "data_channel_is_bus:           \n",
    "    Is data channel 'Business'?         \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "  \n",
    "data_channel_is_socmed:        \n",
    "    Is data channel 'Social Media'?             \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "   \n",
    "data_channel_is_tech:          \n",
    "    Is data channel 'Tech'?             \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    " \n",
    "data_channel_is_world:         \n",
    "    Is data channel 'World'?        \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    " \n",
    "kw_min_min:               \n",
    "    Worst keyword (min. shares)               \n",
    "    float64\n",
    "\n",
    "kw_max_min:                    \n",
    "    Worst keyword (max. shares)         \n",
    "    float64\n",
    "\n",
    "kw_avg_min:                    \n",
    "    Worst keyword (avg. shares)               \n",
    "    float64\n",
    "\n",
    "kw_min_max:                    \n",
    "    Best keyword (min. shares)          \n",
    "    float64\n",
    "\n",
    "kw_max_max:                    \n",
    "    Best keyword (max. shares)               \n",
    "    float64\n",
    "\n",
    "kw_avg_max:                    \n",
    "    Best keyword (avg. shares)               \n",
    "    float64\n",
    "\n",
    "kw_min_avg:                    \n",
    "    Avg. keyword (min. shares)               \n",
    "    float64\n",
    "\n",
    "kw_max_avg:                    \n",
    "    Avg. keyword (max. shares)               \n",
    "    float64\n",
    "\n",
    "kw_avg_avg:                    \n",
    "    Avg. keyword (avg. shares)          \n",
    "    float64\n",
    "\n",
    "self_reference_min_shares:    \n",
    "    Min. shares of referenced articles in Mashable          \n",
    "    float64\n",
    "\n",
    "self_reference_max_shares:     \n",
    "    Max. shares of referenced articles in Mashable          \n",
    "    float64\n",
    "\n",
    "self_reference_avg_sharess:   \n",
    "    Avg. shares of referenced articles in Mashable          \n",
    "    float64\n",
    "\n",
    "weekday_is_monday:             \n",
    "    Was the article published on a Monday?          \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "weekday_is_tuesday:            \n",
    "    Was the article published on a Tuesday?             \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "weekday_is_wednesday:          \n",
    "    Was the article published on a Wednesday?               \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "weekday_is_thursday:           \n",
    "    Was the article published on a Thursday?            \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "weekday_is_friday:             \n",
    "    Was the article published on a Friday?          \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "weekday_is_saturday:           \n",
    "    Was the article published on a Saturday?            \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "weekday_is_sunday:              \n",
    "    Was the article published on a Sunday?          \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "is_weekend:                    \n",
    "    Was the article published on the weekend?               \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "LDA_00:                        \n",
    "    LDA topic modeling \n",
    "    Closeness to LDA topic 0               \n",
    "    float64\n",
    "\n",
    "LDA_01:                       \n",
    "    Closeness to LDA topic 1               \n",
    "    float64\n",
    "\n",
    "LDA_02:                        \n",
    "    Closeness to LDA topic 2               \n",
    "    float64\n",
    "\n",
    "LDA_03:                       \n",
    "    Closeness to LDA topic 3               \n",
    "    float64\n",
    "\n",
    "LDA_04:                        \n",
    "    Closeness to LDA topic 4               \n",
    "    float64\n",
    "\n",
    "global_subjectivity:           \n",
    "    Text subjectivity               \n",
    "    float64\n",
    "\n",
    "global_sentiment_polarity:     \n",
    "    Text sentiment polarity               \n",
    "    float64\n",
    "\n",
    "global_rate_positive_words:    \n",
    "    Rate of positive words in the content               \n",
    "    float64\n",
    "\n",
    "global_rate_negative_words:    \n",
    "    Rate of negative words in the content               \n",
    "    float64\n",
    "\n",
    "rate_positive_words:           \n",
    "    Rate of positive words among non-neutral tokens               \n",
    "    float64\n",
    "\n",
    "rate_negative_words:           \n",
    "    Rate of negative words among non-neutral tokens               \n",
    "    float64\n",
    "\n",
    "avg_positive_polarity:         \n",
    "    Avg. polarity of positive words               \n",
    "    float64\n",
    "\n",
    "min_positive_polarity:         \n",
    "    Min. polarity of positive words               \n",
    "    float64\n",
    "\n",
    "max_positive_polarity:         \n",
    "    Max. polarity of positive words               \n",
    "    float64\n",
    "\n",
    "avg_negative_polarity:         \n",
    "    Avg. polarity of negative  words               \n",
    "    float64\n",
    "\n",
    "min_negative_polarity:         \n",
    "    Min. polarity of negative  words               \n",
    "    float64\n",
    "\n",
    "max_negative_polarity:         \n",
    "    Max. polarity of negative  words               \n",
    "    float64\n",
    "\n",
    "title_subjectivity:            \n",
    "    Title subjectivity               \n",
    "    float64\n",
    "\n",
    "title_sentiment_polarity:      \n",
    "    Title polarity               \n",
    "    float64\n",
    "\n",
    "abs_title_subjectivity:        \n",
    "    Absolute subjectivity level               \n",
    "    float64\n",
    "\n",
    "abs_title_sentiment_polarity:  \n",
    "    Absolute polarity level               \n",
    "    float64\n",
    "\n",
    "shares:                        \n",
    "    Number of shares (target)               \n",
    "    Integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Newly Created (from after preeviously done cleaning & any transformations):\n",
    "url_name:               \n",
    "    URL of the article (non-predictive)               \n",
    "    Float\n",
    "\n",
    "Date:               \n",
    "    The date the article was published               \n",
    "    DateTime\n",
    "\n",
    "Day_of_week:               \n",
    "    What day of the week the article is posted on               \n",
    "    Categorical\n",
    "\n",
    "news_category:               \n",
    "    What news category the article is               \n",
    "    Categorical\n",
    "\n",
    "Year:               \n",
    "    The year the article was published               \n",
    "    Integer\n",
    "\n",
    "Month:               \n",
    "    The month the aticle was published               \n",
    "    Integer\n",
    "\n",
    "log_shares:               \n",
    "    log of the \"shares\" variable               \n",
    "    Float\n",
    "\n",
    "log_n_tokens_content:               \n",
    "    log of the \"n_tokens_content\" variable               \n",
    "    Float\n",
    "\n",
    "log_num_hrefs:               \n",
    "    log of the \"num_hrefs\" variable               \n",
    "    Float\n",
    "\n",
    "log_num_self_hrefs:               \n",
    "    log of the \"num_self_hrefs\" variable               \n",
    "    Float\n",
    "\n",
    "log_num_imgs:               \n",
    "    log of the \"num_imgs\" variable               \n",
    "    Float\n",
    "\n",
    "log_num_videos:               \n",
    "    log of the \"num_videos\" variable               \n",
    "    Float\n",
    "\n",
    "log_kw_max_min:               \n",
    "    log of the \"kw_max_min\" variable               \n",
    "    Float\n",
    "\n",
    "log_kw_min_max:               \n",
    "    log of the \"kw_min_max\" variable               \n",
    "    Float\n",
    "\n",
    "log_kw_avg_avg:               \n",
    "    log of the \"kw_avg_avg\" variable               \n",
    "    Float\n",
    "\n",
    "log_self_reference_min_shares:               \n",
    "    log of the \"self_reference_min_shares\" variable               \n",
    "    Float\n",
    "\n",
    "log_self_reference_max_shares:               \n",
    "    log of the \"self_reference_max_shares\" variable               \n",
    "    Float\n",
    "\n",
    "log_self_reference_avg_shares:               \n",
    "    log of the \"self_reference_avg_shares\" variable               \n",
    "    Float\n",
    "\n",
    "day_of_weekX where X is the day of the week\n",
    "    a binary value meaning either Yes (1) it is day X or No (0) it is not day x\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your dataset has outliers, you may want to remove them before training your model, as outliers can skew the results of the model.\n",
    "\n",
    "If your dataset has categorical variables, you need to encode them before training your model, as most machine learning algorithms can only handle numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>log_n_tokens_content</th>\n",
       "      <th>log_num_hrefs</th>\n",
       "      <th>log_num_self_hrefs</th>\n",
       "      <th>log_num_imgs</th>\n",
       "      <th>log_num_videos</th>\n",
       "      <th>log_kw_max_min</th>\n",
       "      <th>log_kw_min_max</th>\n",
       "      <th>log_kw_avg_avg</th>\n",
       "      <th>log_self_reference_min_shares</th>\n",
       "      <th>log_self_reference_max_shares</th>\n",
       "      <th>log_self_reference_avg_sharess</th>\n",
       "      <th>news_category_Business</th>\n",
       "      <th>news_category_Entertainment</th>\n",
       "      <th>news_category_Lifestyle</th>\n",
       "      <th>news_category_Social_Media</th>\n",
       "      <th>news_category_Tech</th>\n",
       "      <th>news_category_Uncategorized</th>\n",
       "      <th>news_category_World_News</th>\n",
       "      <th>day_of_week_Friday</th>\n",
       "      <th>day_of_week_Monday</th>\n",
       "      <th>day_of_week_Saturday</th>\n",
       "      <th>day_of_week_Sunday</th>\n",
       "      <th>day_of_week_Thursday</th>\n",
       "      <th>day_of_week_Tuesday</th>\n",
       "      <th>day_of_week_Wednesday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>593</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>4.546154</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.022276</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.681548</td>\n",
       "      <td>0.381987</td>\n",
       "      <td>0.152189</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.353939</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>1300</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>4.759494</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.199626</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.714611</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.542580</td>\n",
       "      <td>0.122370</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357269</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-0.338889</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1100</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.535390</td>\n",
       "      <td>5.147748</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.117255</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>0.425089</td>\n",
       "      <td>0.128515</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.337965</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.225794</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1600</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.320768</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.424132</td>\n",
       "      <td>4.631390</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.327017</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>0.506520</td>\n",
       "      <td>0.279769</td>\n",
       "      <td>0.071749</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.417055</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.212354</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2400</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.017506</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.302619</td>\n",
       "      <td>9.680406</td>\n",
       "      <td>8.140199</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.567227</td>\n",
       "      <td>4.313253</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>42.600000</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>571200.000000</td>\n",
       "      <td>2170.324903</td>\n",
       "      <td>3385.393320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040020</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.040008</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.839967</td>\n",
       "      <td>0.440992</td>\n",
       "      <td>0.266721</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.008032</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.385909</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.145833</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>5.521461</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>5.384495</td>\n",
       "      <td>9.994288</td>\n",
       "      <td>7.967156</td>\n",
       "      <td>8.071219</td>\n",
       "      <td>9.179984</td>\n",
       "      <td>8.771990</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.570136</td>\n",
       "      <td>4.589286</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>310130.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>3900.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020009</td>\n",
       "      <td>0.219217</td>\n",
       "      <td>0.020005</td>\n",
       "      <td>0.118088</td>\n",
       "      <td>0.622681</td>\n",
       "      <td>0.384271</td>\n",
       "      <td>0.197662</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.348636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>1400</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>5.416100</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>8.131825</td>\n",
       "      <td>7.313887</td>\n",
       "      <td>7.946497</td>\n",
       "      <td>7.003974</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.424165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.514925</td>\n",
       "      <td>4.263403</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>224885.714286</td>\n",
       "      <td>1880.000000</td>\n",
       "      <td>6433.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.172060</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.742224</td>\n",
       "      <td>0.434468</td>\n",
       "      <td>0.169252</td>\n",
       "      <td>0.039627</td>\n",
       "      <td>0.016317</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.391176</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.179847</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3200</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>6.063785</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.378384</td>\n",
       "      <td>8.366603</td>\n",
       "      <td>8.083845</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.506261</td>\n",
       "      <td>5.005172</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>88.857143</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>266628.571429</td>\n",
       "      <td>1558.755814</td>\n",
       "      <td>4966.668990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028579</td>\n",
       "      <td>0.028573</td>\n",
       "      <td>0.792900</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.121376</td>\n",
       "      <td>0.428833</td>\n",
       "      <td>0.188667</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.407333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.115000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1700</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>6.364751</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.104793</td>\n",
       "      <td>9.752723</td>\n",
       "      <td>7.912769</td>\n",
       "      <td>7.244942</td>\n",
       "      <td>7.244942</td>\n",
       "      <td>7.244942</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.701987</td>\n",
       "      <td>4.471338</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>366200.000000</td>\n",
       "      <td>3035.080555</td>\n",
       "      <td>3613.512953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.799339</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050659</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.517893</td>\n",
       "      <td>0.104892</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.012739</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.247338</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1300</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>5.062595</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>4.584967</td>\n",
       "      <td>12.233693</td>\n",
       "      <td>8.101044</td>\n",
       "      <td>7.650169</td>\n",
       "      <td>7.650169</td>\n",
       "      <td>7.650169</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39644 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timedelta  n_tokens_title  n_unique_tokens  average_token_length  \\\n",
       "0          731.0            12.0         0.663594              4.680365   \n",
       "1          731.0             8.0         0.821705              4.546154   \n",
       "2          731.0             9.0         0.608602              4.759494   \n",
       "3          731.0            10.0         0.535390              5.147748   \n",
       "4          731.0             9.0         0.424132              4.631390   \n",
       "...          ...             ...              ...                   ...   \n",
       "39639        9.0            12.0         0.567227              4.313253   \n",
       "39640        9.0            13.0         0.570136              4.589286   \n",
       "39641        9.0            12.0         0.514925              4.263403   \n",
       "39642        9.0            15.0         0.506261              5.005172   \n",
       "39643        8.0            10.0         0.701987              4.471338   \n",
       "\n",
       "       num_keywords  kw_min_min  kw_avg_min  kw_max_max     kw_avg_max  \\\n",
       "0               5.0         0.0    0.000000         0.0       0.000000   \n",
       "1               9.0         0.0    0.000000         0.0       0.000000   \n",
       "2               7.0         0.0    0.000000         0.0       0.000000   \n",
       "3              10.0         0.0    0.000000         0.0       0.000000   \n",
       "4               8.0         0.0    0.000000         0.0       0.000000   \n",
       "...             ...         ...         ...         ...            ...   \n",
       "39639           5.0        -1.0   42.600000    843300.0  571200.000000   \n",
       "39640          10.0        -1.0  511.000000    843300.0  310130.000000   \n",
       "39641           7.0        -1.0  525.000000    843300.0  224885.714286   \n",
       "39642           7.0        -1.0   88.857143    843300.0  266628.571429   \n",
       "39643           4.0        -1.0   23.500000    843300.0  366200.000000   \n",
       "\n",
       "        kw_min_avg   kw_max_avg  is_weekend    LDA_00    LDA_01    LDA_02  \\\n",
       "0         0.000000     0.000000         0.0  0.500331  0.378279  0.040005   \n",
       "1         0.000000     0.000000         0.0  0.022265  0.022446  0.022276   \n",
       "2         0.000000     0.000000         0.0  0.028575  0.199626  0.028615   \n",
       "3         0.000000     0.000000         0.0  0.020011  0.020317  0.117255   \n",
       "4         0.000000     0.000000         0.0  0.025001  0.327017  0.025001   \n",
       "...            ...          ...         ...       ...       ...       ...   \n",
       "39639  2170.324903  3385.393320         0.0  0.040020  0.040004  0.040008   \n",
       "39640  1500.000000  3900.000000         0.0  0.020009  0.219217  0.020005   \n",
       "39641  1880.000000  6433.333333         0.0  0.028572  0.172060  0.028572   \n",
       "39642  1558.755814  4966.668990         0.0  0.028579  0.028573  0.792900   \n",
       "39643  3035.080555  3613.512953         0.0  0.050001  0.799339  0.050000   \n",
       "\n",
       "         LDA_03    LDA_04  global_subjectivity  global_sentiment_polarity  \\\n",
       "0      0.041263  0.040123             0.521617                   0.092562   \n",
       "1      0.251465  0.681548             0.381987                   0.152189   \n",
       "2      0.714611  0.028572             0.542580                   0.122370   \n",
       "3      0.020007  0.822410             0.425089                   0.128515   \n",
       "4      0.025001  0.597981             0.506520                   0.279769   \n",
       "...         ...       ...                  ...                        ...   \n",
       "39639  0.040000  0.839967             0.440992                   0.266721   \n",
       "39640  0.118088  0.622681             0.384271                   0.197662   \n",
       "39641  0.028572  0.742224             0.434468                   0.169252   \n",
       "39642  0.028571  0.121376             0.428833                   0.188667   \n",
       "39643  0.050659  0.050001             0.517893                   0.104892   \n",
       "\n",
       "       global_rate_positive_words  global_rate_negative_words  \\\n",
       "0                        0.045662                    0.013699   \n",
       "1                        0.038462                    0.007692   \n",
       "2                        0.063291                    0.025316   \n",
       "3                        0.039640                    0.012613   \n",
       "4                        0.071749                    0.013453   \n",
       "...                           ...                         ...   \n",
       "39639                    0.040161                    0.008032   \n",
       "39640                    0.044643                    0.004464   \n",
       "39641                    0.039627                    0.016317   \n",
       "39642                    0.025862                    0.008621   \n",
       "39643                    0.063694                    0.012739   \n",
       "\n",
       "       rate_positive_words  rate_negative_words  avg_positive_polarity  \\\n",
       "0                 0.769231             0.230769               0.378636   \n",
       "1                 0.833333             0.166667               0.353939   \n",
       "2                 0.714286             0.285714               0.357269   \n",
       "3                 0.758621             0.241379               0.337965   \n",
       "4                 0.842105             0.157895               0.417055   \n",
       "...                    ...                  ...                    ...   \n",
       "39639             0.833333             0.166667               0.385909   \n",
       "39640             0.909091             0.090909               0.348636   \n",
       "39641             0.708333             0.291667               0.391176   \n",
       "39642             0.750000             0.250000               0.407333   \n",
       "39643             0.833333             0.166667               0.247338   \n",
       "\n",
       "       min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "0                   0.100000                   0.70              -0.350000   \n",
       "1                   0.033333                   0.70              -0.400000   \n",
       "2                   0.050000                   0.60              -0.338889   \n",
       "3                   0.050000                   0.70              -0.225794   \n",
       "4                   0.100000                   1.00              -0.212354   \n",
       "...                      ...                    ...                    ...   \n",
       "39639               0.136364                   1.00              -0.145833   \n",
       "39640               0.100000                   0.50              -0.071429   \n",
       "39641               0.166667                   0.75              -0.179847   \n",
       "39642               0.100000                   0.80              -0.115000   \n",
       "39643               0.100000                   0.50              -0.200000   \n",
       "\n",
       "       min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                  -0.600000              -0.200000            0.500000   \n",
       "1                  -0.400000              -0.400000            0.250000   \n",
       "2                  -1.000000              -0.050000            0.650000   \n",
       "3                  -0.400000              -0.125000            0.500000   \n",
       "4                  -0.500000              -0.050000            0.333333   \n",
       "...                      ...                    ...                 ...   \n",
       "39639              -0.166667              -0.125000            0.000000   \n",
       "39640              -0.071429              -0.071429            0.800000   \n",
       "39641              -0.312500              -0.025000            0.000000   \n",
       "39642              -0.125000              -0.100000            0.500000   \n",
       "39643              -0.200000              -0.200000            0.333333   \n",
       "\n",
       "       title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                       -0.1875                0.000000   \n",
       "1                        0.2000                0.250000   \n",
       "2                       -0.5000                0.150000   \n",
       "3                       -0.1000                0.000000   \n",
       "4                        0.2500                0.166667   \n",
       "...                         ...                     ...   \n",
       "39639                    0.0000                0.500000   \n",
       "39640                    0.4000                0.300000   \n",
       "39641                    0.0000                0.500000   \n",
       "39642                    0.5000                0.000000   \n",
       "39643                    0.2500                0.166667   \n",
       "\n",
       "       abs_title_sentiment_polarity  shares  year  month  \\\n",
       "0                            0.1875     593  2013      1   \n",
       "1                            0.2000    1300  2013      1   \n",
       "2                            0.5000    1100  2013      1   \n",
       "3                            0.1000    1600  2013      1   \n",
       "4                            0.2500    2400  2013      1   \n",
       "...                             ...     ...   ...    ...   \n",
       "39639                        0.0000    1100  2014     12   \n",
       "39640                        0.4000    1400  2014     12   \n",
       "39641                        0.0000    3200  2014     12   \n",
       "39642                        0.5000    1700  2014     12   \n",
       "39643                        0.2500    1300  2014     12   \n",
       "\n",
       "       log_n_tokens_content  log_num_hrefs  log_num_self_hrefs  log_num_imgs  \\\n",
       "0                  5.393628       1.609438            1.098612      0.693147   \n",
       "1                  4.875197       2.079442            1.609438      0.000000   \n",
       "2                  6.163315       2.484907            0.000000      0.693147   \n",
       "3                  6.320768       2.079442            1.945910      0.693147   \n",
       "4                  7.017506       3.091042            3.091042      3.044522   \n",
       "...                     ...            ...                 ...           ...   \n",
       "39639              5.521461       1.609438            1.098612      0.693147   \n",
       "39640              5.416100       2.079442            2.079442      0.693147   \n",
       "39641              6.063785       1.386294            1.386294      1.386294   \n",
       "39642              6.364751       2.772589            1.098612      1.386294   \n",
       "39643              5.062595       0.693147            0.693147      0.000000   \n",
       "\n",
       "       log_num_videos  log_kw_max_min  log_kw_min_max  log_kw_avg_avg  \\\n",
       "0            0.000000        0.000000        0.000000        0.000000   \n",
       "1            0.000000        0.000000        0.000000        0.000000   \n",
       "2            0.000000        0.000000        0.000000        0.000000   \n",
       "3            0.000000        0.000000        0.000000        0.000000   \n",
       "4            0.000000        0.000000        0.000000        0.000000   \n",
       "...               ...             ...             ...             ...   \n",
       "39639        0.693147        5.384495        9.994288        7.967156   \n",
       "39640        0.693147        8.131825        7.313887        7.946497   \n",
       "39641        0.000000        7.378384        8.366603        8.083845   \n",
       "39642        0.000000        6.104793        9.752723        7.912769   \n",
       "39643        1.098612        4.584967       12.233693        8.101044   \n",
       "\n",
       "       log_self_reference_min_shares  log_self_reference_max_shares  \\\n",
       "0                           6.208590                       6.208590   \n",
       "1                           7.170888                       7.170888   \n",
       "2                           0.000000                       0.000000   \n",
       "3                           7.550135                       7.550135   \n",
       "4                           6.302619                       9.680406   \n",
       "...                              ...                            ...   \n",
       "39639                       8.071219                       9.179984   \n",
       "39640                       7.003974                       7.550135   \n",
       "39641                       7.170888                       7.170888   \n",
       "39642                       7.244942                       7.244942   \n",
       "39643                       7.650169                       7.650169   \n",
       "\n",
       "       log_self_reference_avg_sharess  news_category_Business  \\\n",
       "0                            6.208590                       0   \n",
       "1                            7.170888                       0   \n",
       "2                            0.000000                       0   \n",
       "3                            7.550135                       0   \n",
       "4                            8.140199                       0   \n",
       "...                               ...                     ...   \n",
       "39639                        8.771990                       0   \n",
       "39640                        7.424165                       0   \n",
       "39641                        7.170888                       0   \n",
       "39642                        7.244942                       0   \n",
       "39643                        7.650169                       0   \n",
       "\n",
       "       news_category_Entertainment  news_category_Lifestyle  \\\n",
       "0                                1                        0   \n",
       "1                                0                        0   \n",
       "2                                0                        0   \n",
       "3                                0                        0   \n",
       "4                                0                        0   \n",
       "...                            ...                      ...   \n",
       "39639                            0                        0   \n",
       "39640                            0                        0   \n",
       "39641                            0                        0   \n",
       "39642                            0                        0   \n",
       "39643                            1                        0   \n",
       "\n",
       "       news_category_Social_Media  news_category_Tech  \\\n",
       "0                               0                   0   \n",
       "1                               0                   1   \n",
       "2                               0                   0   \n",
       "3                               0                   1   \n",
       "4                               0                   1   \n",
       "...                           ...                 ...   \n",
       "39639                           0                   1   \n",
       "39640                           0                   1   \n",
       "39641                           0                   1   \n",
       "39642                           0                   0   \n",
       "39643                           0                   0   \n",
       "\n",
       "       news_category_Uncategorized  news_category_World_News  \\\n",
       "0                                0                         0   \n",
       "1                                0                         0   \n",
       "2                                1                         0   \n",
       "3                                0                         0   \n",
       "4                                0                         0   \n",
       "...                            ...                       ...   \n",
       "39639                            0                         0   \n",
       "39640                            0                         0   \n",
       "39641                            0                         0   \n",
       "39642                            0                         1   \n",
       "39643                            0                         0   \n",
       "\n",
       "       day_of_week_Friday  day_of_week_Monday  day_of_week_Saturday  \\\n",
       "0                       0                   1                     0   \n",
       "1                       0                   1                     0   \n",
       "2                       0                   1                     0   \n",
       "3                       0                   1                     0   \n",
       "4                       0                   1                     0   \n",
       "...                   ...                 ...                   ...   \n",
       "39639                   0                   0                     0   \n",
       "39640                   0                   0                     0   \n",
       "39641                   0                   0                     0   \n",
       "39642                   0                   0                     0   \n",
       "39643                   0                   0                     0   \n",
       "\n",
       "       day_of_week_Sunday  day_of_week_Thursday  day_of_week_Tuesday  \\\n",
       "0                       0                     0                    0   \n",
       "1                       0                     0                    0   \n",
       "2                       0                     0                    0   \n",
       "3                       0                     0                    0   \n",
       "4                       0                     0                    0   \n",
       "...                   ...                   ...                  ...   \n",
       "39639                   0                     0                    1   \n",
       "39640                   0                     0                    1   \n",
       "39641                   0                     0                    1   \n",
       "39642                   0                     0                    1   \n",
       "39643                   0                     0                    0   \n",
       "\n",
       "       day_of_week_Wednesday  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          0  \n",
       "...                      ...  \n",
       "39639                      0  \n",
       "39640                      0  \n",
       "39641                      0  \n",
       "39642                      0  \n",
       "39643                      1  \n",
       "\n",
       "[39644 rows x 61 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove certain columns before dimensionality reduction can take place\n",
    "\n",
    "# drop certain columns\n",
    "df1 = df.drop('url_name', axis=1) # was a string\n",
    "df1 = df1.drop('date', axis=1) # datetime change didn't work.\n",
    "df1 = df1.drop('log_shares', axis=1) # not useful\n",
    "\n",
    "# Factor the `news_category` column.\n",
    "df1 = pd.get_dummies(df1, columns=['news_category'])\n",
    "\n",
    "# Factor the `day_of_week` column.\n",
    "df1 = pd.get_dummies(df1, columns=['day_of_week'])\n",
    "\n",
    "# drop Na's\n",
    "df1.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create share_quantile_ranges_variable for target\n",
    "\n",
    "# Create bins using quantiles\n",
    "q1 = df1['shares'].quantile(0.25)\n",
    "q2 = df1['shares'].quantile(0.5)\n",
    "q3 = df1['shares'].quantile(0.75)\n",
    "\n",
    "# Define the bin labels\n",
    "labels = ['<Q1', 'Q1-Q2', 'Q2-Q3', '>Q3']\n",
    "\n",
    "# Cut the shares column into bins\n",
    "df1['share_quantile_ranges'] = pd.cut(df1['shares'], bins=[0, q1, q2, q3, 1000000], labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['url_name', 'date', 'timedelta', 'n_tokens_title', 'n_unique_tokens',\n",
      "       'average_token_length', 'num_keywords', 'kw_min_min', 'kw_avg_min',\n",
      "       'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'is_weekend',\n",
      "       'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
      "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
      "       'global_rate_negative_words', 'rate_positive_words',\n",
      "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
      "       'max_positive_polarity', 'avg_negative_polarity',\n",
      "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
      "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
      "       'abs_title_sentiment_polarity', 'day_of_week', 'news_category', 'year',\n",
      "       'month', 'log_shares', 'log_n_tokens_content', 'log_num_hrefs',\n",
      "       'log_num_self_hrefs', 'log_num_imgs', 'log_num_videos',\n",
      "       'log_kw_max_min', 'log_kw_min_max', 'log_kw_avg_avg',\n",
      "       'log_self_reference_min_shares', 'log_self_reference_max_shares',\n",
      "       'log_self_reference_avg_sharess'],\n",
      "      dtype='object')\n",
      "0         593\n",
      "1        1300\n",
      "2        1100\n",
      "3        1600\n",
      "4        2400\n",
      "         ... \n",
      "39639    1100\n",
      "39640    1400\n",
      "39641    3200\n",
      "39642    1700\n",
      "39643    1300\n",
      "Name: shares, Length: 39644, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove target\n",
    "# X = df.drop(['share_ranges'], axis=1)\n",
    "# y = df['share_ranges']\n",
    "# print(X.columns)\n",
    "# print(y)\n",
    "\n",
    "# Training Test 80/20 Split\n",
    "# random_state = 42\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features in the training and testing sets using standard scalar.\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using QuantileTransformer with n_quantiles=100.\n",
    "# quantile_transformer = QuantileTransformer(n_quantiles=100)\n",
    "\n",
    "# X_train_q = quantile_transformer.fit_transform(X_train)\n",
    "# X_test_q = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is done on the training and testing sets \"X_train\" and \"X_test\" in order put the data on a common scale. This will helpful in improving the model performance as they  arre sensitive to the scale of the data. Scaling will be done for each classification task and both types of scaling will be done on the data seperately so that the two methods can be compared later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Remove variables that are not needed/useful for the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certain variables were deemed unnecessary during lab 1 and were removed during the course of this labs notebook.\n",
    "\n",
    "These variables were removed:\n",
    "\n",
    "Url:        \n",
    "    Dropped as it was better served being split into multiple variables.            \n",
    "    These varriables did end up not being useful or useable, however.  \n",
    "\n",
    "n_tokens_content:           \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "n_non_stop_words:              \n",
    "    Deemed unhelpful\n",
    "\n",
    "n_non_stop_unique_tokens:           \n",
    "    Deemed unhelpful\n",
    "\n",
    "num_hrefs:          \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "num_self_hrefs:         \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "    \n",
    "num_imgs:           \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "num_videos:         \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "kw_max_min:         \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "kw_min_max:         \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "self_reference_min_shares:          \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "self_reference_max_shares:          \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "self_reference_avg_sharess:             \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "weekday_is_X where X is the day of the week:        \n",
    "    Removed previously.         \n",
    "    This was to turn it into a categorical variable for the model being run at the time.        \n",
    "    This variable has been recreated for our current classification problems as shown above with the factoring of day_of_week.      \n",
    "\n",
    "data_channel_is_X where X is the type of data channel:      \n",
    "    Removed previously.     \n",
    "    This was to turn it into a categorical variable for the model being run at the time.        \n",
    "    This variable has been recreated for our current classification problems as shown above with the factoring of news_category.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>news_category</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>log_n_tokens_content</th>\n",
       "      <th>log_num_hrefs</th>\n",
       "      <th>log_num_self_hrefs</th>\n",
       "      <th>log_num_imgs</th>\n",
       "      <th>log_num_videos</th>\n",
       "      <th>log_kw_max_min</th>\n",
       "      <th>log_kw_min_max</th>\n",
       "      <th>log_kw_avg_avg</th>\n",
       "      <th>log_self_reference_min_shares</th>\n",
       "      <th>log_self_reference_max_shares</th>\n",
       "      <th>log_self_reference_avg_sharess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>593</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>4.546154</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.022276</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.681548</td>\n",
       "      <td>0.381987</td>\n",
       "      <td>0.152189</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.353939</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>1300</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>4.759494</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.199626</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.714611</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.542580</td>\n",
       "      <td>0.122370</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357269</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.338889</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1100</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.535390</td>\n",
       "      <td>5.147748</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.117255</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>0.425089</td>\n",
       "      <td>0.128515</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.337965</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.225794</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1600</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.320768</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.424132</td>\n",
       "      <td>4.631390</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.327017</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>0.506520</td>\n",
       "      <td>0.279769</td>\n",
       "      <td>0.071749</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.417055</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.212354</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2400</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.017506</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.302619</td>\n",
       "      <td>9.680406</td>\n",
       "      <td>8.140199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timedelta  n_tokens_title  n_unique_tokens  average_token_length  \\\n",
       "0      731.0            12.0         0.663594              4.680365   \n",
       "1      731.0             8.0         0.821705              4.546154   \n",
       "2      731.0             9.0         0.608602              4.759494   \n",
       "3      731.0            10.0         0.535390              5.147748   \n",
       "4      731.0             9.0         0.424132              4.631390   \n",
       "\n",
       "   num_keywords  kw_min_min  kw_avg_min  kw_max_max  kw_avg_max  kw_min_avg  \\\n",
       "0           5.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1           9.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2           7.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3          10.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4           8.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   kw_max_avg  is_weekend    LDA_00    LDA_01    LDA_02    LDA_03    LDA_04  \\\n",
       "0         0.0         0.0  0.500331  0.378279  0.040005  0.041263  0.040123   \n",
       "1         0.0         0.0  0.022265  0.022446  0.022276  0.251465  0.681548   \n",
       "2         0.0         0.0  0.028575  0.199626  0.028615  0.714611  0.028572   \n",
       "3         0.0         0.0  0.020011  0.020317  0.117255  0.020007  0.822410   \n",
       "4         0.0         0.0  0.025001  0.327017  0.025001  0.025001  0.597981   \n",
       "\n",
       "   global_subjectivity  global_sentiment_polarity  global_rate_positive_words  \\\n",
       "0             0.521617                   0.092562                    0.045662   \n",
       "1             0.381987                   0.152189                    0.038462   \n",
       "2             0.542580                   0.122370                    0.063291   \n",
       "3             0.425089                   0.128515                    0.039640   \n",
       "4             0.506520                   0.279769                    0.071749   \n",
       "\n",
       "   global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "0                    0.013699             0.769231             0.230769   \n",
       "1                    0.007692             0.833333             0.166667   \n",
       "2                    0.025316             0.714286             0.285714   \n",
       "3                    0.012613             0.758621             0.241379   \n",
       "4                    0.013453             0.842105             0.157895   \n",
       "\n",
       "   avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "0               0.378636               0.100000                    0.7   \n",
       "1               0.353939               0.033333                    0.7   \n",
       "2               0.357269               0.050000                    0.6   \n",
       "3               0.337965               0.050000                    0.7   \n",
       "4               0.417055               0.100000                    1.0   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.350000                   -0.6                 -0.200   \n",
       "1              -0.400000                   -0.4                 -0.400   \n",
       "2              -0.338889                   -1.0                 -0.050   \n",
       "3              -0.225794                   -0.4                 -0.125   \n",
       "4              -0.212354                   -0.5                 -0.050   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.500000                   -0.1875                0.000000   \n",
       "1            0.250000                    0.2000                0.250000   \n",
       "2            0.650000                   -0.5000                0.150000   \n",
       "3            0.500000                   -0.1000                0.000000   \n",
       "4            0.333333                    0.2500                0.166667   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares day_of_week  news_category  year  \\\n",
       "0                        0.1875     593      Monday  Entertainment  2013   \n",
       "1                        0.2000    1300      Monday           Tech  2013   \n",
       "2                        0.5000    1100      Monday  Uncategorized  2013   \n",
       "3                        0.1000    1600      Monday           Tech  2013   \n",
       "4                        0.2500    2400      Monday           Tech  2013   \n",
       "\n",
       "   month  log_n_tokens_content  log_num_hrefs  log_num_self_hrefs  \\\n",
       "0      1              5.393628       1.609438            1.098612   \n",
       "1      1              4.875197       2.079442            1.609438   \n",
       "2      1              6.163315       2.484907            0.000000   \n",
       "3      1              6.320768       2.079442            1.945910   \n",
       "4      1              7.017506       3.091042            3.091042   \n",
       "\n",
       "   log_num_imgs  log_num_videos  log_kw_max_min  log_kw_min_max  \\\n",
       "0      0.693147             0.0             0.0             0.0   \n",
       "1      0.000000             0.0             0.0             0.0   \n",
       "2      0.693147             0.0             0.0             0.0   \n",
       "3      0.693147             0.0             0.0             0.0   \n",
       "4      3.044522             0.0             0.0             0.0   \n",
       "\n",
       "   log_kw_avg_avg  log_self_reference_min_shares  \\\n",
       "0             0.0                       6.208590   \n",
       "1             0.0                       7.170888   \n",
       "2             0.0                       0.000000   \n",
       "3             0.0                       7.550135   \n",
       "4             0.0                       6.302619   \n",
       "\n",
       "   log_self_reference_max_shares  log_self_reference_avg_sharess  \n",
       "0                       6.208590                        6.208590  \n",
       "1                       7.170888                        7.170888  \n",
       "2                       0.000000                        0.000000  \n",
       "3                       7.550135                        7.550135  \n",
       "4                       9.680406                        8.140199  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from previously cleaned dataset\n",
    "\n",
    "# drop certain columns\n",
    "# Done above\n",
    "# df1 = df.drop('url_name', axis=1) # was a string\n",
    "# df1 = df1.drop('date', axis=1) # datetime change didn't work.\n",
    "# df1 = df1.drop('log_shares', axis=1) # not useful\n",
    "\n",
    "# drop Na's\n",
    "df1.dropna()\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url_name:           \n",
    "    Dropped due to it being a string variable that wasn't useful\n",
    "\n",
    "date:\n",
    "    Dropped due to it being a datetime variable that wasn't useful\n",
    "\n",
    "log_shares:\n",
    "    Dropped due to it being a datetime variable that wasn't useful.             \n",
    "    The share_quantile_ranges was deemed to be more useful.\n",
    "\n",
    "all Na's were dropped\n",
    "\n",
    "The shares, day_of_week, and news_category variables will be removed when selecting the target variable further down. Variables related to the current task will also be removed such as 'shares', 'day_of_week_Monday' (for example), and news_category_World (for example). This is to avoid these variables essentially giving the model the right answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation Part 2 [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the final dataset that is used for classification/regression\n",
    "(include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### share_quantile_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1-Q2    10152\n",
      "Q2-Q3     9932\n",
      "<Q1       9930\n",
      ">Q3       9630\n",
      "Name: share_quantile_ranges, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create bins using quantiles\n",
    "q1 = df1['shares'].quantile(0.25)\n",
    "q2 = df1['shares'].quantile(0.5)\n",
    "q3 = df1['shares'].quantile(0.75)\n",
    "\n",
    "# Define the bin labels\n",
    "labels = ['<Q1', 'Q1-Q2', 'Q2-Q3', '>Q3']\n",
    "\n",
    "# Cut the shares column into bins\n",
    "df1['share_quantile_ranges'] = pd.cut(df1['shares'], bins=[0, q1, q2, q3, 1000000], labels=labels)\n",
    "\n",
    "# Print the value counts of the share_ranges_quantile column\n",
    "print(df1['share_quantile_ranges'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "share_quantile_ranges: This was created by splitting up the shares variable into bins by using its different quantile values. This allowed for a more even split among the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### factoring day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['day_of_week'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11288\\3940214633.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Factor the `day_of_week` column.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'day_of_week'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\pandas\\core\\reshape\\reshape.py\u001b[0m in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    888\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Input must be a list-like for parameter `columns`\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 890\u001b[1;33m             \u001b[0mdata_to_encode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m         \u001b[1;31m# validate prefixes and separator to avoid silently dropping cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3462\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3464\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3466\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis)\u001b[0m\n\u001b[0;32m   1372\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1374\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['day_of_week'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Factor the `day_of_week` column.\n",
    "df1 = pd.get_dummies(df1, columns=['day_of_week'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "factored day_of_week columns: day_of_week was factored back into 7 columns of 0's and 1's similar to how it was in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### factoring news_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       timedelta  n_tokens_title  n_unique_tokens  average_token_length  \\\n",
      "0          731.0            12.0         0.663594              4.680365   \n",
      "1          731.0             8.0         0.821705              4.546154   \n",
      "2          731.0             9.0         0.608602              4.759494   \n",
      "3          731.0            10.0         0.535390              5.147748   \n",
      "4          731.0             9.0         0.424132              4.631390   \n",
      "...          ...             ...              ...                   ...   \n",
      "39639        9.0            12.0         0.567227              4.313253   \n",
      "39640        9.0            13.0         0.570136              4.589286   \n",
      "39641        9.0            12.0         0.514925              4.263403   \n",
      "39642        9.0            15.0         0.506261              5.005172   \n",
      "39643        8.0            10.0         0.701987              4.471338   \n",
      "\n",
      "       num_keywords  kw_min_min  kw_avg_min  kw_max_max     kw_avg_max  \\\n",
      "0               5.0         0.0    0.000000         0.0       0.000000   \n",
      "1               9.0         0.0    0.000000         0.0       0.000000   \n",
      "2               7.0         0.0    0.000000         0.0       0.000000   \n",
      "3              10.0         0.0    0.000000         0.0       0.000000   \n",
      "4               8.0         0.0    0.000000         0.0       0.000000   \n",
      "...             ...         ...         ...         ...            ...   \n",
      "39639           5.0        -1.0   42.600000    843300.0  571200.000000   \n",
      "39640          10.0        -1.0  511.000000    843300.0  310130.000000   \n",
      "39641           7.0        -1.0  525.000000    843300.0  224885.714286   \n",
      "39642           7.0        -1.0   88.857143    843300.0  266628.571429   \n",
      "39643           4.0        -1.0   23.500000    843300.0  366200.000000   \n",
      "\n",
      "        kw_min_avg   kw_max_avg  is_weekend    LDA_00    LDA_01    LDA_02  \\\n",
      "0         0.000000     0.000000         0.0  0.500331  0.378279  0.040005   \n",
      "1         0.000000     0.000000         0.0  0.022265  0.022446  0.022276   \n",
      "2         0.000000     0.000000         0.0  0.028575  0.199626  0.028615   \n",
      "3         0.000000     0.000000         0.0  0.020011  0.020317  0.117255   \n",
      "4         0.000000     0.000000         0.0  0.025001  0.327017  0.025001   \n",
      "...            ...          ...         ...       ...       ...       ...   \n",
      "39639  2170.324903  3385.393320         0.0  0.040020  0.040004  0.040008   \n",
      "39640  1500.000000  3900.000000         0.0  0.020009  0.219217  0.020005   \n",
      "39641  1880.000000  6433.333333         0.0  0.028572  0.172060  0.028572   \n",
      "39642  1558.755814  4966.668990         0.0  0.028579  0.028573  0.792900   \n",
      "39643  3035.080555  3613.512953         0.0  0.050001  0.799339  0.050000   \n",
      "\n",
      "         LDA_03    LDA_04  global_subjectivity  global_sentiment_polarity  \\\n",
      "0      0.041263  0.040123             0.521617                   0.092562   \n",
      "1      0.251465  0.681548             0.381987                   0.152189   \n",
      "2      0.714611  0.028572             0.542580                   0.122370   \n",
      "3      0.020007  0.822410             0.425089                   0.128515   \n",
      "4      0.025001  0.597981             0.506520                   0.279769   \n",
      "...         ...       ...                  ...                        ...   \n",
      "39639  0.040000  0.839967             0.440992                   0.266721   \n",
      "39640  0.118088  0.622681             0.384271                   0.197662   \n",
      "39641  0.028572  0.742224             0.434468                   0.169252   \n",
      "39642  0.028571  0.121376             0.428833                   0.188667   \n",
      "39643  0.050659  0.050001             0.517893                   0.104892   \n",
      "\n",
      "       global_rate_positive_words  global_rate_negative_words  \\\n",
      "0                        0.045662                    0.013699   \n",
      "1                        0.038462                    0.007692   \n",
      "2                        0.063291                    0.025316   \n",
      "3                        0.039640                    0.012613   \n",
      "4                        0.071749                    0.013453   \n",
      "...                           ...                         ...   \n",
      "39639                    0.040161                    0.008032   \n",
      "39640                    0.044643                    0.004464   \n",
      "39641                    0.039627                    0.016317   \n",
      "39642                    0.025862                    0.008621   \n",
      "39643                    0.063694                    0.012739   \n",
      "\n",
      "       rate_positive_words  rate_negative_words  avg_positive_polarity  \\\n",
      "0                 0.769231             0.230769               0.378636   \n",
      "1                 0.833333             0.166667               0.353939   \n",
      "2                 0.714286             0.285714               0.357269   \n",
      "3                 0.758621             0.241379               0.337965   \n",
      "4                 0.842105             0.157895               0.417055   \n",
      "...                    ...                  ...                    ...   \n",
      "39639             0.833333             0.166667               0.385909   \n",
      "39640             0.909091             0.090909               0.348636   \n",
      "39641             0.708333             0.291667               0.391176   \n",
      "39642             0.750000             0.250000               0.407333   \n",
      "39643             0.833333             0.166667               0.247338   \n",
      "\n",
      "       min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
      "0                   0.100000                   0.70              -0.350000   \n",
      "1                   0.033333                   0.70              -0.400000   \n",
      "2                   0.050000                   0.60              -0.338889   \n",
      "3                   0.050000                   0.70              -0.225794   \n",
      "4                   0.100000                   1.00              -0.212354   \n",
      "...                      ...                    ...                    ...   \n",
      "39639               0.136364                   1.00              -0.145833   \n",
      "39640               0.100000                   0.50              -0.071429   \n",
      "39641               0.166667                   0.75              -0.179847   \n",
      "39642               0.100000                   0.80              -0.115000   \n",
      "39643               0.100000                   0.50              -0.200000   \n",
      "\n",
      "       min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
      "0                  -0.600000              -0.200000            0.500000   \n",
      "1                  -0.400000              -0.400000            0.250000   \n",
      "2                  -1.000000              -0.050000            0.650000   \n",
      "3                  -0.400000              -0.125000            0.500000   \n",
      "4                  -0.500000              -0.050000            0.333333   \n",
      "...                      ...                    ...                 ...   \n",
      "39639              -0.166667              -0.125000            0.000000   \n",
      "39640              -0.071429              -0.071429            0.800000   \n",
      "39641              -0.312500              -0.025000            0.000000   \n",
      "39642              -0.125000              -0.100000            0.500000   \n",
      "39643              -0.200000              -0.200000            0.333333   \n",
      "\n",
      "       title_sentiment_polarity  abs_title_subjectivity  \\\n",
      "0                       -0.1875                0.000000   \n",
      "1                        0.2000                0.250000   \n",
      "2                       -0.5000                0.150000   \n",
      "3                       -0.1000                0.000000   \n",
      "4                        0.2500                0.166667   \n",
      "...                         ...                     ...   \n",
      "39639                    0.0000                0.500000   \n",
      "39640                    0.4000                0.300000   \n",
      "39641                    0.0000                0.500000   \n",
      "39642                    0.5000                0.000000   \n",
      "39643                    0.2500                0.166667   \n",
      "\n",
      "       abs_title_sentiment_polarity  shares  year  month  \\\n",
      "0                            0.1875     593  2013      1   \n",
      "1                            0.2000    1300  2013      1   \n",
      "2                            0.5000    1100  2013      1   \n",
      "3                            0.1000    1600  2013      1   \n",
      "4                            0.2500    2400  2013      1   \n",
      "...                             ...     ...   ...    ...   \n",
      "39639                        0.0000    1100  2014     12   \n",
      "39640                        0.4000    1400  2014     12   \n",
      "39641                        0.0000    3200  2014     12   \n",
      "39642                        0.5000    1700  2014     12   \n",
      "39643                        0.2500    1300  2014     12   \n",
      "\n",
      "       log_n_tokens_content  log_num_hrefs  log_num_self_hrefs  log_num_imgs  \\\n",
      "0                  5.393628       1.609438            1.098612      0.693147   \n",
      "1                  4.875197       2.079442            1.609438      0.000000   \n",
      "2                  6.163315       2.484907            0.000000      0.693147   \n",
      "3                  6.320768       2.079442            1.945910      0.693147   \n",
      "4                  7.017506       3.091042            3.091042      3.044522   \n",
      "...                     ...            ...                 ...           ...   \n",
      "39639              5.521461       1.609438            1.098612      0.693147   \n",
      "39640              5.416100       2.079442            2.079442      0.693147   \n",
      "39641              6.063785       1.386294            1.386294      1.386294   \n",
      "39642              6.364751       2.772589            1.098612      1.386294   \n",
      "39643              5.062595       0.693147            0.693147      0.000000   \n",
      "\n",
      "       log_num_videos  log_kw_max_min  log_kw_min_max  log_kw_avg_avg  \\\n",
      "0            0.000000        0.000000        0.000000        0.000000   \n",
      "1            0.000000        0.000000        0.000000        0.000000   \n",
      "2            0.000000        0.000000        0.000000        0.000000   \n",
      "3            0.000000        0.000000        0.000000        0.000000   \n",
      "4            0.000000        0.000000        0.000000        0.000000   \n",
      "...               ...             ...             ...             ...   \n",
      "39639        0.693147        5.384495        9.994288        7.967156   \n",
      "39640        0.693147        8.131825        7.313887        7.946497   \n",
      "39641        0.000000        7.378384        8.366603        8.083845   \n",
      "39642        0.000000        6.104793        9.752723        7.912769   \n",
      "39643        1.098612        4.584967       12.233693        8.101044   \n",
      "\n",
      "       log_self_reference_min_shares  log_self_reference_max_shares  \\\n",
      "0                           6.208590                       6.208590   \n",
      "1                           7.170888                       7.170888   \n",
      "2                           0.000000                       0.000000   \n",
      "3                           7.550135                       7.550135   \n",
      "4                           6.302619                       9.680406   \n",
      "...                              ...                            ...   \n",
      "39639                       8.071219                       9.179984   \n",
      "39640                       7.003974                       7.550135   \n",
      "39641                       7.170888                       7.170888   \n",
      "39642                       7.244942                       7.244942   \n",
      "39643                       7.650169                       7.650169   \n",
      "\n",
      "       log_self_reference_avg_sharess share_quantile_ranges  \\\n",
      "0                            6.208590                   <Q1   \n",
      "1                            7.170888                 Q1-Q2   \n",
      "2                            0.000000                 Q1-Q2   \n",
      "3                            7.550135                 Q2-Q3   \n",
      "4                            8.140199                 Q2-Q3   \n",
      "...                               ...                   ...   \n",
      "39639                        8.771990                 Q1-Q2   \n",
      "39640                        7.424165                 Q1-Q2   \n",
      "39641                        7.170888                   >Q3   \n",
      "39642                        7.244942                 Q2-Q3   \n",
      "39643                        7.650169                 Q1-Q2   \n",
      "\n",
      "       day_of_week_Friday  day_of_week_Monday  day_of_week_Saturday  \\\n",
      "0                       0                   1                     0   \n",
      "1                       0                   1                     0   \n",
      "2                       0                   1                     0   \n",
      "3                       0                   1                     0   \n",
      "4                       0                   1                     0   \n",
      "...                   ...                 ...                   ...   \n",
      "39639                   0                   0                     0   \n",
      "39640                   0                   0                     0   \n",
      "39641                   0                   0                     0   \n",
      "39642                   0                   0                     0   \n",
      "39643                   0                   0                     0   \n",
      "\n",
      "       day_of_week_Sunday  day_of_week_Thursday  day_of_week_Tuesday  \\\n",
      "0                       0                     0                    0   \n",
      "1                       0                     0                    0   \n",
      "2                       0                     0                    0   \n",
      "3                       0                     0                    0   \n",
      "4                       0                     0                    0   \n",
      "...                   ...                   ...                  ...   \n",
      "39639                   0                     0                    1   \n",
      "39640                   0                     0                    1   \n",
      "39641                   0                     0                    1   \n",
      "39642                   0                     0                    1   \n",
      "39643                   0                     0                    0   \n",
      "\n",
      "       day_of_week_Wednesday  news_category_Business  \\\n",
      "0                          0                       0   \n",
      "1                          0                       0   \n",
      "2                          0                       0   \n",
      "3                          0                       0   \n",
      "4                          0                       0   \n",
      "...                      ...                     ...   \n",
      "39639                      0                       0   \n",
      "39640                      0                       0   \n",
      "39641                      0                       0   \n",
      "39642                      0                       0   \n",
      "39643                      1                       0   \n",
      "\n",
      "       news_category_Entertainment  news_category_Lifestyle  \\\n",
      "0                                1                        0   \n",
      "1                                0                        0   \n",
      "2                                0                        0   \n",
      "3                                0                        0   \n",
      "4                                0                        0   \n",
      "...                            ...                      ...   \n",
      "39639                            0                        0   \n",
      "39640                            0                        0   \n",
      "39641                            0                        0   \n",
      "39642                            0                        0   \n",
      "39643                            1                        0   \n",
      "\n",
      "       news_category_Social_Media  news_category_Tech  \\\n",
      "0                               0                   0   \n",
      "1                               0                   1   \n",
      "2                               0                   0   \n",
      "3                               0                   1   \n",
      "4                               0                   1   \n",
      "...                           ...                 ...   \n",
      "39639                           0                   1   \n",
      "39640                           0                   1   \n",
      "39641                           0                   1   \n",
      "39642                           0                   0   \n",
      "39643                           0                   0   \n",
      "\n",
      "       news_category_Uncategorized  news_category_World_News  \n",
      "0                                0                         0  \n",
      "1                                0                         0  \n",
      "2                                1                         0  \n",
      "3                                0                         0  \n",
      "4                                0                         0  \n",
      "...                            ...                       ...  \n",
      "39639                            0                         0  \n",
      "39640                            0                         0  \n",
      "39641                            0                         0  \n",
      "39642                            0                         1  \n",
      "39643                            0                         0  \n",
      "\n",
      "[39644 rows x 62 columns]\n"
     ]
    }
   ],
   "source": [
    "# Factor the `news_category` column.\n",
    "df1 = pd.get_dummies(df1, columns=['news_category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "factored news_category columns: day_of_week was factored back into 6 columns of 0's and 1's representative if it is that day or not similar to how it was in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>log_n_tokens_content</th>\n",
       "      <th>log_num_hrefs</th>\n",
       "      <th>log_num_self_hrefs</th>\n",
       "      <th>log_num_imgs</th>\n",
       "      <th>log_num_videos</th>\n",
       "      <th>log_kw_max_min</th>\n",
       "      <th>log_kw_min_max</th>\n",
       "      <th>log_kw_avg_avg</th>\n",
       "      <th>log_self_reference_min_shares</th>\n",
       "      <th>log_self_reference_max_shares</th>\n",
       "      <th>log_self_reference_avg_sharess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>354.530471</td>\n",
       "      <td>10.398749</td>\n",
       "      <td>0.548216</td>\n",
       "      <td>4.548239</td>\n",
       "      <td>7.223767</td>\n",
       "      <td>26.106801</td>\n",
       "      <td>312.366967</td>\n",
       "      <td>752324.066694</td>\n",
       "      <td>259281.938083</td>\n",
       "      <td>1117.146610</td>\n",
       "      <td>5657.211151</td>\n",
       "      <td>0.130915</td>\n",
       "      <td>0.184599</td>\n",
       "      <td>0.141256</td>\n",
       "      <td>0.216321</td>\n",
       "      <td>0.223770</td>\n",
       "      <td>0.234029</td>\n",
       "      <td>0.443370</td>\n",
       "      <td>0.119309</td>\n",
       "      <td>0.039625</td>\n",
       "      <td>0.016612</td>\n",
       "      <td>0.682150</td>\n",
       "      <td>0.287934</td>\n",
       "      <td>0.353825</td>\n",
       "      <td>0.095446</td>\n",
       "      <td>0.756728</td>\n",
       "      <td>-0.259524</td>\n",
       "      <td>-0.521944</td>\n",
       "      <td>-0.107500</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.071425</td>\n",
       "      <td>0.341843</td>\n",
       "      <td>0.156064</td>\n",
       "      <td>3395.380184</td>\n",
       "      <td>2013.540939</td>\n",
       "      <td>6.615856</td>\n",
       "      <td>5.889971</td>\n",
       "      <td>2.156564</td>\n",
       "      <td>1.208878</td>\n",
       "      <td>1.116427</td>\n",
       "      <td>0.400420</td>\n",
       "      <td>6.393888</td>\n",
       "      <td>5.045209</td>\n",
       "      <td>7.976327</td>\n",
       "      <td>6.195185</td>\n",
       "      <td>6.917477</td>\n",
       "      <td>6.667697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>214.163767</td>\n",
       "      <td>2.114037</td>\n",
       "      <td>3.520708</td>\n",
       "      <td>0.844406</td>\n",
       "      <td>1.909130</td>\n",
       "      <td>69.633215</td>\n",
       "      <td>620.783887</td>\n",
       "      <td>214502.129573</td>\n",
       "      <td>135102.247285</td>\n",
       "      <td>1137.456951</td>\n",
       "      <td>6098.871957</td>\n",
       "      <td>0.337312</td>\n",
       "      <td>0.262975</td>\n",
       "      <td>0.219707</td>\n",
       "      <td>0.282145</td>\n",
       "      <td>0.295191</td>\n",
       "      <td>0.289183</td>\n",
       "      <td>0.116685</td>\n",
       "      <td>0.096931</td>\n",
       "      <td>0.017429</td>\n",
       "      <td>0.010828</td>\n",
       "      <td>0.190206</td>\n",
       "      <td>0.156156</td>\n",
       "      <td>0.104542</td>\n",
       "      <td>0.071315</td>\n",
       "      <td>0.247786</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>0.290290</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.324247</td>\n",
       "      <td>0.265450</td>\n",
       "      <td>0.188791</td>\n",
       "      <td>0.226294</td>\n",
       "      <td>11626.950749</td>\n",
       "      <td>0.498327</td>\n",
       "      <td>3.390683</td>\n",
       "      <td>1.255442</td>\n",
       "      <td>0.809445</td>\n",
       "      <td>0.692698</td>\n",
       "      <td>0.973755</td>\n",
       "      <td>0.680486</td>\n",
       "      <td>1.311168</td>\n",
       "      <td>4.521016</td>\n",
       "      <td>0.489467</td>\n",
       "      <td>3.076913</td>\n",
       "      <td>3.432430</td>\n",
       "      <td>3.280186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.393750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>164.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.470870</td>\n",
       "      <td>4.478404</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>141.750000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>172846.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3562.101631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>0.025012</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028574</td>\n",
       "      <td>0.396167</td>\n",
       "      <td>0.057757</td>\n",
       "      <td>0.028384</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.306244</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.328383</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>946.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.509388</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.100319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.776304</td>\n",
       "      <td>6.461468</td>\n",
       "      <td>7.003974</td>\n",
       "      <td>6.889782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>339.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.539226</td>\n",
       "      <td>4.664082</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>235.500000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>244572.222223</td>\n",
       "      <td>1023.635611</td>\n",
       "      <td>4355.688836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.040727</td>\n",
       "      <td>0.453457</td>\n",
       "      <td>0.119117</td>\n",
       "      <td>0.039023</td>\n",
       "      <td>0.015337</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.358755</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.253333</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.016157</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.493754</td>\n",
       "      <td>7.244942</td>\n",
       "      <td>7.962442</td>\n",
       "      <td>7.090910</td>\n",
       "      <td>7.937732</td>\n",
       "      <td>7.696667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>542.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>4.854839</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>330980.000000</td>\n",
       "      <td>2056.781032</td>\n",
       "      <td>6019.953968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240958</td>\n",
       "      <td>0.150831</td>\n",
       "      <td>0.334218</td>\n",
       "      <td>0.375763</td>\n",
       "      <td>0.399986</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.177832</td>\n",
       "      <td>0.050279</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.411428</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.186905</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2800.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.575076</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>6.908755</td>\n",
       "      <td>8.974745</td>\n",
       "      <td>8.189031</td>\n",
       "      <td>7.863651</td>\n",
       "      <td>8.987322</td>\n",
       "      <td>8.556606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>8.041534</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>42827.857143</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>3613.039819</td>\n",
       "      <td>298400.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926994</td>\n",
       "      <td>0.925947</td>\n",
       "      <td>0.919999</td>\n",
       "      <td>0.926534</td>\n",
       "      <td>0.927191</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727841</td>\n",
       "      <td>0.155488</td>\n",
       "      <td>0.184932</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.044876</td>\n",
       "      <td>5.720312</td>\n",
       "      <td>4.762174</td>\n",
       "      <td>4.859812</td>\n",
       "      <td>4.521789</td>\n",
       "      <td>12.606193</td>\n",
       "      <td>13.645079</td>\n",
       "      <td>10.682093</td>\n",
       "      <td>13.645079</td>\n",
       "      <td>13.645079</td>\n",
       "      <td>13.645079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          timedelta  n_tokens_title  n_unique_tokens  average_token_length  \\\n",
       "count  39644.000000    39644.000000     39644.000000          39644.000000   \n",
       "mean     354.530471       10.398749         0.548216              4.548239   \n",
       "std      214.163767        2.114037         3.520708              0.844406   \n",
       "min        8.000000        2.000000         0.000000              0.000000   \n",
       "25%      164.000000        9.000000         0.470870              4.478404   \n",
       "50%      339.000000       10.000000         0.539226              4.664082   \n",
       "75%      542.000000       12.000000         0.608696              4.854839   \n",
       "max      731.000000       23.000000       701.000000              8.041534   \n",
       "\n",
       "       num_keywords    kw_min_min    kw_avg_min     kw_max_max     kw_avg_max  \\\n",
       "count  39644.000000  39644.000000  39644.000000   39644.000000   39644.000000   \n",
       "mean       7.223767     26.106801    312.366967  752324.066694  259281.938083   \n",
       "std        1.909130     69.633215    620.783887  214502.129573  135102.247285   \n",
       "min        1.000000     -1.000000     -1.000000       0.000000       0.000000   \n",
       "25%        6.000000     -1.000000    141.750000  843300.000000  172846.875000   \n",
       "50%        7.000000     -1.000000    235.500000  843300.000000  244572.222223   \n",
       "75%        9.000000      4.000000    357.000000  843300.000000  330980.000000   \n",
       "max       10.000000    377.000000  42827.857143  843300.000000  843300.000000   \n",
       "\n",
       "         kw_min_avg     kw_max_avg    is_weekend        LDA_00        LDA_01  \\\n",
       "count  39644.000000   39644.000000  39644.000000  39644.000000  39644.000000   \n",
       "mean    1117.146610    5657.211151      0.130915      0.184599      0.141256   \n",
       "std     1137.456951    6098.871957      0.337312      0.262975      0.219707   \n",
       "min       -1.000000       0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000    3562.101631      0.000000      0.025051      0.025012   \n",
       "50%     1023.635611    4355.688836      0.000000      0.033387      0.033345   \n",
       "75%     2056.781032    6019.953968      0.000000      0.240958      0.150831   \n",
       "max     3613.039819  298400.000000      1.000000      0.926994      0.925947   \n",
       "\n",
       "             LDA_02        LDA_03        LDA_04  global_subjectivity  \\\n",
       "count  39644.000000  39644.000000  39644.000000         39644.000000   \n",
       "mean       0.216321      0.223770      0.234029             0.443370   \n",
       "std        0.282145      0.295191      0.289183             0.116685   \n",
       "min        0.000000      0.000000      0.000000             0.000000   \n",
       "25%        0.028571      0.028571      0.028574             0.396167   \n",
       "50%        0.040004      0.040001      0.040727             0.453457   \n",
       "75%        0.334218      0.375763      0.399986             0.508333   \n",
       "max        0.919999      0.926534      0.927191             1.000000   \n",
       "\n",
       "       global_sentiment_polarity  global_rate_positive_words  \\\n",
       "count               39644.000000                39644.000000   \n",
       "mean                    0.119309                    0.039625   \n",
       "std                     0.096931                    0.017429   \n",
       "min                    -0.393750                    0.000000   \n",
       "25%                     0.057757                    0.028384   \n",
       "50%                     0.119117                    0.039023   \n",
       "75%                     0.177832                    0.050279   \n",
       "max                     0.727841                    0.155488   \n",
       "\n",
       "       global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "count                39644.000000         39644.000000         39644.000000   \n",
       "mean                     0.016612             0.682150             0.287934   \n",
       "std                      0.010828             0.190206             0.156156   \n",
       "min                      0.000000             0.000000             0.000000   \n",
       "25%                      0.009615             0.600000             0.185185   \n",
       "50%                      0.015337             0.710526             0.280000   \n",
       "75%                      0.021739             0.800000             0.384615   \n",
       "max                      0.184932             1.000000             1.000000   \n",
       "\n",
       "       avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "count           39644.000000           39644.000000           39644.000000   \n",
       "mean                0.353825               0.095446               0.756728   \n",
       "std                 0.104542               0.071315               0.247786   \n",
       "min                 0.000000               0.000000               0.000000   \n",
       "25%                 0.306244               0.050000               0.600000   \n",
       "50%                 0.358755               0.100000               0.800000   \n",
       "75%                 0.411428               0.100000               1.000000   \n",
       "max                 1.000000               1.000000               1.000000   \n",
       "\n",
       "       avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "count           39644.000000           39644.000000           39644.000000   \n",
       "mean               -0.259524              -0.521944              -0.107500   \n",
       "std                 0.127726               0.290290               0.095373   \n",
       "min                -1.000000              -1.000000              -1.000000   \n",
       "25%                -0.328383              -0.700000              -0.125000   \n",
       "50%                -0.253333              -0.500000              -0.100000   \n",
       "75%                -0.186905              -0.300000              -0.050000   \n",
       "max                 0.000000               0.000000               0.000000   \n",
       "\n",
       "       title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "count        39644.000000              39644.000000            39644.000000   \n",
       "mean             0.282353                  0.071425                0.341843   \n",
       "std              0.324247                  0.265450                0.188791   \n",
       "min              0.000000                 -1.000000                0.000000   \n",
       "25%              0.000000                  0.000000                0.166667   \n",
       "50%              0.150000                  0.000000                0.500000   \n",
       "75%              0.500000                  0.150000                0.500000   \n",
       "max              1.000000                  1.000000                0.500000   \n",
       "\n",
       "       abs_title_sentiment_polarity         shares          year  \\\n",
       "count                  39644.000000   39644.000000  39644.000000   \n",
       "mean                       0.156064    3395.380184   2013.540939   \n",
       "std                        0.226294   11626.950749      0.498327   \n",
       "min                        0.000000       1.000000   2013.000000   \n",
       "25%                        0.000000     946.000000   2013.000000   \n",
       "50%                        0.000000    1400.000000   2014.000000   \n",
       "75%                        0.250000    2800.000000   2014.000000   \n",
       "max                        1.000000  843300.000000   2014.000000   \n",
       "\n",
       "              month  log_n_tokens_content  log_num_hrefs  log_num_self_hrefs  \\\n",
       "count  39644.000000          39644.000000   39644.000000        39644.000000   \n",
       "mean       6.615856              5.889971       2.156564            1.208878   \n",
       "std        3.390683              1.255442       0.809445            0.692698   \n",
       "min        1.000000              0.000000       0.000000            0.000000   \n",
       "25%        4.000000              5.509388       1.609438            0.693147   \n",
       "50%        7.000000              6.016157       2.197225            1.386294   \n",
       "75%       10.000000              6.575076       2.708050            1.609438   \n",
       "max       12.000000              9.044876       5.720312            4.762174   \n",
       "\n",
       "       log_num_imgs  log_num_videos  log_kw_max_min  log_kw_min_max  \\\n",
       "count  39644.000000    39644.000000    39644.000000    39644.000000   \n",
       "mean       1.116427        0.400420        6.393888        5.045209   \n",
       "std        0.973755        0.680486        1.311168        4.521016   \n",
       "min        0.000000        0.000000        0.000000        0.000000   \n",
       "25%        0.693147        0.000000        6.100319        0.000000   \n",
       "50%        0.693147        0.000000        6.493754        7.244942   \n",
       "75%        1.609438        0.693147        6.908755        8.974745   \n",
       "max        4.859812        4.521789       12.606193       13.645079   \n",
       "\n",
       "       log_kw_avg_avg  log_self_reference_min_shares  \\\n",
       "count    39644.000000                   39644.000000   \n",
       "mean         7.976327                       6.195185   \n",
       "std          0.489467                       3.076913   \n",
       "min          0.000000                       0.000000   \n",
       "25%          7.776304                       6.461468   \n",
       "50%          7.962442                       7.090910   \n",
       "75%          8.189031                       7.863651   \n",
       "max         10.682093                      13.645079   \n",
       "\n",
       "       log_self_reference_max_shares  log_self_reference_avg_sharess  \n",
       "count                   39644.000000                    39644.000000  \n",
       "mean                        6.917477                        6.667697  \n",
       "std                         3.432430                        3.280186  \n",
       "min                         0.000000                        0.000000  \n",
       "25%                         7.003974                        6.889782  \n",
       "50%                         7.937732                        7.696667  \n",
       "75%                         8.987322                        8.556606  \n",
       "max                        13.645079                       13.645079  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 1 (10 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose and explain your evaluation metrics that you will use: (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why are the measure(s) appropriate for analyzing the results of your modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Give a detailed explanation backing up any assertions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 2 (10 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the method you will use for dividing your data into training and testing splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "share_quantile_ranges task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove target, related variables, and other targets\n",
    "X1 = df1.drop(['share_quantile_ranges', 'shares', 'day_of_week', 'news_category'], axis=1) \n",
    "y1 = df1['share_quantile_ranges']\n",
    "# print(X1.columns)\n",
    "# print(y1)\n",
    "\n",
    "# Splitting the Data for share_quantile_ranges task \n",
    "random_state = 42\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "day_of_week task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove target, related variables, and other targets\n",
    "X2 = df1.drop(['share_quantile_ranges', 'day_of_week', 'news_category', 'day_of_week_Monday', 'day_of_week_Tuesday', 'day_of_week_Wednesday', 'day_of_week_Thursday', 'day_of_week_Friday', 'day_of_week_Saturday', 'day_of_week_Sunnday'], axis=1) # Removing target categorical variable and other categorical variables\n",
    "y2 = df1['day_of_week']\n",
    "# print(X2.columns)\n",
    "# print(y2)\n",
    "\n",
    "# Splitting the Data for day_of_week task\n",
    "random_state = 42\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Data for news_category task\n",
    "X3 = df1.drop(['share_quantile_ranges', 'day_of_week', 'news_category', 'news_category_Lifestyle', 'news_category_Entertainment', 'news_category_Business', 'news_category_Social Media', 'news_category_Tech', 'news_category_World'], axis=1) # Removing target categorical variable and other categorical variables\n",
    "y3 = df1['news_category']\n",
    "# print(X2.columns)\n",
    "# print(y2)\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (i.e., are you using Stratified 10-fold cross validation? Why?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Explain why your chosen method is appropriate or use more than one method as appropriate.\n",
    "For example, if you are using time series data then you should be using continuous training and testing sets across time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 3 (20 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create three different classification/regression models (e.g., random forest,KNN, and SVM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two modeling techniques must be new (but the third could be SVM or logistic regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features in the training and testing sets using StandardScalar.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Use previous train/test split\n",
    "\n",
    "# for share_quantile_range_task\n",
    "X_train1 = scaler.fit_transform(X_train1)\n",
    "X_test1 = scaler.transform(X_test1)\n",
    "\n",
    "# for day_of_week task\n",
    "X_train2 = scaler.fit_transform(X_train2)\n",
    "X_test2 = scaler.transform(X_test2)\n",
    "\n",
    "# for news_category task\n",
    "X_train2 = scaler.fit_transform(X_train3)\n",
    "X_test2 = scaler.transform(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features in the training and testing sets using QuantileTransformer.\n",
    "quantile_transformer = QuantileTransformer(n_quantiles=100)\n",
    "\n",
    "# Use previous train/test split\n",
    "\n",
    "# for share_quantile_range_task\n",
    "X_train_q1 = quantile_transformer.fit_transform(X_train1)\n",
    "X_test_q1 = quantile_transformer.fit_transform(X_test1)\n",
    "\n",
    "# for day_of_week task\n",
    "X_train_q2 = quantile_transformer.fit_transform(X_train2)\n",
    "X_test_q2 = quantile_transformer.fit_transform(X_test2)\n",
    "\n",
    "# for news_category task\n",
    "X_train_q3 = quantile_transformer.fit_transform(X_train3)\n",
    "X_test_q3 = quantile_transformer.fit_transform(X_test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random forest model\n",
    "rf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### share_quantile_ranges task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StandardScalar transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the share_quantile_ranges model\n",
    "rf.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for share_quantile_ranges task\n",
    "rf_y_pred1 = rf.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest on share_quantile_ranges Task\n",
      "[0.38907807 0.38282191 0.39315732 0.38366453]\n",
      "---------------------------------------------------------\n",
      "Random Forest accuracy: 0.3890780678521882\n",
      "Random Forest precision: 0.38282190554013523\n",
      "Random Forest recall: 0.3931573150609805\n",
      "Random Forest F1 score: 0.3836645269345822\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the share_quantile_ranges task.\n",
    "rf_accuracy1 = accuracy_score(y_test1, rf_y_pred1)\n",
    "rf_precision1 = precision_score(y_test1, rf_y_pred1, average='macro')\n",
    "rf_recall1 = recall_score(y_test1, rf_y_pred1, average='macro')\n",
    "rf_f1_score1 = f1_score(y_test1, rf_y_pred1, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics\n",
    "rf_model_scores1 = np.array([rf_accuracy1, rf_precision1, rf_recall1, rf_f1_score1])\n",
    "\n",
    "# Print the model evaluation metrics\n",
    "print('Random Forest on share_quantile_ranges Task')\n",
    "print(rf_model_scores1)\n",
    "print('---------------------------------------------------------')\n",
    "print('Random Forest accuracy:', rf_accuracy1)\n",
    "print('Random Forest precision:', rf_precision1)\n",
    "print('Random Forest recall:', rf_recall1)\n",
    "print('Random Forest F1 score:', rf_f1_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QuantileTransformer transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the share_quantile_ranges model\n",
    "rf.fit(X_train_q1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for share_quantile_ranges task\n",
    "rf_y_pred_q1 = rf.predict(X_test_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest on share_quantile_ranges Task\n",
      "[0.38567285 0.37939086 0.3895652  0.37985307]\n",
      "---------------------------------------------------------\n",
      "Random Forest accuracy: 0.3856728465128011\n",
      "Random Forest precision: 0.37939085952319035\n",
      "Random Forest recall: 0.3895651957056333\n",
      "Random Forest F1 score: 0.379853067019191\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the share_quantile_ranges task.\n",
    "rf_accuracy_q1 = accuracy_score(y_test1, rf_y_pred_q1)\n",
    "rf_precision_q1 = precision_score(y_test1, rf_y_pred_q1, average='macro')\n",
    "rf_recall_q1 = recall_score(y_test1, rf_y_pred_q1, average='macro')\n",
    "rf_f1_score_q1 = f1_score(y_test1, rf_y_pred_q1, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics\n",
    "rf_model_scores_q1 = np.array([rf_accuracy_q1, rf_precision_q1, rf_recall_q1, rf_f1_score_q1])\n",
    "\n",
    "# Print the model evaluation metrics\n",
    "print('Random Forest on share_quantile_ranges Task')\n",
    "print(rf_model_scores_q1)\n",
    "print('---------------------------------------------------------')\n",
    "print('Random Forest accuracy:', rf_accuracy_q1)\n",
    "print('Random Forest precision:', rf_precision_q1)\n",
    "print('Random Forest recall:', rf_recall_q1)\n",
    "print('Random Forest F1 score:', rf_f1_score_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### day_of_week task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StandardScalar transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the day_of_week task\n",
    "rf.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for day_of_week task\n",
    "rf_y_pred2 = rf.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest on day_of_week Task\n",
      "[0.28275949 0.33304333 0.3302997  0.32854619]\n",
      "---------------------------------------------------------\n",
      "Random Forest accuracy: 0.2827594904779922\n",
      "Random Forest precision: 0.33304333466302033\n",
      "Random Forest recall: 0.3302996994121769\n",
      "Random Forest F1 score: 0.32854618784877127\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for day_of_week task.\n",
    "rf_accuracy2 = accuracy_score(y_test2, rf_y_pred2)\n",
    "rf_precision2 = precision_score(y_test2, rf_y_pred2, average='macro')\n",
    "rf_recall2 = recall_score(y_test2, rf_y_pred2, average='macro')\n",
    "rf_f1_score2 = f1_score(y_test2, rf_y_pred2, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics\n",
    "rf_model_scores2 = np.array([rf_accuracy2, rf_precision2, rf_recall2, rf_f1_score2])\n",
    "\n",
    "# Print the model evaluation metrics\n",
    "print('Random Forest on day_of_week Task')\n",
    "print(rf_model_scores2)\n",
    "print('---------------------------------------------------------')\n",
    "print('Random Forest accuracy:', rf_accuracy2)\n",
    "print('Random Forest precision:', rf_precision2)\n",
    "print('Random Forest recall:', rf_recall2)\n",
    "print('Random Forest F1 score:', rf_f1_score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QuantileTransformer transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the day_of_week task\n",
    "rf.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for day_of_week task\n",
    "rf_y_pred_q2 = rf.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest on day_of_week Task\n",
      "[0.28616471 0.34311973 0.33840983 0.33718642]\n",
      "---------------------------------------------------------\n",
      "Random Forest accuracy: 0.28616471181737924\n",
      "Random Forest precision: 0.3431197278279966\n",
      "Random Forest recall: 0.33840983190598267\n",
      "Random Forest F1 score: 0.3371864216106551\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for day_of_week task.\n",
    "rf_accuracy_q2 = accuracy_score(y_test2, rf_y_pred_q2)\n",
    "rf_precision_q2 = precision_score(y_test2, rf_y_pred_q2, average='macro')\n",
    "rf_recall_q2 = recall_score(y_test2, rf_y_pred_q2, average='macro')\n",
    "rf_f1_score_q2 = f1_score(y_test2, rf_y_pred_q2, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics\n",
    "rf_model_scores_q2 = np.array([rf_accuracy_q2, rf_precision_q2, rf_recall_q2, rf_f1_score_q2])\n",
    "\n",
    "# Print the model evaluation metrics\n",
    "print('Random Forest on day_of_week Task')\n",
    "print(rf_model_scores_q2)\n",
    "print('---------------------------------------------------------')\n",
    "print('Random Forest accuracy:', rf_accuracy_q2)\n",
    "print('Random Forest precision:', rf_precision_q2)\n",
    "print('Random Forest recall:', rf_recall_q2)\n",
    "print('Random Forest F1 score:', rf_f1_score_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### news_category task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StandardScalar transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the news_category task\n",
    "rf.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for news_category task\n",
    "rf_y_pred3 = rf.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest on news_category Task\n",
      "[0.81485685 0.8135997  0.71224579 0.71922204]\n",
      "---------------------------------------------------------\n",
      "Random Forest accuracy: 0.8148568545844369\n",
      "Random Forest precision: 0.8135996999336987\n",
      "Random Forest recall: 0.7122457894894169\n",
      "Random Forest F1 score: 0.7192220430359926\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for news_category task.\n",
    "rf_accuracy3 = accuracy_score(y_test3, rf_y_pred3)\n",
    "rf_precision3 = precision_score(y_test3, rf_y_pred3, average='macro')\n",
    "rf_recall3 = recall_score(y_test3, rf_y_pred3, average='macro')\n",
    "rf_f1_score3 = f1_score(y_test3, rf_y_pred3, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics\n",
    "rf_model_scores3 = np.array([rf_accuracy3, rf_precision3, rf_recall3, rf_f1_score3])\n",
    "\n",
    "# Print the model evaluation metrics\n",
    "print('Random Forest on news_category Task')\n",
    "print(rf_model_scores3)\n",
    "print('---------------------------------------------------------')\n",
    "print('Random Forest accuracy:', rf_accuracy3)\n",
    "print('Random Forest precision:', rf_precision3)\n",
    "print('Random Forest recall:', rf_recall3)\n",
    "print('Random Forest F1 score:', rf_f1_score3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QuantileTransformer transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the news_category task\n",
    "rf.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for news_category task\n",
    "rf_y_pred_q3 = rf.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest on news_category Task\n",
      "[0.81372178 0.81221139 0.71031349 0.71716444]\n",
      "---------------------------------------------------------\n",
      "Random Forest accuracy: 0.8137217808046412\n",
      "Random Forest precision: 0.8122113870327031\n",
      "Random Forest recall: 0.7103134865434326\n",
      "Random Forest F1 score: 0.717164441443604\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for news_category task.\n",
    "rf_accuracy_q3 = accuracy_score(y_test3, rf_y_pred_q3)\n",
    "rf_precision_q3 = precision_score(y_test3, rf_y_pred_q3, average='macro')\n",
    "rf_recall_q3 = recall_score(y_test3, rf_y_pred_q3, average='macro')\n",
    "rf_f1_score_q3 = f1_score(y_test3, rf_y_pred_q3, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics\n",
    "rf_model_scores_q3 = np.array([rf_accuracy_q3, rf_precision_q3, rf_recall_q3, rf_f1_score_q3])\n",
    "\n",
    "# Print the model evaluation metrics\n",
    "print('Random Forest on news_category Task')\n",
    "print(rf_model_scores_q3)\n",
    "print('---------------------------------------------------------')\n",
    "print('Random Forest accuracy:', rf_accuracy_q3)\n",
    "print('Random Forest precision:', rf_precision_q3)\n",
    "print('Random Forest recall:', rf_recall_q3)\n",
    "print('Random Forest F1 score:', rf_f1_score_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### share_quantile_ranges task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StandardScalar transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for the share_quantile_ranges task.\n",
    "knn.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for the share_quantile_ranges task.\n",
    "knn_y_pred1 = knn.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN on share_quantile_ranges Task\n",
      "[0.32450498 0.31773669 0.32712195 0.31448291]\n",
      "---------------------------------------------------------\n",
      "KNN accuracy: 0.3245049817127002\n",
      "KNN precision: 0.31773668549484646\n",
      "KNN recall: 0.327121953052251\n",
      "KNN F1 score: 0.3144829053052932\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the share_quantile_ranges task.\n",
    "knn_accuracy1 = accuracy_score(y_test1, knn_y_pred1)\n",
    "knn_precision1 = precision_score(y_test1, knn_y_pred1, average='macro')\n",
    "knn_recall1 = recall_score(y_test1, knn_y_pred1, average='macro')\n",
    "knn_f1_score1 = f1_score(y_test1, knn_y_pred1, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the share_quantile_ranges task.\n",
    "knn_model_scores1 = np.array([knn_accuracy1, knn_precision1, knn_recall1, knn_f1_score1])\n",
    "print('KNN on share_quantile_ranges Task')\n",
    "print(knn_model_scores1)\n",
    "print('---------------------------------------------------------')\n",
    "print('KNN accuracy:', knn_accuracy1)\n",
    "print('KNN precision:', knn_precision1)\n",
    "print('KNN recall:', knn_recall1)\n",
    "print('KNN F1 score:', knn_f1_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QuantileTransformer transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for the share_quantile_ranges task.\n",
    "knn.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for the share_quantile_ranges task.\n",
    "knn_y_pred_q1 = knn.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN on share_quantile_ranges Task\n",
      "[0.32450498 0.31773669 0.32712195 0.31448291]\n",
      "---------------------------------------------------------\n",
      "KNN accuracy: 0.3245049817127002\n",
      "KNN precision: 0.31773668549484646\n",
      "KNN recall: 0.327121953052251\n",
      "KNN F1 score: 0.3144829053052932\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the share_quantile_ranges task.\n",
    "knn_accuracy_q1 = accuracy_score(y_test1, knn_y_pred_q1)\n",
    "knn_precision_q1 = precision_score(y_test1, knn_y_pred_q1, average='macro')\n",
    "knn_recall_q1 = recall_score(y_test1, knn_y_pred_q1, average='macro')\n",
    "knn_f1_score_q1 = f1_score(y_test1, knn_y_pred_q1, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the share_quantile_ranges task.\n",
    "knn_model_scores_q1 = np.array([knn_accuracy_q1, knn_precision_q1, knn_recall_q1, knn_f1_score_q1])\n",
    "print('KNN on share_quantile_ranges Task')\n",
    "print(knn_model_scores_q1)\n",
    "print('---------------------------------------------------------')\n",
    "print('KNN accuracy:', knn_accuracy_q1)\n",
    "print('KNN precision:', knn_precision_q1)\n",
    "print('KNN recall:', knn_recall_q1)\n",
    "print('KNN F1 score:', knn_f1_score_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### day_of_week task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StandardScalar transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for the day_of_week task.\n",
    "knn.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for the day_of_week task.\n",
    "knn_y_pred2 = knn.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN on day_of_week Task\n",
      "[0.25728339 0.31427165 0.30393469 0.30641747]\n",
      "---------------------------------------------------------\n",
      "KNN accuracy: 0.25728339008702233\n",
      "KNN precision: 0.31427164806698027\n",
      "KNN recall: 0.30393469147706526\n",
      "KNN F1 score: 0.3064174676728721\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the day_of_week task.\n",
    "knn_accuracy2 = accuracy_score(y_test2, knn_y_pred2)\n",
    "knn_precision2 = precision_score(y_test2, knn_y_pred2, average='macro')\n",
    "knn_recall2 = recall_score(y_test2, knn_y_pred2, average='macro')\n",
    "knn_f1_score2 = f1_score(y_test2, knn_y_pred2, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics the day_of_week task.\n",
    "knn_model_scores2 = np.array([knn_accuracy2, knn_precision2, knn_recall2, knn_f1_score2])\n",
    "print('KNN on day_of_week Task')\n",
    "print(knn_model_scores2)\n",
    "print('---------------------------------------------------------')\n",
    "print('KNN accuracy:', knn_accuracy2)\n",
    "print('KNN precision:', knn_precision2)\n",
    "print('KNN recall:', knn_recall2)\n",
    "print('KNN F1 score:', knn_f1_score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QuantileTransformer transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for the day_of_week task.\n",
    "knn.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for the day_of_week task.\n",
    "knn_y_pred_q2 = knn.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN on day_of_week Task\n",
      "[0.25728339 0.31427165 0.30393469 0.30641747]\n",
      "---------------------------------------------------------\n",
      "KNN accuracy: 0.25728339008702233\n",
      "KNN precision: 0.31427164806698027\n",
      "KNN recall: 0.30393469147706526\n",
      "KNN F1 score: 0.3064174676728721\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the day_of_week task.\n",
    "knn_accuracy_q2 = accuracy_score(y_test2, knn_y_pred_q2)\n",
    "knn_precision_q2 = precision_score(y_test2, knn_y_pred_q2, average='macro')\n",
    "knn_recall_q2 = recall_score(y_test2, knn_y_pred_q2, average='macro')\n",
    "knn_f1_score_q2 = f1_score(y_test2, knn_y_pred_q2, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics the day_of_week task.\n",
    "knn_model_scores_q2 = np.array([knn_accuracy_q2, knn_precision_q2, knn_recall_q2, knn_f1_score_q2])\n",
    "print('KNN on day_of_week Task')\n",
    "print(knn_model_scores_q2)\n",
    "print('---------------------------------------------------------')\n",
    "print('KNN accuracy:', knn_accuracy_q2)\n",
    "print('KNN precision:', knn_precision_q2)\n",
    "print('KNN recall:', knn_recall_q2)\n",
    "print('KNN F1 score:', knn_f1_score_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### news_category task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StandardScalar transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for the news_category task.\n",
    "knn.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for the news_category task.\n",
    "knn_y_pred3 = knn.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN on news_category Task\n",
      "[0.39563627 0.35235025 0.3457044  0.34365525]\n",
      "---------------------------------------------------------\n",
      "KNN accuracy: 0.3956362719132299\n",
      "KNN precision: 0.3523502516748679\n",
      "KNN recall: 0.3457044013980392\n",
      "KNN F1 score: 0.3436552451578701\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the news_category task.\n",
    "knn_accuracy3 = accuracy_score(y_test3, knn_y_pred3)\n",
    "knn_precision3 = precision_score(y_test3, knn_y_pred3, average='macro')\n",
    "knn_recall3 = recall_score(y_test3, knn_y_pred3, average='macro')\n",
    "knn_f1_score3 = f1_score(y_test3, knn_y_pred3, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the news_category task.\n",
    "knn_model_scores3 = np.array([knn_accuracy3, knn_precision3, knn_recall3, knn_f1_score3])\n",
    "print('KNN on news_category Task')\n",
    "print(knn_model_scores3)\n",
    "print('---------------------------------------------------------')\n",
    "print('KNN accuracy:', knn_accuracy3)\n",
    "print('KNN precision:', knn_precision3)\n",
    "print('KNN recall:', knn_recall3)\n",
    "print('KNN F1 score:', knn_f1_score3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QuantileTransformer transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for the news_category task.\n",
    "knn.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for the news_category task.\n",
    "knn_y_pred_q3 = knn.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN on news_category Task\n",
      "[0.39563627 0.35235025 0.3457044  0.34365525]\n",
      "---------------------------------------------------------\n",
      "KNN accuracy: 0.3956362719132299\n",
      "KNN precision: 0.3523502516748679\n",
      "KNN recall: 0.3457044013980392\n",
      "KNN F1 score: 0.3436552451578701\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the news_category task.\n",
    "knn_accuracy_q3 = accuracy_score(y_test3, knn_y_pred_q3)\n",
    "knn_precision_q3 = precision_score(y_test3, knn_y_pred_q3, average='macro')\n",
    "knn_recall_q3 = recall_score(y_test3, knn_y_pred_q3, average='macro')\n",
    "knn_f1_score_q3 = f1_score(y_test3, knn_y_pred_q3, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the news_category task.\n",
    "knn_model_scores_q3 = np.array([knn_accuracy_q3, knn_precision_q3, knn_recall_q3, knn_f1_score_q3])\n",
    "print('KNN on news_category Task')\n",
    "print(knn_model_scores_q3)\n",
    "print('---------------------------------------------------------')\n",
    "print('KNN accuracy:', knn_accuracy_q3)\n",
    "print('KNN precision:', knn_precision_q3)\n",
    "print('KNN recall:', knn_recall_q3)\n",
    "print('KNN F1 score:', knn_f1_score_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### share_quantile_ranges task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model parameters\n",
    "C = 1.0\n",
    "kernel = 'linear'\n",
    "\n",
    "# Create the support vector machine model\n",
    "support_vector_machine_model = SVC(C=C, kernel=kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StandardScalar transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the support vector machine model on the training set using a linear kernel for the share_quantile_ranges task.\n",
    "support_vector_machine_model.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data for the share_quantile_ranges task.\n",
    "svm_y_pred1 = support_vector_machine_model.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM on share_quantile_range Task\n",
      "[0.36713331 0.36271659 0.37147229 0.35387166]\n",
      "---------------------------------------------------------\n",
      "Support vector machine accuracy: 0.36713330810947153\n",
      "Support vector machine precision: 0.3627165857563903\n",
      "Support vector machine recall: 0.3714722854438608\n",
      "Support vector machine F1 score: 0.35387166342080445\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the share_quantile_ranges task.\n",
    "svm_accuracy1 = accuracy_score(y_test1, svm_y_pred1)\n",
    "svm_precision1 = precision_score(y_test1, svm_y_pred1, average='macro')\n",
    "svm_recall1 = recall_score(y_test1, svm_y_pred1, average='macro')\n",
    "svm_f1_score1 = f1_score(y_test1, svm_y_pred1, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the share_quantile_ranges task.\n",
    "svm_model_scores1 = np.array([svm_accuracy1, svm_precision1, svm_recall1, svm_f1_score1])\n",
    "print('SVM on share_quantile_range Task')\n",
    "print(svm_model_scores1)\n",
    "print('---------------------------------------------------------')\n",
    "print('Support vector machine accuracy:', svm_accuracy1)\n",
    "print('Support vector machine precision:', svm_precision1)\n",
    "print('Support vector machine recall:', svm_recall1)\n",
    "print('Support vector machine F1 score:', svm_f1_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QuantileTransformer transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the support vector machine model on the training set using a linear kernel for the share_quantile_ranges task.\n",
    "support_vector_machine_model.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data for the share_quantile_ranges task.\n",
    "svm_y_pred_q1 = support_vector_machine_model.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM on share_quantile_range Task\n",
      "[0.36713331 0.36271659 0.37147229 0.35387166]\n",
      "---------------------------------------------------------\n",
      "Support vector machine accuracy: 0.36713330810947153\n",
      "Support vector machine precision: 0.3627165857563903\n",
      "Support vector machine recall: 0.3714722854438608\n",
      "Support vector machine F1 score: 0.35387166342080445\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the share_quantile_ranges task.\n",
    "svm_accuracy_q1 = accuracy_score(y_test1, svm_y_pred_q1)\n",
    "svm_precision_q1 = precision_score(y_test1, svm_y_pred_q1, average='macro')\n",
    "svm_recall_q1 = recall_score(y_test1, svm_y_pred_q1, average='macro')\n",
    "svm_f1_score_q1 = f1_score(y_test1, svm_y_pred_q1, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the share_quantile_ranges task.\n",
    "svm_model_scores_q1 = np.array([svm_accuracy_q1, svm_precision_q1, svm_recall_q1, svm_f1_score_q1])\n",
    "print('SVM on share_quantile_range Task')\n",
    "print(svm_model_scores_q1)\n",
    "print('---------------------------------------------------------')\n",
    "print('Support vector machine accuracy:', svm_accuracy_q1)\n",
    "print('Support vector machine precision:', svm_precision_q1)\n",
    "print('Support vector machine recall:', svm_recall_q1)\n",
    "print('Support vector machine F1 score:', svm_f1_score_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### day_of_week task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StandardScalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the support vector machine model on the training set using a linear kernel for the day_of_week task.\n",
    "support_vector_machine_model.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data for the day_of_week task.\n",
    "svm_y_pred2 = support_vector_machine_model.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM on day_of_week Task\n",
      "[0.26333712 0.27883048 0.29842274 0.27805353]\n",
      "---------------------------------------------------------\n",
      "Support vector machine accuracy: 0.2633371169125993\n",
      "Support vector machine precision: 0.27883047980987885\n",
      "Support vector machine recall: 0.29842273802114316\n",
      "Support vector machine F1 score: 0.2780535294508563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the day_of_week task.\n",
    "svm_accuracy2 = accuracy_score(y_test2, svm_y_pred2)\n",
    "svm_precision2 = precision_score(y_test2, svm_y_pred2, average='macro')\n",
    "svm_recall2 = recall_score(y_test2, svm_y_pred2, average='macro')\n",
    "svm_f1_score2 = f1_score(y_test2, svm_y_pred2, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the day_of_week task\n",
    "svm_model_scores2 = np.array([svm_accuracy2, svm_precision2, svm_recall2, svm_f1_score2])\n",
    "print('SVM on day_of_week Task')\n",
    "print(svm_model_scores2)\n",
    "print('---------------------------------------------------------')\n",
    "print('Support vector machine accuracy:', svm_accuracy2)\n",
    "print('Support vector machine precision:', svm_precision2)\n",
    "print('Support vector machine recall:', svm_recall2)\n",
    "print('Support vector machine F1 score:', svm_f1_score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the support vector machine model on the training set using a linear kernel for the day_of_week task.\n",
    "support_vector_machine_model.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data for the day_of_week task.\n",
    "svm_y_pred_q2 = support_vector_machine_model.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM on day_of_week Task\n",
      "[0.26333712 0.27883048 0.29842274 0.27805353]\n",
      "---------------------------------------------------------\n",
      "Support vector machine accuracy: 0.2633371169125993\n",
      "Support vector machine precision: 0.27883047980987885\n",
      "Support vector machine recall: 0.29842273802114316\n",
      "Support vector machine F1 score: 0.2780535294508563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the day_of_week task.\n",
    "svm_accuracy_q2 = accuracy_score(y_test2, svm_y_pred_q2)\n",
    "svm_precision_q2 = precision_score(y_test2, svm_y_pred_q2, average='macro')\n",
    "svm_recall_q2 = recall_score(y_test2, svm_y_pred_q2, average='macro')\n",
    "svm_f1_score_q2 = f1_score(y_test2, svm_y_pred_q2, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the day_of_week task\n",
    "svm_model_scores_q2 = np.array([svm_accuracy_q2, svm_precision_q2, svm_recall_q2, svm_f1_score_q2])\n",
    "print('SVM on day_of_week Task')\n",
    "print(svm_model_scores_q2)\n",
    "print('---------------------------------------------------------')\n",
    "print('Support vector machine accuracy:', svm_accuracy_q2)\n",
    "print('Support vector machine precision:', svm_precision_q2)\n",
    "print('Support vector machine recall:', svm_recall_q2)\n",
    "print('Support vector machine F1 score:', svm_f1_score_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### news_category task\n",
    "This task appears to be incompatible with the SVM model as it takes much too long to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StandardScalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the support vector machine model on the training set using a linear kernel for the news_category task.\n",
    "# support_vector_machine_model.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data for the news_category task.\n",
    "# svm_y_pred3 = support_vector_machine_model.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the news_category task.\n",
    "# svm_accuracy3 = accuracy_score(y_test3, svm_y_pred3)\n",
    "# svm_precision3 = precision_score(y_test3, svm_y_pred3, average='macro')\n",
    "# svm_recall3 = recall_score(y_test3, svm_y_pred3, average='macro')\n",
    "# svm_f1_score3 = f1_score(y_test3, svm_y_pred3, average='macro')\n",
    "\n",
    "# # Create an array to store the model evaluation metrics for the news_category task\n",
    "# svm_model_scores3 = np.array([svm_accuracy3, svm_precision3, svm_recall3, svm_f1_score3])\n",
    "# print('SVM on news_category Task')\n",
    "# print(svm_model_scores3)\n",
    "# print('---------------------------------------------------------')\n",
    "# print('Support vector machine accuracy:', svm_accuracy3)\n",
    "# print('Support vector machine precision:', svm_precision3)\n",
    "# print('Support vector machine recall:', svm_recall3)\n",
    "# print('Support vector machine F1 score:', svm_f1_score3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the support vector machine model on the training set using a linear kernel for the news_category task.\n",
    "# support_vector_machine_model.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions on the test data for the news_category task.\n",
    "# svm_y_pred_q3 = support_vector_machine_model.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the news_category task.\n",
    "# svm_accuracy_q3 = accuracy_score(y_test3, svm_y_pred_q3)\n",
    "# svm_precision_q3 = precision_score(y_test3, svm_y_pred_q3, average='macro')\n",
    "# svm_recall_q3 = recall_score(y_test3, svm_y_pred_q3, average='macro')\n",
    "# svm_f1_score_q3 = f1_score(y_test3, svm_y_pred_q3, average='macro')\n",
    "\n",
    "# # Create an array to store the model evaluation metrics for the news_category task\n",
    "# svm_model_scores_q3 = np.array([svm_accuracy_q3, svm_precision_q3, svm_recall_q3, svm_f1_score_q3])\n",
    "# print('SVM on news_category Task')\n",
    "# print(svm_model_scores_q3)\n",
    "# print('---------------------------------------------------------')\n",
    "# print('Support vector machine accuracy:', svm_accuracy_q3)\n",
    "# print('Support vector machine precision:', svm_precision_q3)\n",
    "# print('Support vector machine recall:', svm_recall_q3)\n",
    "# print('Support vector machine F1 score:', svm_f1_score_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction Using Feature Selection\n",
    "\n",
    "# # Create a SelectKBest object with k=10.\n",
    "# selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "# # Fit the selector to the data.\n",
    "# selector.fit(X, y)\n",
    "\n",
    "# # Get the selected features.\n",
    "# selected_features = selector.get_support()\n",
    "\n",
    "# # Reduce the dataset to the selected features.\n",
    "# X_reduced = X[:, selected_features]\n",
    "\n",
    "# We will want to identify the elbow point because this is the point where the rrate of change of the curve starts to rapidly decrease \n",
    "\n",
    "# # Calculate the inertia for different values of k.\n",
    "# inertias = []\n",
    "# for k in range(2, 10):\n",
    "#     kmeans = KMeans(n_clusters=k)\n",
    "#     kmeans.fit(data)\n",
    "#     inertias.append(kmeans.inertia_)\n",
    "\n",
    "# # Plot the inertia vs k.\n",
    "# plt.plot(range(2, 10), inertias, 'bx-')\n",
    "# plt.xlabel('k')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 4 (10 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the results using your chosen method of evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use visualizations of the results to bolster the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Explain any visuals and analyze why they are interesting to someone that might use this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 5 (10 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss the advantages of each model for each classification task, if any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there are not advantages, explain why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest Neighbors (KNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machine (SVM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is any model better than another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Is the difference significant with 95% confidence? Use proper statistical comparison methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### share_quantile_ranges task\n",
    "StandardScalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScalar transformed accuracy scores\n",
    "print(rf_model_scores1) # share_quantile_ranges task\n",
    "print(knn_model_scores1) # day_of_week task\n",
    "print(svm_model_scores1) # news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest vs. KNN\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n",
      "Random Forest vs. SVM\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n",
      "KNN vs. SVM\n",
      "The difference between the two models is statistically significant.\n"
     ]
    }
   ],
   "source": [
    "# Load the model accuracy scores\n",
    "model1_scores = rf_model_scores1\n",
    "model2_scores = knn_model_scores1\n",
    "model3_scores = svm_model_scores1\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model2_scores)\n",
    "\n",
    "print(\"Random Forest vs. KNN\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model1_scores, model3_scores)\n",
    "\n",
    "print(\"Random Forest vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value3 = ttest_rel(model2_scores, model3_scores)\n",
    "\n",
    "print(\"KNN vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantileTransformer transformed accuracy scores\n",
    "print(rf_model_scores_q1) # share_quantile_ranges task\n",
    "print(knn_model_scores_q1) # day_of_week task\n",
    "print(svm_model_scores_q1) # news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest vs. KNN\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n",
      "Random Forest vs. SVM\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n",
      "KNN vs. SVM\n",
      "The difference between the two models is statistically significant.\n"
     ]
    }
   ],
   "source": [
    "# Load the model accuracy scores\n",
    "model1_scores = rf_model_scores_q1\n",
    "model2_scores = knn_model_scores_q1\n",
    "model3_scores = svm_model_scores_q1\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model2_scores)\n",
    "\n",
    "print(\"Random Forest vs. KNN\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model1_scores, model3_scores)\n",
    "\n",
    "print(\"Random Forest vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value3 = ttest_rel(model2_scores, model3_scores)\n",
    "\n",
    "print(\"KNN vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### day_of_week task\n",
    "StandardScalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScalar transformed accuracy scores\n",
    "print(rf_model_scores2) # share_quantile_ranges task\n",
    "print(knn_model_scores2) # day_of_week task\n",
    "print(svm_model_scores2) # news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest vs. KNN\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n",
      "Random Forest vs. SVM\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n",
      "KNN vs. SVM\n",
      "The difference between the two models is statistically significant.\n"
     ]
    }
   ],
   "source": [
    "# Load the model accuracy scores\n",
    "model1_scores = rf_model_scores2\n",
    "model2_scores = knn_model_scores2\n",
    "model3_scores = svm_model_scores2\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model2_scores)\n",
    "\n",
    "print(\"Random Forest vs. KNN\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model1_scores, model3_scores)\n",
    "\n",
    "print(\"Random Forest vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value3 = ttest_rel(model2_scores, model3_scores)\n",
    "\n",
    "print(\"KNN vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantileTransformer transformed accuracy scores\n",
    "print(rf_model_scores_q2) # share_quantile_ranges task\n",
    "print(knn_model_scores_q2) # day_of_week task\n",
    "print(svm_model_scores_q2) # news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest vs. KNN\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n",
      "Random Forest vs. SVM\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n",
      "KNN vs. SVM\n",
      "The difference between the two models is statistically significant.\n"
     ]
    }
   ],
   "source": [
    "# Load the model accuracy scores\n",
    "model1_scores = rf_model_scores_q2\n",
    "model2_scores = knn_model_scores_q2\n",
    "model3_scores = svm_model_scores_q2\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model2_scores)\n",
    "\n",
    "print(\"Random Forest vs. KNN\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model1_scores, model3_scores)\n",
    "\n",
    "print(\"Random Forest vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value3 = ttest_rel(model2_scores, model3_scores)\n",
    "\n",
    "print(\"KNN vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### news_category task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StandardScalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScalar transformed accuracy scores\n",
    "print(rf_model_scores3) # share_quantile_ranges task\n",
    "print(knn_model_scores3) # day_of_week task\n",
    "# print(svm_model_scores_q2) # news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest vs. KNN\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the model accuracy scores\n",
    "model1_scores = rf_model_scores3\n",
    "model2_scores = knn_model_scores3\n",
    "# model3_scores = svm_model_scores3\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model2_scores)\n",
    "\n",
    "print(\"Random Forest vs. KNN\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# # Perform a paired t-test\n",
    "# t_statistic, p_value2 = ttest_rel(model1_scores, model3_scores)\n",
    "\n",
    "# print(\"Random Forest vs. SVM\")\n",
    "# # Check if the difference is significant with 95% confidence\n",
    "# if p_value2 < 0.05:\n",
    "#   print(\"The difference between the two models is statistically significant.\")\n",
    "# else:\n",
    "#   print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "# print('---------------------------------------------------------')\n",
    "\n",
    "# # Perform a paired t-test\n",
    "# t_statistic, p_value3 = ttest_rel(model2_scores, model3_scores)\n",
    "\n",
    "# print(\"KNN vs. SVM\")\n",
    "# # Check if the difference is significant with 95% confidence\n",
    "# if p_value2 < 0.05:\n",
    "#   print(\"The difference between the two models is statistically significant.\")\n",
    "# else:\n",
    "#   print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantileTransformer transformed accuracy scores\n",
    "print(rf_model_scores_q3) # share_quantile_ranges task\n",
    "print(knn_model_scores_q3) # day_of_week task\n",
    "# print(svm_model_scores_q3) # news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest vs. KNN\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the model accuracy scores\n",
    "model1_scores = rf_model_scores_q3\n",
    "model2_scores = knn_model_scores_q3\n",
    "# model3_scores = svm_model_scores_q3\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model2_scores)\n",
    "\n",
    "print(\"Random Forest vs. KNN\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# # Perform a paired t-test\n",
    "# t_statistic, p_value2 = ttest_rel(model1_scores, model3_scores)\n",
    "\n",
    "# print(\"Random Forest vs. SVM\")\n",
    "# # Check if the difference is significant with 95% confidence\n",
    "# if p_value2 < 0.05:\n",
    "#   print(\"The difference between the two models is statistically significant.\")\n",
    "# else:\n",
    "#   print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "# print('---------------------------------------------------------')\n",
    "\n",
    "# # Perform a paired t-test\n",
    "# t_statistic, p_value3 = ttest_rel(model2_scores, model3_scores)\n",
    "\n",
    "# print(\"KNN vs. SVM\")\n",
    "# # Check if the difference is significant with 95% confidence\n",
    "# if p_value2 < 0.05:\n",
    "#   print(\"The difference between the two models is statistically significant.\")\n",
    "# else:\n",
    "#   print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now for determining if there is a statistically significant difference between transformation types StandardScalar & QuantileTransformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.38907807 0.38282191 0.39315732 0.38366453]\n",
      "[0.28275949 0.33304333 0.3302997  0.32854619]\n",
      "[0.81485685 0.8135997  0.71224579 0.71922204]\n",
      "[0.38567285 0.37939086 0.3895652  0.37985307]\n",
      "[0.28616471 0.34311973 0.33840983 0.33718642]\n",
      "[0.81372178 0.81221139 0.71031349 0.71716444]\n"
     ]
    }
   ],
   "source": [
    "# StandardScalar transformed accuracy scores\n",
    "print(rf_model_scores1) # share_quantile_ranges task\n",
    "print(rf_model_scores2) # day_of_week task\n",
    "print(rf_model_scores3) # news_category task\n",
    "\n",
    "# QuantileTransformer transformed accuracy scores\n",
    "print(rf_model_scores_q1) # share_quantile_ranges task\n",
    "print(rf_model_scores_q2) # day_of_week task\n",
    "print(rf_model_scores_q3) # news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Models\n",
      "StandardScalar vs. QuantileTransformation on share_quantile_ranges task\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n",
      "StandardScalar vs. QuantileTransformation on day_of_week task\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n",
      "StandardScalar vs. QuantileTransformation on news_category task\n",
      "The difference between the two models is statistically significant.\n"
     ]
    }
   ],
   "source": [
    "# Load the model accuracy scores for QuantileTransformer transformed\n",
    "model1_scores = rf_model_scores1\n",
    "model2_scores = rf_model_scores2\n",
    "model3_scores = rf_model_scores3\n",
    "\n",
    "# Load the model accuracy scores for QuantileTransformer transformed\n",
    "model4_scores = rf_model_scores_q1\n",
    "model5_scores = rf_model_scores_q2\n",
    "model6_scores = rf_model_scores_q3\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model4_scores)\n",
    "print(\"Random Forest Models\")\n",
    "print(\"StandardScalar vs. QuantileTransformation on share_quantile_ranges task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model2_scores, model5_scores)\n",
    "\n",
    "print(\"StandardScalar vs. QuantileTransformation on day_of_week task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value3 = ttest_rel(model3_scores, model6_scores)\n",
    "\n",
    "print(\"StandardScalar vs. QuantileTransformation on news_category task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32450498 0.31773669 0.32712195 0.31448291]\n",
      "[0.25728339 0.31427165 0.30393469 0.30641747]\n",
      "[0.39563627 0.35235025 0.3457044  0.34365525]\n",
      "[0.32450498 0.31773669 0.32712195 0.31448291]\n",
      "[0.25728339 0.31427165 0.30393469 0.30641747]\n",
      "[0.39563627 0.35235025 0.3457044  0.34365525]\n"
     ]
    }
   ],
   "source": [
    "# StandardScalar transformed accuracy scores\n",
    "print(knn_model_scores1) # share_quantile_ranges task\n",
    "print(knn_model_scores2) # day_of_week task\n",
    "print(knn_model_scores3) # news_category task\n",
    "\n",
    "# QuantileTransformer transformed accuracy scores\n",
    "print(knn_model_scores_q1) # share_quantile_ranges task\n",
    "print(knn_model_scores_q2) # day_of_week task\n",
    "print(knn_model_scores_q3) # news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Models\n",
      "StandardScalar vs. QuantileTransformation on share_quantile_ranges task\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n",
      "StandardScalar vs. QuantileTransformation on day_of_week task\n",
      "The difference between the two models is statistically significant.\n",
      "---------------------------------------------------------\n",
      "StandardScalar vs. QuantileTransformation on news_category task\n",
      "The difference between the two models is statistically significant.\n"
     ]
    }
   ],
   "source": [
    "# Load the model accuracy scores for QuantileTransformer transformed\n",
    "model1_scores = rf_model_scores1\n",
    "model2_scores = rf_model_scores2\n",
    "model3_scores = rf_model_scores3\n",
    "\n",
    "# Load the model accuracy scores for QuantileTransformer transformed\n",
    "model4_scores = rf_model_scores_q1\n",
    "model5_scores = rf_model_scores_q2\n",
    "model6_scores = rf_model_scores_q3\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model4_scores)\n",
    "print(\"Random Forest Models\")\n",
    "print(\"StandardScalar vs. QuantileTransformation on share_quantile_ranges task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model2_scores, model5_scores)\n",
    "\n",
    "print(\"StandardScalar vs. QuantileTransformation on day_of_week task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value3 = ttest_rel(model3_scores, model6_scores)\n",
    "\n",
    "print(\"StandardScalar vs. QuantileTransformation on news_category task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScalar transformed accuracy scores\n",
    "print(svm_model_scores1) # share_quantile_ranges task\n",
    "print(svm_model_scores2) # day_of_week task\n",
    "# print(svm_model_scores1) # news_category task\n",
    "\n",
    "# QuantileTransformer transformed accuracy scores\n",
    "print(svm_model_scores_q1) # share_quantile_ranges task\n",
    "print(svm_model_scores_q2) # day_of_week task\n",
    "# print(svm_model_scores_q3) # news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Models\n",
      "StandardScalar vs. QuantileTransformation on share_quantile_ranges task\n",
      "The difference between the two models is not statistically significant.\n",
      "---------------------------------------------------------\n",
      "StandardScalar vs. QuantileTransformation on day_of_week task\n",
      "The difference between the two models is not statistically significant.\n",
      "---------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Load the model accuracy scores for QuantileTransformer transformed\n",
    "model1_scores = svm_model_scores1\n",
    "model2_scores = svm_model_scores2\n",
    "# model3_scores = svm_model_scores3\n",
    "\n",
    "# Load the model accuracy scores for QuantileTransformer transformed\n",
    "model4_scores = svm_model_scores_q1\n",
    "model5_scores = svm_model_scores_q2\n",
    "# model6_scores = svm_model_scores_q3\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model4_scores)\n",
    "print(\"Random Forest Models\")\n",
    "print(\"StandardScalar vs. QuantileTransformation on share_quantile_ranges task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model2_scores, model5_scores)\n",
    "\n",
    "print(\"StandardScalar vs. QuantileTransformation on day_of_week task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# # Perform a paired t-test\n",
    "# t_statistic, p_value3 = ttest_rel(model3_scores, model6_scores)\n",
    "\n",
    "# print(\"StandardScalar vs. QuantileTransformation on news_category task\")\n",
    "# # Check if the difference is significant with 95% confidence\n",
    "# if p_value2 < 0.05:\n",
    "#   print(\"The difference between the two models is statistically significant.\")\n",
    "# else:\n",
    "#   print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation 6 (10 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which attributes from your analysis are most important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use proper methods discussed in class to evaluate the importance of different attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment (5 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you measure the model's value if it was used by these parties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How would your deploy your model for interested parties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What other data should be collected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How often would the model need to be updated, etc.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exceptional Work (10 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You have free reign to provide additional modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
