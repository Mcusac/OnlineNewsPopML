{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Team Member Names\n",
    "- Name 1: Matthew D. Cusack\n",
    "- Name 2: Tim Cabaza\n",
    "- Name 3: Amy Adyanthaya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "________\n",
    "# Classification\n",
    "____\n",
    "\n",
    "## Contents\n",
    "* <a href=\"#DataPrep1\">Data Preparation Part 1</a>\n",
    "* <a href=\"#DataPrep2\">Data Preparation Part 2</a>\n",
    "* <a href=\"#ModelEval1\">Modeling and Evaluation 1</a>\n",
    "* <a href=\"#ModelEval2\">Modeling and Evaluation 2</a>\n",
    "* <a href=\"#ModelEval3\">Modeling and Evaluation 3</a>\n",
    "    * <a href=\"#RFmodel\">Random Forest Model</a>\n",
    "    * <a href=\"#KNNmodel\">KNN Model</a>\n",
    "    * <a href=\"#SVMmodel\">SVM Model</a>\n",
    "* <a href=\"#ModelEval4\">Modeling and Evaluation 4</a>\n",
    "* <a href=\"#ModelEval5\">Modeling and Evaluation 5</a>\n",
    "    * <a href=\"#TaskEval\">Comparing Task Performance Between Different Types of Models</a>\n",
    "        * <a href=\"#sqrTask\">On \"share_quantile_range\" Task</a>\n",
    "        * <a href=\"#dowTask\">On \"day_of_week\" Task</a>\n",
    "* <a href=\"#ModelEval6\">Modeling and Evaluation 6</a>\n",
    "* <a href=\"#Deployment\">Deployment</a>\n",
    "* <a href=\"#Exceptional\">Exceptional Work</a>\n",
    "    * <a href=\"#ncTask\">Comparing Task Performance Between Different Types of ModelsOn \"news_category\" Task</a>\n",
    "    * <a href=\"#ScalerEval\">Comparing the StandardScalar and QuantileTransformer Versions of The Models</a>\n",
    "        * <a href=\"#RFEval\">Random Forest Models</a>\n",
    "        * <a href=\"#KNNEval\">KNN Models</a>\n",
    "        * <a href=\"#SVMEval\">SVM Models</a>\n",
    "_______"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# this import allows you train and test you test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# this import allows you to standardize your data, scaling so that all features have a mean of zero and a standard deviation of 1. \n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "# this import allows you to create a logistic regression model; type of machine learning model that can be used for classification tasks \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# this import allows you to create a support vector machine SVM model, a type of ML model that can be used for classification tasks. \n",
    "from sklearn.svm import SVC\n",
    "# this import allows you to perform CV on your model, a technique for evaluating the performance of a ML on unseen data\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# these imports allow you to calculate various evaluation metrics for your ML model. Eval metrics are used to asses the performance of a ML on held-out test set. \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "# for testing differences with 95% confidence\n",
    "from scipy.stats import ttest_rel\n",
    "# for RandomForest models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# for KNN models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# for feature selection\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_name</th>\n",
       "      <th>date</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>news_category</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>log_shares</th>\n",
       "      <th>log_n_tokens_content</th>\n",
       "      <th>log_num_hrefs</th>\n",
       "      <th>log_num_self_hrefs</th>\n",
       "      <th>log_num_imgs</th>\n",
       "      <th>log_num_videos</th>\n",
       "      <th>log_kw_max_min</th>\n",
       "      <th>log_kw_min_max</th>\n",
       "      <th>log_kw_avg_avg</th>\n",
       "      <th>log_self_reference_min_shares</th>\n",
       "      <th>log_self_reference_max_shares</th>\n",
       "      <th>log_self_reference_avg_sharess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon-instant-video-browser/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>593</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.386879</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reeddit-reddit/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>4.546154</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.022276</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.681548</td>\n",
       "      <td>0.381987</td>\n",
       "      <td>0.152189</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.353939</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>1300</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rage-comics-dying/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>4.759494</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.199626</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.714611</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.542580</td>\n",
       "      <td>0.122370</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357269</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.338889</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1100</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.003974</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>power-matters-alliance-organization/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.535390</td>\n",
       "      <td>5.147748</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.117255</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>0.425089</td>\n",
       "      <td>0.128515</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.337965</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.225794</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1600</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.378384</td>\n",
       "      <td>6.320768</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>polaroid-android-camera/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.424132</td>\n",
       "      <td>4.631390</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.327017</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>0.506520</td>\n",
       "      <td>0.279769</td>\n",
       "      <td>0.071749</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.417055</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.212354</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2400</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.783641</td>\n",
       "      <td>7.017506</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.302619</td>\n",
       "      <td>9.680406</td>\n",
       "      <td>8.140199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               url_name        date  timedelta  \\\n",
       "0         amazon-instant-video-browser/  2013-01-07      731.0   \n",
       "1                       reeddit-reddit/  2013-01-07      731.0   \n",
       "2                    rage-comics-dying/  2013-01-07      731.0   \n",
       "3  power-matters-alliance-organization/  2013-01-07      731.0   \n",
       "4              polaroid-android-camera/  2013-01-07      731.0   \n",
       "\n",
       "   n_tokens_title  n_unique_tokens  average_token_length  num_keywords  \\\n",
       "0            12.0         0.663594              4.680365           5.0   \n",
       "1             8.0         0.821705              4.546154           9.0   \n",
       "2             9.0         0.608602              4.759494           7.0   \n",
       "3            10.0         0.535390              5.147748          10.0   \n",
       "4             9.0         0.424132              4.631390           8.0   \n",
       "\n",
       "   kw_min_min  kw_avg_min  kw_max_max  kw_avg_max  kw_min_avg  kw_max_avg  \\\n",
       "0         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   is_weekend    LDA_00    LDA_01    LDA_02    LDA_03    LDA_04  \\\n",
       "0         0.0  0.500331  0.378279  0.040005  0.041263  0.040123   \n",
       "1         0.0  0.022265  0.022446  0.022276  0.251465  0.681548   \n",
       "2         0.0  0.028575  0.199626  0.028615  0.714611  0.028572   \n",
       "3         0.0  0.020011  0.020317  0.117255  0.020007  0.822410   \n",
       "4         0.0  0.025001  0.327017  0.025001  0.025001  0.597981   \n",
       "\n",
       "   global_subjectivity  global_sentiment_polarity  global_rate_positive_words  \\\n",
       "0             0.521617                   0.092562                    0.045662   \n",
       "1             0.381987                   0.152189                    0.038462   \n",
       "2             0.542580                   0.122370                    0.063291   \n",
       "3             0.425089                   0.128515                    0.039640   \n",
       "4             0.506520                   0.279769                    0.071749   \n",
       "\n",
       "   global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "0                    0.013699             0.769231             0.230769   \n",
       "1                    0.007692             0.833333             0.166667   \n",
       "2                    0.025316             0.714286             0.285714   \n",
       "3                    0.012613             0.758621             0.241379   \n",
       "4                    0.013453             0.842105             0.157895   \n",
       "\n",
       "   avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "0               0.378636               0.100000                    0.7   \n",
       "1               0.353939               0.033333                    0.7   \n",
       "2               0.357269               0.050000                    0.6   \n",
       "3               0.337965               0.050000                    0.7   \n",
       "4               0.417055               0.100000                    1.0   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.350000                   -0.6                 -0.200   \n",
       "1              -0.400000                   -0.4                 -0.400   \n",
       "2              -0.338889                   -1.0                 -0.050   \n",
       "3              -0.225794                   -0.4                 -0.125   \n",
       "4              -0.212354                   -0.5                 -0.050   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.500000                   -0.1875                0.000000   \n",
       "1            0.250000                    0.2000                0.250000   \n",
       "2            0.650000                   -0.5000                0.150000   \n",
       "3            0.500000                   -0.1000                0.000000   \n",
       "4            0.333333                    0.2500                0.166667   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares day_of_week  news_category  year  \\\n",
       "0                        0.1875     593      Monday  Entertainment  2013   \n",
       "1                        0.2000    1300      Monday           Tech  2013   \n",
       "2                        0.5000    1100      Monday  Uncategorized  2013   \n",
       "3                        0.1000    1600      Monday           Tech  2013   \n",
       "4                        0.2500    2400      Monday           Tech  2013   \n",
       "\n",
       "   month  log_shares  log_n_tokens_content  log_num_hrefs  log_num_self_hrefs  \\\n",
       "0      1    6.386879              5.393628       1.609438            1.098612   \n",
       "1      1    7.170888              4.875197       2.079442            1.609438   \n",
       "2      1    7.003974              6.163315       2.484907            0.000000   \n",
       "3      1    7.378384              6.320768       2.079442            1.945910   \n",
       "4      1    7.783641              7.017506       3.091042            3.091042   \n",
       "\n",
       "   log_num_imgs  log_num_videos  log_kw_max_min  log_kw_min_max  \\\n",
       "0      0.693147             0.0             0.0             0.0   \n",
       "1      0.000000             0.0             0.0             0.0   \n",
       "2      0.693147             0.0             0.0             0.0   \n",
       "3      0.693147             0.0             0.0             0.0   \n",
       "4      3.044522             0.0             0.0             0.0   \n",
       "\n",
       "   log_kw_avg_avg  log_self_reference_min_shares  \\\n",
       "0             0.0                       6.208590   \n",
       "1             0.0                       7.170888   \n",
       "2             0.0                       0.000000   \n",
       "3             0.0                       7.550135   \n",
       "4             0.0                       6.302619   \n",
       "\n",
       "   log_self_reference_max_shares  log_self_reference_avg_sharess  \n",
       "0                       6.208590                        6.208590  \n",
       "1                       7.170888                        7.170888  \n",
       "2                       0.000000                        0.000000  \n",
       "3                       7.550135                        7.550135  \n",
       "4                       9.680406                        8.140199  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file path\n",
    "file_path = \"../1 - Visualization and Data Preprocessing/Data/ONPClean2.csv\" # previously cleaned\n",
    "# file_path = '../1 - Visualization and Data Preprocessing/Data/OnlineNewsPopularity.csv' # unclean\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Set the maximum number of columns to display to None\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"DataPrep1\"></a>\n",
    "# Data Preparation Part 1 [10 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15328\\2477548042.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Original (before any cleaning):\n",
    "url:        \n",
    "    Containes the url of the article with the date      \n",
    "    Object\n",
    "\n",
    "timedelta:               \n",
    "    Days between the article publication and the dataset acquisition (non-predictive)               \n",
    "    float64\n",
    "\n",
    "n_tokens_title:               \n",
    "    Number of words in the title               \n",
    "    float64\n",
    "\n",
    "n_tokens_content:               \n",
    "    Number of words in the content               \n",
    "    float64\n",
    "\n",
    "n_unique_tokens:               \n",
    "    Rate of unique words in the content               \n",
    "    float64\n",
    "\n",
    "n_non_stop_words:           \n",
    "    Rate of non-stop words in the content           \n",
    "    float64\n",
    "\n",
    "n_non_stop_unique_tokens:      \n",
    "    Rate of unique non-stop words in the content      \n",
    "    float64\n",
    "\n",
    "num_hrefs:                    \n",
    "    Number of links                 \n",
    "    float64\n",
    "\n",
    "num_self_hrefs:               \n",
    "    Number of links to other articles published by Mashable            \n",
    "    float64\n",
    "\n",
    "num_imgs:                      \n",
    "    Number of images        \n",
    "    float64\n",
    "\n",
    "num_videos:                    \n",
    "    Number of videos            \n",
    "    float64\n",
    "    \n",
    "average_token_length:               \n",
    "    Average length of the words in the content               \n",
    "    Float64\n",
    "\n",
    "num_keywords:               \n",
    "    Number of keywords in the metadata               \n",
    "    float64\n",
    "\n",
    "data_channel_is_lifestyle:     \n",
    "    Is data channel 'Lifestyle'?            \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "data_channel_is_entertainment:          \n",
    "    Is data channel 'Entertainment'?            \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "data_channel_is_bus:           \n",
    "    Is data channel 'Business'?         \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "  \n",
    "data_channel_is_socmed:        \n",
    "    Is data channel 'Social Media'?             \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "   \n",
    "data_channel_is_tech:          \n",
    "    Is data channel 'Tech'?             \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    " \n",
    "data_channel_is_world:         \n",
    "    Is data channel 'World'?        \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    " \n",
    "kw_min_min:               \n",
    "    Worst keyword (min. shares)               \n",
    "    float64\n",
    "\n",
    "kw_max_min:                    \n",
    "    Worst keyword (max. shares)         \n",
    "    float64\n",
    "\n",
    "kw_avg_min:                    \n",
    "    Worst keyword (avg. shares)               \n",
    "    float64\n",
    "\n",
    "kw_min_max:                    \n",
    "    Best keyword (min. shares)          \n",
    "    float64\n",
    "\n",
    "kw_max_max:                    \n",
    "    Best keyword (max. shares)               \n",
    "    float64\n",
    "\n",
    "kw_avg_max:                    \n",
    "    Best keyword (avg. shares)               \n",
    "    float64\n",
    "\n",
    "kw_min_avg:                    \n",
    "    Avg. keyword (min. shares)               \n",
    "    float64\n",
    "\n",
    "kw_max_avg:                    \n",
    "    Avg. keyword (max. shares)               \n",
    "    float64\n",
    "\n",
    "kw_avg_avg:                    \n",
    "    Avg. keyword (avg. shares)          \n",
    "    float64\n",
    "\n",
    "self_reference_min_shares:    \n",
    "    Min. shares of referenced articles in Mashable          \n",
    "    float64\n",
    "\n",
    "self_reference_max_shares:     \n",
    "    Max. shares of referenced articles in Mashable          \n",
    "    float64\n",
    "\n",
    "self_reference_avg_sharess:   \n",
    "    Avg. shares of referenced articles in Mashable          \n",
    "    float64\n",
    "\n",
    "weekday_is_monday:             \n",
    "    Was the article published on a Monday?          \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "weekday_is_tuesday:            \n",
    "    Was the article published on a Tuesday?             \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "weekday_is_wednesday:          \n",
    "    Was the article published on a Wednesday?               \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "weekday_is_thursday:           \n",
    "    Was the article published on a Thursday?            \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "weekday_is_friday:             \n",
    "    Was the article published on a Friday?          \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "weekday_is_saturday:           \n",
    "    Was the article published on a Saturday?            \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "weekday_is_sunday:              \n",
    "    Was the article published on a Sunday?          \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "is_weekend:                    \n",
    "    Was the article published on the weekend?               \n",
    "    Binary (Yes = 1 / No = 0)       \n",
    "    float64\n",
    "\n",
    "LDA_00:                        \n",
    "    LDA topic modeling \n",
    "    Closeness to LDA topic 0               \n",
    "    float64\n",
    "\n",
    "LDA_01:                       \n",
    "    Closeness to LDA topic 1               \n",
    "    float64\n",
    "\n",
    "LDA_02:                        \n",
    "    Closeness to LDA topic 2               \n",
    "    float64\n",
    "\n",
    "LDA_03:                       \n",
    "    Closeness to LDA topic 3               \n",
    "    float64\n",
    "\n",
    "LDA_04:                        \n",
    "    Closeness to LDA topic 4               \n",
    "    float64\n",
    "\n",
    "global_subjectivity:           \n",
    "    Text subjectivity               \n",
    "    float64\n",
    "\n",
    "global_sentiment_polarity:     \n",
    "    Text sentiment polarity               \n",
    "    float64\n",
    "\n",
    "global_rate_positive_words:    \n",
    "    Rate of positive words in the content               \n",
    "    float64\n",
    "\n",
    "global_rate_negative_words:    \n",
    "    Rate of negative words in the content               \n",
    "    float64\n",
    "\n",
    "rate_positive_words:           \n",
    "    Rate of positive words among non-neutral tokens               \n",
    "    float64\n",
    "\n",
    "rate_negative_words:           \n",
    "    Rate of negative words among non-neutral tokens               \n",
    "    float64\n",
    "\n",
    "avg_positive_polarity:         \n",
    "    Avg. polarity of positive words               \n",
    "    float64\n",
    "\n",
    "min_positive_polarity:         \n",
    "    Min. polarity of positive words               \n",
    "    float64\n",
    "\n",
    "max_positive_polarity:         \n",
    "    Max. polarity of positive words               \n",
    "    float64\n",
    "\n",
    "avg_negative_polarity:         \n",
    "    Avg. polarity of negative  words               \n",
    "    float64\n",
    "\n",
    "min_negative_polarity:         \n",
    "    Min. polarity of negative  words               \n",
    "    float64\n",
    "\n",
    "max_negative_polarity:         \n",
    "    Max. polarity of negative  words               \n",
    "    float64\n",
    "\n",
    "title_subjectivity:            \n",
    "    Title subjectivity               \n",
    "    float64\n",
    "\n",
    "title_sentiment_polarity:      \n",
    "    Title polarity               \n",
    "    float64\n",
    "\n",
    "abs_title_subjectivity:        \n",
    "    Absolute subjectivity level               \n",
    "    float64\n",
    "\n",
    "abs_title_sentiment_polarity:  \n",
    "    Absolute polarity level               \n",
    "    float64\n",
    "\n",
    "shares:                        \n",
    "    Number of shares (target)               \n",
    "    Integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Newly Created (from after preeviously done cleaning & any transformations):\n",
    "url_name:               \n",
    "    URL of the article (non-predictive)               \n",
    "    Float\n",
    "\n",
    "Date:               \n",
    "    The date the article was published               \n",
    "    DateTime\n",
    "\n",
    "Day_of_week:               \n",
    "    What day of the week the article is posted on               \n",
    "    Categorical\n",
    "\n",
    "news_category:               \n",
    "    What news category the article is               \n",
    "    Categorical\n",
    "\n",
    "Year:               \n",
    "    The year the article was published               \n",
    "    Integer\n",
    "\n",
    "Month:               \n",
    "    The month the aticle was published               \n",
    "    Integer\n",
    "\n",
    "log_shares:               \n",
    "    log of the \"shares\" variable               \n",
    "    Float\n",
    "\n",
    "log_n_tokens_content:               \n",
    "    log of the \"n_tokens_content\" variable               \n",
    "    Float\n",
    "\n",
    "log_num_hrefs:               \n",
    "    log of the \"num_hrefs\" variable               \n",
    "    Float\n",
    "\n",
    "log_num_self_hrefs:               \n",
    "    log of the \"num_self_hrefs\" variable               \n",
    "    Float\n",
    "\n",
    "log_num_imgs:               \n",
    "    log of the \"num_imgs\" variable               \n",
    "    Float\n",
    "\n",
    "log_num_videos:               \n",
    "    log of the \"num_videos\" variable               \n",
    "    Float\n",
    "\n",
    "log_kw_max_min:               \n",
    "    log of the \"kw_max_min\" variable               \n",
    "    Float\n",
    "\n",
    "log_kw_min_max:               \n",
    "    log of the \"kw_min_max\" variable               \n",
    "    Float\n",
    "\n",
    "log_kw_avg_avg:               \n",
    "    log of the \"kw_avg_avg\" variable               \n",
    "    Float\n",
    "\n",
    "log_self_reference_min_shares:               \n",
    "    log of the \"self_reference_min_shares\" variable               \n",
    "    Float\n",
    "\n",
    "log_self_reference_max_shares:               \n",
    "    log of the \"self_reference_max_shares\" variable               \n",
    "    Float\n",
    "\n",
    "log_self_reference_avg_shares:               \n",
    "    log of the \"self_reference_avg_shares\" variable               \n",
    "    Float\n",
    "\n",
    "day_of_weekX where X is the day of the week\n",
    "    a binary value meaning either Yes (1) it is day X or No (0) it is not day x\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your dataset has outliers, you may want to remove them before training your model, as outliers can skew the results of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>news_category</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>log_n_tokens_content</th>\n",
       "      <th>log_num_hrefs</th>\n",
       "      <th>log_num_self_hrefs</th>\n",
       "      <th>log_num_imgs</th>\n",
       "      <th>log_num_videos</th>\n",
       "      <th>log_kw_max_min</th>\n",
       "      <th>log_kw_min_max</th>\n",
       "      <th>log_kw_avg_avg</th>\n",
       "      <th>log_self_reference_min_shares</th>\n",
       "      <th>log_self_reference_max_shares</th>\n",
       "      <th>log_self_reference_avg_sharess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>593</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>4.546154</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.022276</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.681548</td>\n",
       "      <td>0.381987</td>\n",
       "      <td>0.152189</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.353939</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>1300</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>4.759494</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.199626</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.714611</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.542580</td>\n",
       "      <td>0.122370</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357269</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.338889</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1100</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.535390</td>\n",
       "      <td>5.147748</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.117255</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>0.425089</td>\n",
       "      <td>0.128515</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.337965</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.225794</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1600</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.320768</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.424132</td>\n",
       "      <td>4.631390</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.327017</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>0.506520</td>\n",
       "      <td>0.279769</td>\n",
       "      <td>0.071749</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.417055</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.212354</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2400</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.017506</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.302619</td>\n",
       "      <td>9.680406</td>\n",
       "      <td>8.140199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timedelta  n_tokens_title  n_unique_tokens  average_token_length  \\\n",
       "0      731.0            12.0         0.663594              4.680365   \n",
       "1      731.0             8.0         0.821705              4.546154   \n",
       "2      731.0             9.0         0.608602              4.759494   \n",
       "3      731.0            10.0         0.535390              5.147748   \n",
       "4      731.0             9.0         0.424132              4.631390   \n",
       "\n",
       "   num_keywords  kw_min_min  kw_avg_min  kw_max_max  kw_avg_max  kw_min_avg  \\\n",
       "0           5.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1           9.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2           7.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3          10.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4           8.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   kw_max_avg  is_weekend    LDA_00    LDA_01    LDA_02    LDA_03    LDA_04  \\\n",
       "0         0.0         0.0  0.500331  0.378279  0.040005  0.041263  0.040123   \n",
       "1         0.0         0.0  0.022265  0.022446  0.022276  0.251465  0.681548   \n",
       "2         0.0         0.0  0.028575  0.199626  0.028615  0.714611  0.028572   \n",
       "3         0.0         0.0  0.020011  0.020317  0.117255  0.020007  0.822410   \n",
       "4         0.0         0.0  0.025001  0.327017  0.025001  0.025001  0.597981   \n",
       "\n",
       "   global_subjectivity  global_sentiment_polarity  global_rate_positive_words  \\\n",
       "0             0.521617                   0.092562                    0.045662   \n",
       "1             0.381987                   0.152189                    0.038462   \n",
       "2             0.542580                   0.122370                    0.063291   \n",
       "3             0.425089                   0.128515                    0.039640   \n",
       "4             0.506520                   0.279769                    0.071749   \n",
       "\n",
       "   global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "0                    0.013699             0.769231             0.230769   \n",
       "1                    0.007692             0.833333             0.166667   \n",
       "2                    0.025316             0.714286             0.285714   \n",
       "3                    0.012613             0.758621             0.241379   \n",
       "4                    0.013453             0.842105             0.157895   \n",
       "\n",
       "   avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "0               0.378636               0.100000                    0.7   \n",
       "1               0.353939               0.033333                    0.7   \n",
       "2               0.357269               0.050000                    0.6   \n",
       "3               0.337965               0.050000                    0.7   \n",
       "4               0.417055               0.100000                    1.0   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.350000                   -0.6                 -0.200   \n",
       "1              -0.400000                   -0.4                 -0.400   \n",
       "2              -0.338889                   -1.0                 -0.050   \n",
       "3              -0.225794                   -0.4                 -0.125   \n",
       "4              -0.212354                   -0.5                 -0.050   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.500000                   -0.1875                0.000000   \n",
       "1            0.250000                    0.2000                0.250000   \n",
       "2            0.650000                   -0.5000                0.150000   \n",
       "3            0.500000                   -0.1000                0.000000   \n",
       "4            0.333333                    0.2500                0.166667   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares day_of_week  news_category  year  \\\n",
       "0                        0.1875     593      Monday  Entertainment  2013   \n",
       "1                        0.2000    1300      Monday           Tech  2013   \n",
       "2                        0.5000    1100      Monday  Uncategorized  2013   \n",
       "3                        0.1000    1600      Monday           Tech  2013   \n",
       "4                        0.2500    2400      Monday           Tech  2013   \n",
       "\n",
       "   month  log_n_tokens_content  log_num_hrefs  log_num_self_hrefs  \\\n",
       "0      1              5.393628       1.609438            1.098612   \n",
       "1      1              4.875197       2.079442            1.609438   \n",
       "2      1              6.163315       2.484907            0.000000   \n",
       "3      1              6.320768       2.079442            1.945910   \n",
       "4      1              7.017506       3.091042            3.091042   \n",
       "\n",
       "   log_num_imgs  log_num_videos  log_kw_max_min  log_kw_min_max  \\\n",
       "0      0.693147             0.0             0.0             0.0   \n",
       "1      0.000000             0.0             0.0             0.0   \n",
       "2      0.693147             0.0             0.0             0.0   \n",
       "3      0.693147             0.0             0.0             0.0   \n",
       "4      3.044522             0.0             0.0             0.0   \n",
       "\n",
       "   log_kw_avg_avg  log_self_reference_min_shares  \\\n",
       "0             0.0                       6.208590   \n",
       "1             0.0                       7.170888   \n",
       "2             0.0                       0.000000   \n",
       "3             0.0                       7.550135   \n",
       "4             0.0                       6.302619   \n",
       "\n",
       "   log_self_reference_max_shares  log_self_reference_avg_sharess  \n",
       "0                       6.208590                        6.208590  \n",
       "1                       7.170888                        7.170888  \n",
       "2                       0.000000                        0.000000  \n",
       "3                       7.550135                        7.550135  \n",
       "4                       9.680406                        8.140199  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove certain columns before dimensionality reduction can take place\n",
    "\n",
    "# drop certain columns\n",
    "df1 = df.drop('url_name', axis=1) # was a string and not helpful\n",
    "df1 = df1.drop('date', axis=1) # datetime change didn't work.\n",
    "df1 = df1.drop('log_shares', axis=1) # not as useful\n",
    "\n",
    "#Factor columns that need it for certain models\n",
    "# Factor the `news_category` column for other two tasks.\n",
    "# df1 = pd.get_dummies(df1, drop_first=False,columns=['news_category'])\n",
    "\n",
    "# Factor the `day_of_week` column for other two tasks.\n",
    "# df1 = pd.get_dummies(df1, columns=['day_of_week'], drop_first=False)\n",
    "\n",
    "# drop Na's\n",
    "df1.dropna()\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1-Q2    10152\n",
      "Q2-Q3     9932\n",
      "<Q1       9930\n",
      ">Q3       9630\n",
      "Name: share_quantile_ranges, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create share_quantile_ranges_variable for target\n",
    "\n",
    "# Create bins using quantiles\n",
    "q1 = df1['shares'].quantile(0.25)\n",
    "q2 = df1['shares'].quantile(0.5)\n",
    "q3 = df1['shares'].quantile(0.75)\n",
    "\n",
    "# Define the bin labels\n",
    "labels = ['<Q1', 'Q1-Q2', 'Q2-Q3', '>Q3']\n",
    "\n",
    "# Cut the shares column into bins\n",
    "df1['share_quantile_ranges'] = pd.cut(df1['shares'], bins=[0, q1, q2, q3, 1000000], labels=labels)\n",
    "\n",
    "print(df1['share_quantile_ranges'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove target\n",
    "# X = df1.drop(['share_quantile_ranges', 'day_of_week', 'news_category'], axis=1)\n",
    "# y = df1['share_quantile_ranges']\n",
    "# print(X.columns)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_state = 42\n",
    "\n",
    "# # Create a StratifiedKFold object with n_splits=10\n",
    "# kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing folds\n",
    "# X_train, X_test, y_train, y_test = [], [], [], []\n",
    "# for train_index, test_index in kfold.split(X, y):\n",
    "#     X_train.append(X[train_index])\n",
    "#     X_test.append(X[test_index])\n",
    "#     y_train.append(y[train_index])\n",
    "#     y_test.append(y[test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features in the training and testing sets using standard scalar.\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features using QuantileTransformer with n_quantiles=100 AFTER using StandardScalar().\n",
    "# quantile_transformer = QuantileTransformer(n_quantiles=100)\n",
    "\n",
    "# X_train_q = quantile_transformer.fit_transform(X_train)\n",
    "# X_test_q = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling is done on the training and testing sets \"X_train\" and \"X_test\" in order put the data on a common scale. This will helpful in improving the model performance as they  arre sensitive to the scale of the data. Scaling will be done for each classification task and both types of scaling will be done on the data seperately so that the two methods can be compared later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Certain variables were deemed unnecessary during lab 1 and were removed during the course of this labs notebook.\n",
    "\n",
    "These variables were removed:\n",
    "\n",
    "Url:        \n",
    "    Dropped as it was better served being split into multiple variables.            \n",
    "    These varriables did end up not being useful or useable, however.  \n",
    "\n",
    "n_tokens_content:           \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "n_non_stop_words:              \n",
    "    Deemed unhelpful\n",
    "\n",
    "n_non_stop_unique_tokens:           \n",
    "    Deemed unhelpful\n",
    "\n",
    "num_hrefs:          \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "num_self_hrefs:         \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "    \n",
    "num_imgs:           \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "num_videos:         \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "kw_max_min:         \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "kw_min_max:         \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "self_reference_min_shares:          \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "self_reference_max_shares:          \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "self_reference_avg_sharess:             \n",
    "    was removed after being log transformed.            \n",
    "    This helped with the very skewed data.\n",
    "\n",
    "weekday_is_X where X is the day of the week:        \n",
    "    Removed previously.         \n",
    "    This was to turn it into a categorical variable for the model being run at the time.        \n",
    "    This variable has been recreated for our current classification problems as shown above with the factoring of day_of_week.      \n",
    "\n",
    "data_channel_is_X where X is the type of data channel:      \n",
    "    Removed previously.     \n",
    "    This was to turn it into a categorical variable for the model being run at the time.        \n",
    "    This variable has been recreated for our current classification problems as shown above with the factoring of news_category.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>news_category</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>log_n_tokens_content</th>\n",
       "      <th>log_num_hrefs</th>\n",
       "      <th>log_num_self_hrefs</th>\n",
       "      <th>log_num_imgs</th>\n",
       "      <th>log_num_videos</th>\n",
       "      <th>log_kw_max_min</th>\n",
       "      <th>log_kw_min_max</th>\n",
       "      <th>log_kw_avg_avg</th>\n",
       "      <th>log_self_reference_min_shares</th>\n",
       "      <th>log_self_reference_max_shares</th>\n",
       "      <th>log_self_reference_avg_sharess</th>\n",
       "      <th>share_quantile_ranges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>593</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>&lt;Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>4.546154</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.022276</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.681548</td>\n",
       "      <td>0.381987</td>\n",
       "      <td>0.152189</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.353939</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>1300</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>Q1-Q2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>4.759494</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.199626</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.714611</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.542580</td>\n",
       "      <td>0.122370</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357269</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.338889</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1100</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Q1-Q2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.535390</td>\n",
       "      <td>5.147748</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.117255</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>0.425089</td>\n",
       "      <td>0.128515</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.337965</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.225794</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1600</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.320768</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>Q2-Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.424132</td>\n",
       "      <td>4.631390</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.327017</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>0.506520</td>\n",
       "      <td>0.279769</td>\n",
       "      <td>0.071749</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.417055</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.212354</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2400</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.017506</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.302619</td>\n",
       "      <td>9.680406</td>\n",
       "      <td>8.140199</td>\n",
       "      <td>Q2-Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timedelta  n_tokens_title  n_unique_tokens  average_token_length  \\\n",
       "0      731.0            12.0         0.663594              4.680365   \n",
       "1      731.0             8.0         0.821705              4.546154   \n",
       "2      731.0             9.0         0.608602              4.759494   \n",
       "3      731.0            10.0         0.535390              5.147748   \n",
       "4      731.0             9.0         0.424132              4.631390   \n",
       "\n",
       "   num_keywords  kw_min_min  kw_avg_min  kw_max_max  kw_avg_max  kw_min_avg  \\\n",
       "0           5.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1           9.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2           7.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3          10.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4           8.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   kw_max_avg  is_weekend    LDA_00    LDA_01    LDA_02    LDA_03    LDA_04  \\\n",
       "0         0.0         0.0  0.500331  0.378279  0.040005  0.041263  0.040123   \n",
       "1         0.0         0.0  0.022265  0.022446  0.022276  0.251465  0.681548   \n",
       "2         0.0         0.0  0.028575  0.199626  0.028615  0.714611  0.028572   \n",
       "3         0.0         0.0  0.020011  0.020317  0.117255  0.020007  0.822410   \n",
       "4         0.0         0.0  0.025001  0.327017  0.025001  0.025001  0.597981   \n",
       "\n",
       "   global_subjectivity  global_sentiment_polarity  global_rate_positive_words  \\\n",
       "0             0.521617                   0.092562                    0.045662   \n",
       "1             0.381987                   0.152189                    0.038462   \n",
       "2             0.542580                   0.122370                    0.063291   \n",
       "3             0.425089                   0.128515                    0.039640   \n",
       "4             0.506520                   0.279769                    0.071749   \n",
       "\n",
       "   global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "0                    0.013699             0.769231             0.230769   \n",
       "1                    0.007692             0.833333             0.166667   \n",
       "2                    0.025316             0.714286             0.285714   \n",
       "3                    0.012613             0.758621             0.241379   \n",
       "4                    0.013453             0.842105             0.157895   \n",
       "\n",
       "   avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "0               0.378636               0.100000                    0.7   \n",
       "1               0.353939               0.033333                    0.7   \n",
       "2               0.357269               0.050000                    0.6   \n",
       "3               0.337965               0.050000                    0.7   \n",
       "4               0.417055               0.100000                    1.0   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.350000                   -0.6                 -0.200   \n",
       "1              -0.400000                   -0.4                 -0.400   \n",
       "2              -0.338889                   -1.0                 -0.050   \n",
       "3              -0.225794                   -0.4                 -0.125   \n",
       "4              -0.212354                   -0.5                 -0.050   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.500000                   -0.1875                0.000000   \n",
       "1            0.250000                    0.2000                0.250000   \n",
       "2            0.650000                   -0.5000                0.150000   \n",
       "3            0.500000                   -0.1000                0.000000   \n",
       "4            0.333333                    0.2500                0.166667   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares day_of_week  news_category  year  \\\n",
       "0                        0.1875     593      Monday  Entertainment  2013   \n",
       "1                        0.2000    1300      Monday           Tech  2013   \n",
       "2                        0.5000    1100      Monday  Uncategorized  2013   \n",
       "3                        0.1000    1600      Monday           Tech  2013   \n",
       "4                        0.2500    2400      Monday           Tech  2013   \n",
       "\n",
       "   month  log_n_tokens_content  log_num_hrefs  log_num_self_hrefs  \\\n",
       "0      1              5.393628       1.609438            1.098612   \n",
       "1      1              4.875197       2.079442            1.609438   \n",
       "2      1              6.163315       2.484907            0.000000   \n",
       "3      1              6.320768       2.079442            1.945910   \n",
       "4      1              7.017506       3.091042            3.091042   \n",
       "\n",
       "   log_num_imgs  log_num_videos  log_kw_max_min  log_kw_min_max  \\\n",
       "0      0.693147             0.0             0.0             0.0   \n",
       "1      0.000000             0.0             0.0             0.0   \n",
       "2      0.693147             0.0             0.0             0.0   \n",
       "3      0.693147             0.0             0.0             0.0   \n",
       "4      3.044522             0.0             0.0             0.0   \n",
       "\n",
       "   log_kw_avg_avg  log_self_reference_min_shares  \\\n",
       "0             0.0                       6.208590   \n",
       "1             0.0                       7.170888   \n",
       "2             0.0                       0.000000   \n",
       "3             0.0                       7.550135   \n",
       "4             0.0                       6.302619   \n",
       "\n",
       "   log_self_reference_max_shares  log_self_reference_avg_sharess  \\\n",
       "0                       6.208590                        6.208590   \n",
       "1                       7.170888                        7.170888   \n",
       "2                       0.000000                        0.000000   \n",
       "3                       7.550135                        7.550135   \n",
       "4                       9.680406                        8.140199   \n",
       "\n",
       "  share_quantile_ranges  \n",
       "0                   <Q1  \n",
       "1                 Q1-Q2  \n",
       "2                 Q1-Q2  \n",
       "3                 Q2-Q3  \n",
       "4                 Q2-Q3  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from previously cleaned dataset\n",
    "\n",
    "# drop certain columns\n",
    "# Done above\n",
    "# df1 = df.drop('url_name', axis=1) # was a string\n",
    "# df1 = df1.drop('date', axis=1) # datetime change didn't work.\n",
    "# df1 = df1.drop('log_shares', axis=1) # not useful\n",
    "\n",
    "# drop Na's\n",
    "df1.dropna()\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url_name:           \n",
    "    Dropped due to it being a string variable that wasn't useful\n",
    "\n",
    "date:\n",
    "    Dropped due to it being a datetime variable that wasn't useful\n",
    "\n",
    "log_shares:\n",
    "    Dropped due to it being a datetime variable that wasn't useful.             \n",
    "    The share_quantile_ranges was deemed to be more useful.\n",
    "\n",
    "All Na's were dropped (how many?)\n",
    "\n",
    "The shares, day_of_week, and news_category variables will be removed when selecting the target variable further down. Variables related to the current task will also be removed such as 'shares', 'day_of_week_Monday' (for example), and news_category_World (for example). This is to avoid these variables essentially giving the model the right answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the data\n",
    "X = df1.drop(['share_quantile_ranges', 'shares', 'day_of_week', 'news_category'], axis=1) \n",
    "y = df1['share_quantile_ranges']\n",
    "\n",
    "# Initialize the sequential feature selector\n",
    "sfs = SequentialFeatureSelector(estimator=LogisticRegression(), scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "# Fit the selector to the data\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = sfs.get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = X.columns[selected_features]\n",
    "\n",
    "# Print the column names of the selected features\n",
    "print(selected_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using feature selection with a LogisticRegression estimator several variables were able to be removed. The following variables were deemed unimportant to the model:\n",
    "\n",
    "    'timedelta'\n",
    "    'n_tokens_title'\n",
    "    'average_token_length'   \n",
    "    'num_keywords'\n",
    "    'kw_min_min'\n",
    "    'kw_avg_min'\n",
    "    'kw_max_max'\n",
    "    'kw_avg_max' \n",
    "    'kw_min_avg'\n",
    "    'kw_max_avg'\n",
    "    'LDA_00'\n",
    "    'LDA_04'\n",
    "    'global_sentiment_polarity'\n",
    "    'rate_negative_words'\n",
    "    'title_subjectivity'\n",
    "    'shares'\n",
    "    'month'\n",
    "    'log_n_tokens_content'\n",
    "    'log_num_hrefs'  \n",
    "    'log_num_self_hrefs'\n",
    "    'log_num_imgs'\n",
    "    'log_num_videos'\n",
    "    'log_kw_max_min'\n",
    "    'log_self_reference_min_shares'\n",
    "\n",
    "##### Using feature selection with a LogisticRegression estimator the following variables were deemed most important to the model:\n",
    "    'n_unique_tokens'\n",
    "    'is_weekend'\n",
    "    'LDA_01'\n",
    "    'LDA_02'\n",
    "    'LDA_03'\n",
    "    'global_subjectivity'\n",
    "    'global_rate_positive_words'\n",
    "    'global_rate_negative_words'\n",
    "    'rate_positive_words'\n",
    "    'avg_positive_polarity\n",
    "    'min_positive_polarity'\n",
    "    'max_positive_polarity'\n",
    "    'avg_negative_polarity'\n",
    "    'min_negative_polarity'\n",
    "    'max_negative_polarity'\n",
    "    'title_sentiment_polarity'\n",
    "    'abs_title_subjectivity'\n",
    "    'abs_title_sentiment_polarity\n",
    "    'year'\n",
    "    'log_kw_min_max'\n",
    "    'log_kw_avg_avg'\n",
    "    'log_self_reference_max_shares'\n",
    "    'log_self_reference_avg_sharess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X = df1.drop(['share_quantile_ranges', 'shares', 'day_of_week', 'news_category'], axis=1) \n",
    "y = df1['share_quantile_ranges']\n",
    "\n",
    "# Initialize the sequential feature selector\n",
    "sfs = SequentialFeatureSelector(estimator=RandomForestClassifier(), scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "# Fit the selector to the data\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = sfs.get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = X.columns[selected_features]\n",
    "\n",
    "# Print the column names of the selected features\n",
    "print(selected_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using feature selection with a RandomForest estimator several variables were able to be removed. The following variables were deemed unimportant to the model:\n",
    "\n",
    "    'timedelta'\n",
    "    'n_tokens_title'\n",
    "    'average_token_length'\n",
    "    'kw_min_min'\n",
    "    'kw_max_max'\n",
    "    'kw_avg_max'\n",
    "    'kw_max_avg'\n",
    "    'LDA_01'\n",
    "    'LDA_02'\n",
    "    'global_rate_negative_words'\n",
    "    'rate_positive_words'\n",
    "    'rate_negative_words'\n",
    "    'min_positive_polarity'\n",
    "    'avg_negative_polarity'\n",
    "    'min_negative_polarity'\n",
    "    'max_negative_polarity'\n",
    "    'title_sentiment_polarity'\n",
    "    'abs_title_subjectivity'\n",
    "    'shares'\n",
    "    'year'\n",
    "    'month'\n",
    "    'log_num_videos'\n",
    "    'log_kw_min_max'\n",
    "    'log_self_reference_max_shares'\n",
    "\n",
    "##### Using feature selection with a RandomForest estimator the following variables were deemed most important to the model:\n",
    "    'n_unique_tokens'\n",
    "    'num_keywords'\n",
    "    'kw_avg_min'\n",
    "    'kw_min_avg'\n",
    "    'is_weekend'\n",
    "    'LDA_00'\n",
    "    'LDA_03'\n",
    "    'LDA_04'\n",
    "    'global_subjectivity'\n",
    "    'global_sentiment_polarity'\n",
    "    'global_rate_positive_words'\n",
    "    'avg_positive_polarity'\n",
    "    'max_positive_polarity'\n",
    "    'title_subjectivity'\n",
    "    'abs_title_sentiment_polarity'\n",
    "    'log_n_tokens_content'\n",
    "    'log_num_hrefs'\n",
    "    'log_num_self_hrefs'\n",
    "    'log_num_imgs'\n",
    "    'log_kw_max_min'\n",
    "    'log_kw_avg_avg'\n",
    "    'log_self_reference_min_shares'\n",
    "    'log_self_reference_avg_sharess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X = df1.drop(['share_quantile_ranges', 'shares', 'day_of_week', 'news_category'], axis=1) \n",
    "y = df1['share_quantile_ranges']\n",
    "\n",
    "# Initialize the sequential feature selector\n",
    "sfs = SequentialFeatureSelector(estimator=KNeighborsClassifier(), scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "# Fit the selector to the data\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = sfs.get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = X.columns[selected_features]\n",
    "\n",
    "# Print the column names of the selected features\n",
    "print(selected_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using feature selection with a KNN estimator several variables were able to be removed. The following variables were deemed unimportant to the model:\n",
    "\n",
    "    'timedelta'\n",
    "    'n_tokens_title'\n",
    "    'average_token_length'\n",
    "    'num_keywords'\n",
    "    'kw_min_min'\n",
    "    'kw_avg_min'\n",
    "    'kw_max_max'\n",
    "    'kw_avg_max'\n",
    "    'kw_min_avg'\n",
    "    'kw_max_avg'\n",
    "    'max_positive_polarity'\n",
    "    'avg_negative_polarity'\n",
    "    'min_negative_polarity'\n",
    "    'title_subjectivity'\n",
    "    'title_sentiment_polarity'\n",
    "    'abs_title_subjectivity'\n",
    "    'shares'\n",
    "    'year'\n",
    "    'month'\n",
    "    'log_num_self_hrefs'\n",
    "    'log_num_imgs'\n",
    "    'log_kw_max_min'\n",
    "    'log_kw_min_max'\n",
    "    'log_self_reference_min_shares'\n",
    "\n",
    "##### Using feature selection with a KNN estimator the following variables were deemed most important to the model:\n",
    "    'n_unique_tokens'\n",
    "    'is_weekend'\n",
    "    'LDA_00'\n",
    "    'LDA_01'\n",
    "    'LDA_02'\n",
    "    'LDA_03'\n",
    "    'LDA_04'\n",
    "    'global_subjectivity'\n",
    "    'global_sentiment_polarity',\n",
    "    'global_rate_positive_words'\n",
    "    'global_rate_negative_words'\n",
    "    'rate_positive_words'\n",
    "    'rate_negative_words'\n",
    "    'avg_positive_polarity'\n",
    "    'min_positive_polarity'\n",
    "    'max_negative_polarity'\n",
    "    'abs_title_sentiment_polarity'\n",
    "    'log_n_tokens_content'\n",
    "    'log_num_hrefs'\n",
    "    'log_num_videos'\n",
    "    'log_kw_avg_avg'\n",
    "    'log_self_reference_max_shares'\n",
    "    'log_self_reference_avg_sharess'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X = df1.drop(['share_quantile_ranges', 'shares', 'day_of_week', 'news_category'], axis=1) \n",
    "y = df1['share_quantile_ranges']\n",
    "\n",
    "# Initialize the sequential feature selector\n",
    "sfs = SequentialFeatureSelector(estimator=SVC(C=1, kernel=\"linear\"), scoring='f1_macro', n_jobs=-1)\n",
    "\n",
    "# Fit the selector to the data\n",
    "sfs.fit(X, y)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = sfs.get_support(indices=True)\n",
    "\n",
    "# Get the column names of the selected features\n",
    "selected_feature_names = X.columns[selected_features]\n",
    "\n",
    "# Print the column names of the selected features\n",
    "print(selected_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using feature selection with a SVM estimator several variables were able to be removed. The following variables were deemed unimportant to the model:\n",
    "\n",
    "    'timedelta'\n",
    "    'n_tokens_title'\n",
    "    'n_unique_tokens'\n",
    "    'average_token_length'\n",
    "    'num_keywords'\n",
    "    'kw_min_min'\n",
    "    'kw_avg_min'\n",
    "    'kw_max_max'\n",
    "    'kw_avg_max'\n",
    "    'kw_min_avg'\n",
    "    'kw_max_avg'\n",
    "    'is_weekend'\n",
    "    'LDA_00'\n",
    "    'LDA_01'\n",
    "    'LDA_02'\n",
    "    'LDA_03'\n",
    "    'LDA_04'\n",
    "    'global_subjectivity'\n",
    "    'global_sentiment_polarity'\n",
    "    'global_rate_positive_words'\n",
    "    'global_rate_negative_words'\n",
    "    'rate_positive_words'\n",
    "    'rate_negative_words'\n",
    "    'avg_positive_polarity'\n",
    "    'min_positive_polarity'\n",
    "    'max_positive_polarity'\n",
    "    'avg_negative_polarity'\n",
    "    'min_negative_polarity'\n",
    "    'max_negative_polarity'\n",
    "    'title_subjectivity'\n",
    "    'title_sentiment_polarity'\n",
    "    'abs_title_subjectivity'\n",
    "    'abs_title_sentiment_polarity'\n",
    "    'shares'\n",
    "    'year'\n",
    "    'month'\n",
    "    'log_n_tokens_content'\n",
    "    'log_num_hrefs'\n",
    "    'log_num_self_hrefs'\n",
    "    'log_num_imgs'\n",
    "    'log_num_videos'\n",
    "    'log_kw_max_min'\n",
    "    'log_kw_min_max'\n",
    "    'log_kw_avg_avg'\n",
    "    'log_self_reference_min_shares'\n",
    "    'log_self_reference_max_shares'\n",
    "    'log_self_reference_avg_sharess'\n",
    "\n",
    "\n",
    "##### Using feature selection with a SVM estimator the following variables were deemed most important to the model:\n",
    "After 20 hours the feature selection had still not returned any results. Due to this, we will be using a mix of previous selected features to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"DataPrep2\"></a>\n",
    "# Data Preperation Part 2 [5 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describe the final dataset that is used for classification/regression\n",
    "(include a description of any newly formed variables you created)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### share_quantile_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1-Q2    10152\n",
      "Q2-Q3     9932\n",
      "<Q1       9930\n",
      ">Q3       9630\n",
      "Name: share_quantile_ranges, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # Create bins using quantiles\n",
    "# q1 = df1['shares'].quantile(0.25)\n",
    "# q2 = df1['shares'].quantile(0.5)\n",
    "# q3 = df1['shares'].quantile(0.75)\n",
    "\n",
    "# # Define the bin labels\n",
    "# labels = ['<Q1', 'Q1-Q2', 'Q2-Q3', '>Q3']\n",
    "\n",
    "# # Cut the shares column into bins\n",
    "# df1['share_quantile_ranges'] = pd.cut(df1['shares'], bins=[0, q1, q2, q3, 1000000], labels=labels)\n",
    "\n",
    "# Print the value counts of the share_ranges_quantile column\n",
    "print(df1['share_quantile_ranges'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "share_quantile_ranges: This was created by splitting up the shares variable into bins by using its different quantile values. This allowed for a more even split among the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### factoring day_of_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Factor the `day_of_week` column.\n",
    "# df1 = pd.get_dummies(df1, columns=['day_of_week'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "factored day_of_week columns: day_of_week was factored back into 7 columns of 0's and 1's similar to how it was in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### factoring news_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Factor the `news_category` column.\n",
    "# df1 = pd.get_dummies(df1, columns=['news_category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "factored news_category columns: day_of_week was factored back into 6 columns of 0's and 1's representative if it is that day or not similar to how it was in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>log_n_tokens_content</th>\n",
       "      <th>log_num_hrefs</th>\n",
       "      <th>log_num_self_hrefs</th>\n",
       "      <th>log_num_imgs</th>\n",
       "      <th>log_num_videos</th>\n",
       "      <th>log_kw_max_min</th>\n",
       "      <th>log_kw_min_max</th>\n",
       "      <th>log_kw_avg_avg</th>\n",
       "      <th>log_self_reference_min_shares</th>\n",
       "      <th>log_self_reference_max_shares</th>\n",
       "      <th>log_self_reference_avg_sharess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "      <td>39644.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>354.530471</td>\n",
       "      <td>10.398749</td>\n",
       "      <td>0.548216</td>\n",
       "      <td>4.548239</td>\n",
       "      <td>7.223767</td>\n",
       "      <td>26.106801</td>\n",
       "      <td>312.366967</td>\n",
       "      <td>752324.066694</td>\n",
       "      <td>259281.938083</td>\n",
       "      <td>1117.146610</td>\n",
       "      <td>5657.211151</td>\n",
       "      <td>0.130915</td>\n",
       "      <td>0.184599</td>\n",
       "      <td>0.141256</td>\n",
       "      <td>0.216321</td>\n",
       "      <td>0.223770</td>\n",
       "      <td>0.234029</td>\n",
       "      <td>0.443370</td>\n",
       "      <td>0.119309</td>\n",
       "      <td>0.039625</td>\n",
       "      <td>0.016612</td>\n",
       "      <td>0.682150</td>\n",
       "      <td>0.287934</td>\n",
       "      <td>0.353825</td>\n",
       "      <td>0.095446</td>\n",
       "      <td>0.756728</td>\n",
       "      <td>-0.259524</td>\n",
       "      <td>-0.521944</td>\n",
       "      <td>-0.107500</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.071425</td>\n",
       "      <td>0.341843</td>\n",
       "      <td>0.156064</td>\n",
       "      <td>3395.380184</td>\n",
       "      <td>2013.540939</td>\n",
       "      <td>6.615856</td>\n",
       "      <td>5.889971</td>\n",
       "      <td>2.156564</td>\n",
       "      <td>1.208878</td>\n",
       "      <td>1.116427</td>\n",
       "      <td>0.400420</td>\n",
       "      <td>6.393888</td>\n",
       "      <td>5.045209</td>\n",
       "      <td>7.976327</td>\n",
       "      <td>6.195185</td>\n",
       "      <td>6.917477</td>\n",
       "      <td>6.667697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>214.163767</td>\n",
       "      <td>2.114037</td>\n",
       "      <td>3.520708</td>\n",
       "      <td>0.844406</td>\n",
       "      <td>1.909130</td>\n",
       "      <td>69.633215</td>\n",
       "      <td>620.783887</td>\n",
       "      <td>214502.129573</td>\n",
       "      <td>135102.247285</td>\n",
       "      <td>1137.456951</td>\n",
       "      <td>6098.871957</td>\n",
       "      <td>0.337312</td>\n",
       "      <td>0.262975</td>\n",
       "      <td>0.219707</td>\n",
       "      <td>0.282145</td>\n",
       "      <td>0.295191</td>\n",
       "      <td>0.289183</td>\n",
       "      <td>0.116685</td>\n",
       "      <td>0.096931</td>\n",
       "      <td>0.017429</td>\n",
       "      <td>0.010828</td>\n",
       "      <td>0.190206</td>\n",
       "      <td>0.156156</td>\n",
       "      <td>0.104542</td>\n",
       "      <td>0.071315</td>\n",
       "      <td>0.247786</td>\n",
       "      <td>0.127726</td>\n",
       "      <td>0.290290</td>\n",
       "      <td>0.095373</td>\n",
       "      <td>0.324247</td>\n",
       "      <td>0.265450</td>\n",
       "      <td>0.188791</td>\n",
       "      <td>0.226294</td>\n",
       "      <td>11626.950749</td>\n",
       "      <td>0.498327</td>\n",
       "      <td>3.390683</td>\n",
       "      <td>1.255442</td>\n",
       "      <td>0.809445</td>\n",
       "      <td>0.692698</td>\n",
       "      <td>0.973755</td>\n",
       "      <td>0.680486</td>\n",
       "      <td>1.311168</td>\n",
       "      <td>4.521016</td>\n",
       "      <td>0.489467</td>\n",
       "      <td>3.076913</td>\n",
       "      <td>3.432430</td>\n",
       "      <td>3.280186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.393750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>164.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.470870</td>\n",
       "      <td>4.478404</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>141.750000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>172846.875000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3562.101631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>0.025012</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.028574</td>\n",
       "      <td>0.396167</td>\n",
       "      <td>0.057757</td>\n",
       "      <td>0.028384</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.306244</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-0.328383</td>\n",
       "      <td>-0.700000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>946.000000</td>\n",
       "      <td>2013.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.509388</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.100319</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.776304</td>\n",
       "      <td>6.461468</td>\n",
       "      <td>7.003974</td>\n",
       "      <td>6.889782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>339.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.539226</td>\n",
       "      <td>4.664082</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>235.500000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>244572.222223</td>\n",
       "      <td>1023.635611</td>\n",
       "      <td>4355.688836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>0.033345</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.040001</td>\n",
       "      <td>0.040727</td>\n",
       "      <td>0.453457</td>\n",
       "      <td>0.119117</td>\n",
       "      <td>0.039023</td>\n",
       "      <td>0.015337</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.358755</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.253333</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1400.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.016157</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.493754</td>\n",
       "      <td>7.244942</td>\n",
       "      <td>7.962442</td>\n",
       "      <td>7.090910</td>\n",
       "      <td>7.937732</td>\n",
       "      <td>7.696667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>542.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>4.854839</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>357.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>330980.000000</td>\n",
       "      <td>2056.781032</td>\n",
       "      <td>6019.953968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.240958</td>\n",
       "      <td>0.150831</td>\n",
       "      <td>0.334218</td>\n",
       "      <td>0.375763</td>\n",
       "      <td>0.399986</td>\n",
       "      <td>0.508333</td>\n",
       "      <td>0.177832</td>\n",
       "      <td>0.050279</td>\n",
       "      <td>0.021739</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.411428</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.186905</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>2800.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.575076</td>\n",
       "      <td>2.708050</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>6.908755</td>\n",
       "      <td>8.974745</td>\n",
       "      <td>8.189031</td>\n",
       "      <td>7.863651</td>\n",
       "      <td>8.987322</td>\n",
       "      <td>8.556606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>731.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>701.000000</td>\n",
       "      <td>8.041534</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>377.000000</td>\n",
       "      <td>42827.857143</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>3613.039819</td>\n",
       "      <td>298400.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.926994</td>\n",
       "      <td>0.925947</td>\n",
       "      <td>0.919999</td>\n",
       "      <td>0.926534</td>\n",
       "      <td>0.927191</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.727841</td>\n",
       "      <td>0.155488</td>\n",
       "      <td>0.184932</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>843300.000000</td>\n",
       "      <td>2014.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>9.044876</td>\n",
       "      <td>5.720312</td>\n",
       "      <td>4.762174</td>\n",
       "      <td>4.859812</td>\n",
       "      <td>4.521789</td>\n",
       "      <td>12.606193</td>\n",
       "      <td>13.645079</td>\n",
       "      <td>10.682093</td>\n",
       "      <td>13.645079</td>\n",
       "      <td>13.645079</td>\n",
       "      <td>13.645079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          timedelta  n_tokens_title  n_unique_tokens  average_token_length  \\\n",
       "count  39644.000000    39644.000000     39644.000000          39644.000000   \n",
       "mean     354.530471       10.398749         0.548216              4.548239   \n",
       "std      214.163767        2.114037         3.520708              0.844406   \n",
       "min        8.000000        2.000000         0.000000              0.000000   \n",
       "25%      164.000000        9.000000         0.470870              4.478404   \n",
       "50%      339.000000       10.000000         0.539226              4.664082   \n",
       "75%      542.000000       12.000000         0.608696              4.854839   \n",
       "max      731.000000       23.000000       701.000000              8.041534   \n",
       "\n",
       "       num_keywords    kw_min_min    kw_avg_min     kw_max_max     kw_avg_max  \\\n",
       "count  39644.000000  39644.000000  39644.000000   39644.000000   39644.000000   \n",
       "mean       7.223767     26.106801    312.366967  752324.066694  259281.938083   \n",
       "std        1.909130     69.633215    620.783887  214502.129573  135102.247285   \n",
       "min        1.000000     -1.000000     -1.000000       0.000000       0.000000   \n",
       "25%        6.000000     -1.000000    141.750000  843300.000000  172846.875000   \n",
       "50%        7.000000     -1.000000    235.500000  843300.000000  244572.222223   \n",
       "75%        9.000000      4.000000    357.000000  843300.000000  330980.000000   \n",
       "max       10.000000    377.000000  42827.857143  843300.000000  843300.000000   \n",
       "\n",
       "         kw_min_avg     kw_max_avg    is_weekend        LDA_00        LDA_01  \\\n",
       "count  39644.000000   39644.000000  39644.000000  39644.000000  39644.000000   \n",
       "mean    1117.146610    5657.211151      0.130915      0.184599      0.141256   \n",
       "std     1137.456951    6098.871957      0.337312      0.262975      0.219707   \n",
       "min       -1.000000       0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000    3562.101631      0.000000      0.025051      0.025012   \n",
       "50%     1023.635611    4355.688836      0.000000      0.033387      0.033345   \n",
       "75%     2056.781032    6019.953968      0.000000      0.240958      0.150831   \n",
       "max     3613.039819  298400.000000      1.000000      0.926994      0.925947   \n",
       "\n",
       "             LDA_02        LDA_03        LDA_04  global_subjectivity  \\\n",
       "count  39644.000000  39644.000000  39644.000000         39644.000000   \n",
       "mean       0.216321      0.223770      0.234029             0.443370   \n",
       "std        0.282145      0.295191      0.289183             0.116685   \n",
       "min        0.000000      0.000000      0.000000             0.000000   \n",
       "25%        0.028571      0.028571      0.028574             0.396167   \n",
       "50%        0.040004      0.040001      0.040727             0.453457   \n",
       "75%        0.334218      0.375763      0.399986             0.508333   \n",
       "max        0.919999      0.926534      0.927191             1.000000   \n",
       "\n",
       "       global_sentiment_polarity  global_rate_positive_words  \\\n",
       "count               39644.000000                39644.000000   \n",
       "mean                    0.119309                    0.039625   \n",
       "std                     0.096931                    0.017429   \n",
       "min                    -0.393750                    0.000000   \n",
       "25%                     0.057757                    0.028384   \n",
       "50%                     0.119117                    0.039023   \n",
       "75%                     0.177832                    0.050279   \n",
       "max                     0.727841                    0.155488   \n",
       "\n",
       "       global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "count                39644.000000         39644.000000         39644.000000   \n",
       "mean                     0.016612             0.682150             0.287934   \n",
       "std                      0.010828             0.190206             0.156156   \n",
       "min                      0.000000             0.000000             0.000000   \n",
       "25%                      0.009615             0.600000             0.185185   \n",
       "50%                      0.015337             0.710526             0.280000   \n",
       "75%                      0.021739             0.800000             0.384615   \n",
       "max                      0.184932             1.000000             1.000000   \n",
       "\n",
       "       avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "count           39644.000000           39644.000000           39644.000000   \n",
       "mean                0.353825               0.095446               0.756728   \n",
       "std                 0.104542               0.071315               0.247786   \n",
       "min                 0.000000               0.000000               0.000000   \n",
       "25%                 0.306244               0.050000               0.600000   \n",
       "50%                 0.358755               0.100000               0.800000   \n",
       "75%                 0.411428               0.100000               1.000000   \n",
       "max                 1.000000               1.000000               1.000000   \n",
       "\n",
       "       avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "count           39644.000000           39644.000000           39644.000000   \n",
       "mean               -0.259524              -0.521944              -0.107500   \n",
       "std                 0.127726               0.290290               0.095373   \n",
       "min                -1.000000              -1.000000              -1.000000   \n",
       "25%                -0.328383              -0.700000              -0.125000   \n",
       "50%                -0.253333              -0.500000              -0.100000   \n",
       "75%                -0.186905              -0.300000              -0.050000   \n",
       "max                 0.000000               0.000000               0.000000   \n",
       "\n",
       "       title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "count        39644.000000              39644.000000            39644.000000   \n",
       "mean             0.282353                  0.071425                0.341843   \n",
       "std              0.324247                  0.265450                0.188791   \n",
       "min              0.000000                 -1.000000                0.000000   \n",
       "25%              0.000000                  0.000000                0.166667   \n",
       "50%              0.150000                  0.000000                0.500000   \n",
       "75%              0.500000                  0.150000                0.500000   \n",
       "max              1.000000                  1.000000                0.500000   \n",
       "\n",
       "       abs_title_sentiment_polarity         shares          year  \\\n",
       "count                  39644.000000   39644.000000  39644.000000   \n",
       "mean                       0.156064    3395.380184   2013.540939   \n",
       "std                        0.226294   11626.950749      0.498327   \n",
       "min                        0.000000       1.000000   2013.000000   \n",
       "25%                        0.000000     946.000000   2013.000000   \n",
       "50%                        0.000000    1400.000000   2014.000000   \n",
       "75%                        0.250000    2800.000000   2014.000000   \n",
       "max                        1.000000  843300.000000   2014.000000   \n",
       "\n",
       "              month  log_n_tokens_content  log_num_hrefs  log_num_self_hrefs  \\\n",
       "count  39644.000000          39644.000000   39644.000000        39644.000000   \n",
       "mean       6.615856              5.889971       2.156564            1.208878   \n",
       "std        3.390683              1.255442       0.809445            0.692698   \n",
       "min        1.000000              0.000000       0.000000            0.000000   \n",
       "25%        4.000000              5.509388       1.609438            0.693147   \n",
       "50%        7.000000              6.016157       2.197225            1.386294   \n",
       "75%       10.000000              6.575076       2.708050            1.609438   \n",
       "max       12.000000              9.044876       5.720312            4.762174   \n",
       "\n",
       "       log_num_imgs  log_num_videos  log_kw_max_min  log_kw_min_max  \\\n",
       "count  39644.000000    39644.000000    39644.000000    39644.000000   \n",
       "mean       1.116427        0.400420        6.393888        5.045209   \n",
       "std        0.973755        0.680486        1.311168        4.521016   \n",
       "min        0.000000        0.000000        0.000000        0.000000   \n",
       "25%        0.693147        0.000000        6.100319        0.000000   \n",
       "50%        0.693147        0.000000        6.493754        7.244942   \n",
       "75%        1.609438        0.693147        6.908755        8.974745   \n",
       "max        4.859812        4.521789       12.606193       13.645079   \n",
       "\n",
       "       log_kw_avg_avg  log_self_reference_min_shares  \\\n",
       "count    39644.000000                   39644.000000   \n",
       "mean         7.976327                       6.195185   \n",
       "std          0.489467                       3.076913   \n",
       "min          0.000000                       0.000000   \n",
       "25%          7.776304                       6.461468   \n",
       "50%          7.962442                       7.090910   \n",
       "75%          8.189031                       7.863651   \n",
       "max         10.682093                      13.645079   \n",
       "\n",
       "       log_self_reference_max_shares  log_self_reference_avg_sharess  \n",
       "count                   39644.000000                    39644.000000  \n",
       "mean                        6.917477                        6.667697  \n",
       "std                         3.432430                        3.280186  \n",
       "min                         0.000000                        0.000000  \n",
       "25%                         7.003974                        6.889782  \n",
       "50%                         7.937732                        7.696667  \n",
       "75%                         8.987322                        8.556606  \n",
       "max                        13.645079                       13.645079  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"ModelEval1\"></a>\n",
    "# Modeling and Evaluation 1 (10 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score was chosen as our most important evaluation metric for analyzing the results of the modeling.\n",
    "\n",
    "F1 Score balances both precision and recall on the positive class which is ideal for this type of classification problem that predicts popularity based off shares, day of week, or news category as it is easy to interpret and communicate to our stakeholder Mashable. \n",
    "\n",
    "Accuracy is also tracked as it does provide correctly classified observations and it is always important to note but it is not the most significant metric in our final analysis of the models.\n",
    "\n",
    "Overall F1 the best metric to avoid overfitting issues due to any imbalances in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"ModelEval2\"></a>\n",
    "# Modeling and Evaluation 2 (10 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing your data into training and testing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_state = 42\n",
    "\n",
    "# Create a StratifiedKFold object with n_splits=10\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state = random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "share_quantile_ranges task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timedelta', 'n_tokens_title', 'n_unique_tokens',\n",
      "       'average_token_length', 'num_keywords', 'kw_min_min', 'kw_avg_min',\n",
      "       'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'is_weekend',\n",
      "       'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
      "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
      "       'global_rate_negative_words', 'rate_positive_words',\n",
      "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
      "       'max_positive_polarity', 'avg_negative_polarity',\n",
      "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
      "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
      "       'abs_title_sentiment_polarity', 'year', 'month', 'log_n_tokens_content',\n",
      "       'log_num_hrefs', 'log_num_self_hrefs', 'log_num_imgs', 'log_num_videos',\n",
      "       'log_kw_max_min', 'log_kw_min_max', 'log_kw_avg_avg',\n",
      "       'log_self_reference_min_shares', 'log_self_reference_max_shares',\n",
      "       'log_self_reference_avg_sharess', 'day_of_week_Friday',\n",
      "       'day_of_week_Monday', 'day_of_week_Saturday', 'day_of_week_Sunday',\n",
      "       'day_of_week_Thursday', 'day_of_week_Tuesday', 'day_of_week_Wednesday',\n",
      "       'news_category_Business', 'news_category_Entertainment',\n",
      "       'news_category_Lifestyle', 'news_category_Social_Media',\n",
      "       'news_category_Tech', 'news_category_Uncategorized',\n",
      "       'news_category_World_News'],\n",
      "      dtype='object')\n",
      "0          <Q1\n",
      "1        Q1-Q2\n",
      "2        Q1-Q2\n",
      "3        Q2-Q3\n",
      "4        Q2-Q3\n",
      "         ...  \n",
      "39639    Q1-Q2\n",
      "39640    Q1-Q2\n",
      "39641      >Q3\n",
      "39642    Q2-Q3\n",
      "39643    Q1-Q2\n",
      "Name: share_quantile_ranges, Length: 39644, dtype: category\n",
      "Categories (4, object): ['<Q1' < 'Q1-Q2' < 'Q2-Q3' < '>Q3']\n"
     ]
    }
   ],
   "source": [
    "# Remove target, related variables, and factor other targets due to them being categorical\n",
    "X0 = pd.get_dummies(df1, columns=['day_of_week'])\n",
    "X0 = pd.get_dummies(X0, columns=['news_category'])\n",
    "X1 = X0.drop(['share_quantile_ranges', 'shares'], axis=1) \n",
    "y1 = X0['share_quantile_ranges']\n",
    "print(X1.columns)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing folds\n",
    "X_train1, X_test1, y_train1, y_test1 = [], [], [], []\n",
    "for train_index, test_index in kfold.split(X1, y1):\n",
    "    X_train1.append(X1.iloc[train_index])  # Use iloc to index the DataFrame\n",
    "    X_test1.append(X1.iloc[test_index])\n",
    "    y_train1.append(y1.iloc[train_index])\n",
    "    y_test1.append(y1.iloc[test_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the X1 DataFrame contains the columns that are being indexed\n",
    "missing_columns = set(train_index) - set(X1.columns)\n",
    "if missing_columns:\n",
    "    raise KeyError(f\"The following columns are not in the X1 DataFrame: {missing_columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "day_of_week task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove target, related variables, and factor other targets due to them being categorical\n",
    "X0 = pd.get_dummies(df1, columns=['news_category'])\n",
    "X2 = X0.drop(['share_quantile_ranges', 'day_of_week'], axis=1) # Removing target categorical variable and other categorical variables\n",
    "y2 = X0['day_of_week']\n",
    "# print(X2.columns)\n",
    "# print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing folds\n",
    "X_train2, X_test2, y_train2, y_test2 = [], [], [], []\n",
    "for train_index, test_index in kfold.split(X2, y2):\n",
    "    X_train2.append(X2[train_index])\n",
    "    X_test2.append(X2[test_index])\n",
    "    y_train2.append(y2[train_index])\n",
    "    y_test2.append(y2[test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the Data for news_category task and other targets due to them being categorical\n",
    "X0 = pd.get_dummies(df1, columns=['day_of_week'])\n",
    "X3 = X0.drop(['share_quantile_ranges', 'news_category'], axis=1) # Removing target categorical variable and other categorical variables\n",
    "y3 = X0['news_category']\n",
    "# print(X2.columns)\n",
    "# print(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing folds\n",
    "X_train3, X_test3, y_train3, y_test3 = [], [], [], []\n",
    "for train_index, test_index in kfold.split(X3, y3):\n",
    "    X_train3.append(X3[train_index])\n",
    "    X_test3.append(X3[test_index])\n",
    "    y_train3.append(y3[train_index])\n",
    "    y_test3.append(y3[test_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using 10-fold cross validation for our training/testing split.\n",
    " \n",
    "    It ensures that the training and testing splits are representative of the overall dataset. This is important because it helps to avoid overfitting the model to the training data.\n",
    "    It reduces the risk of variance. This is because the model is trained and evaluated on 10 different folds of the data, rather than just a single split.\n",
    "    It is relatively easy to implement. There are many libraries and frameworks that provide support for stratified cross-validation.\n",
    "\n",
    "\n",
    "    It can be used to compare different machine learning models.\n",
    "    It can be used to identify the best hyperparameters for your model.\n",
    "    It can be used to estimate the confidence interval for your model's performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"ModelEval3\"></a>\n",
    "# Modeling and Evaluation 3 (20 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15328\\1205627452.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# for share_quantile_range_task\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX_train1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mX_test1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 852\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    853\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    804\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    805\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    845\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"allow-nan\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m             \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfirst_call\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m         )\n\u001b[0;32m    849\u001b[0m         \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Scale the features in the training and testing sets using StandardScalar.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Use previous train/test split\n",
    "\n",
    "# for share_quantile_range_task\n",
    "X_train1 = scaler.fit_transform(X_train1)\n",
    "X_test1 = scaler.transform(X_test1)\n",
    "\n",
    "# for day_of_week task\n",
    "X_train2 = scaler.fit_transform(X_train2)\n",
    "X_test2 = scaler.transform(X_test2)\n",
    "\n",
    "# for news_category task\n",
    "X_train3 = scaler.fit_transform(X_train3)\n",
    "X_test3 = scaler.transform(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15328\\2488404419.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# for share_quantile_range_task\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX_train_q1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquantile_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mX_test_q1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquantile_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    851\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 852\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    853\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m   2584\u001b[0m             )\n\u001b[0;32m   2585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2586\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_fit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2587\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001b[0m in \u001b[0;36m_check_inputs\u001b[1;34m(self, X, in_fit, accept_sparse_negative, copy)\u001b[0m\n\u001b[0;32m   2679\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2680\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2681\u001b[1;33m             \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"allow-nan\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2682\u001b[0m         )\n\u001b[0;32m   2683\u001b[0m         \u001b[1;31m# we only accept positive sparse matrix when ignore_implicit_zeros is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Scale the features in the training and testing sets using QuantileTransformer after using StandardScalar.\n",
    "quantile_transformer = QuantileTransformer(n_quantiles=100)\n",
    "\n",
    "# Use previous train/test split\n",
    "\n",
    "# for share_quantile_range_task\n",
    "X_train_q1 = quantile_transformer.fit_transform(X_train1)\n",
    "X_test_q1 = quantile_transformer.fit_transform(X_test1)\n",
    "\n",
    "# for day_of_week task\n",
    "X_train_q2 = quantile_transformer.fit_transform(X_train2)\n",
    "X_test_q2 = quantile_transformer.fit_transform(X_test2)\n",
    "\n",
    "# for news_category task\n",
    "X_train_q3 = quantile_transformer.fit_transform(X_train3)\n",
    "X_test_q3 = quantile_transformer.fit_transform(X_test3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "## Create three different classification/regression models\n",
    "<a id=\"RFmodel\"></a>\n",
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the random forest model\n",
    "rf = RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### share_quantile_ranges task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15328\\2473104191.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Train the share_quantile_ranges model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    326\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sparse multilabel-indicator for y is not supported.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         X, y = self._validate_data(\n\u001b[1;32m--> 328\u001b[1;33m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         )\n\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    974\u001b[0m         \u001b[0mensure_min_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m         \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    977\u001b[0m     )\n\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    745\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 746\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    747\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m                 raise ValueError(\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (10,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# Train the share_quantile_ranges model\n",
    "rf.fit(X_train_q1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for share_quantile_ranges task\n",
    "rf_y_pred_q1 = rf.predict(X_test_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the share_quantile_ranges task.\n",
    "rf_accuracy_q1 = accuracy_score(y_test1, rf_y_pred_q1)\n",
    "rf_precision_q1 = precision_score(y_test1, rf_y_pred_q1, average='macro')\n",
    "rf_recall_q1 = recall_score(y_test1, rf_y_pred_q1, average='macro')\n",
    "rf_f1_score_q1 = f1_score(y_test1, rf_y_pred_q1, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics\n",
    "rf_model_scores_q1 = np.array([rf_accuracy_q1, rf_precision_q1, rf_recall_q1, rf_f1_score_q1])\n",
    "\n",
    "# Print the model evaluation metrics\n",
    "print('Random Forest on share_quantile_ranges Task')\n",
    "print(rf_model_scores_q1)\n",
    "print('---------------------------------------------------------')\n",
    "print('Random Forest accuracy:', rf_accuracy_q1)\n",
    "print('Random Forest precision:', rf_precision_q1)\n",
    "print('Random Forest recall:', rf_recall_q1)\n",
    "print('Random Forest F1 score:', rf_f1_score_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### day_of_week task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the day_of_week task\n",
    "rf.fit(X_train_q2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for day_of_week task\n",
    "rf_y_pred_q2 = rf.predict(X_test_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for day_of_week task.\n",
    "rf_accuracy_q2 = accuracy_score(y_test2, rf_y_pred_q2)\n",
    "rf_precision_q2 = precision_score(y_test2, rf_y_pred_q2, average='macro')\n",
    "rf_recall_q2 = recall_score(y_test2, rf_y_pred_q2, average='macro')\n",
    "rf_f1_score_q2 = f1_score(y_test2, rf_y_pred_q2, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics\n",
    "rf_model_scores_q2 = np.array([rf_accuracy_q2, rf_precision_q2, rf_recall_q2, rf_f1_score_q2])\n",
    "\n",
    "# Print the model evaluation metrics\n",
    "print('Random Forest on day_of_week Task')\n",
    "print(rf_model_scores_q2)\n",
    "print('---------------------------------------------------------')\n",
    "print('Random Forest accuracy:', rf_accuracy_q2)\n",
    "print('Random Forest precision:', rf_precision_q2)\n",
    "print('Random Forest recall:', rf_recall_q2)\n",
    "print('Random Forest F1 score:', rf_f1_score_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"KNNmodel\"></a>\n",
    "#### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### share_quantile_ranges task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the KNN model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for the share_quantile_ranges task.\n",
    "knn.fit(X_train_q1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for the share_quantile_ranges task.\n",
    "knn_y_pred_q1 = knn.predict(X_test_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the share_quantile_ranges task.\n",
    "knn_accuracy_q1 = accuracy_score(y_test1, knn_y_pred_q1)\n",
    "knn_precision_q1 = precision_score(y_test1, knn_y_pred_q1, average='macro')\n",
    "knn_recall_q1 = recall_score(y_test1, knn_y_pred_q1, average='macro')\n",
    "knn_f1_score_q1 = f1_score(y_test1, knn_y_pred_q1, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the share_quantile_ranges task.\n",
    "knn_model_scores_q1 = np.array([knn_accuracy_q1, knn_precision_q1, knn_recall_q1, knn_f1_score_q1])\n",
    "print('KNN on share_quantile_ranges Task')\n",
    "print(knn_model_scores_q1)\n",
    "print('---------------------------------------------------------')\n",
    "print('KNN accuracy:', knn_accuracy_q1)\n",
    "print('KNN precision:', knn_precision_q1)\n",
    "print('KNN recall:', knn_recall_q1)\n",
    "print('KNN F1 score:', knn_f1_score_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### day_of_week task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for the day_of_week task.\n",
    "knn.fit(X_train_q2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for the day_of_week task.\n",
    "knn_y_pred_q2 = knn.predict(X_test_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the day_of_week task.\n",
    "knn_accuracy_q2 = accuracy_score(y_test2, knn_y_pred_q2)\n",
    "knn_precision_q2 = precision_score(y_test2, knn_y_pred_q2, average='macro')\n",
    "knn_recall_q2 = recall_score(y_test2, knn_y_pred_q2, average='macro')\n",
    "knn_f1_score_q2 = f1_score(y_test2, knn_y_pred_q2, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics the day_of_week task.\n",
    "knn_model_scores_q2 = np.array([knn_accuracy_q2, knn_precision_q2, knn_recall_q2, knn_f1_score_q2])\n",
    "print('KNN on day_of_week Task')\n",
    "print(knn_model_scores_q2)\n",
    "print('---------------------------------------------------------')\n",
    "print('KNN accuracy:', knn_accuracy_q2)\n",
    "print('KNN precision:', knn_precision_q2)\n",
    "print('KNN recall:', knn_recall_q2)\n",
    "print('KNN F1 score:', knn_f1_score_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"SVMmodel\"></a>\n",
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model parameters\n",
    "C = 1.0\n",
    "kernel = 'linear'\n",
    "\n",
    "# Create the support vector machine model\n",
    "support_vector_machine_model = SVC(C=C, kernel=kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### share_quantile_ranges task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the support vector machine model on the training set using a linear kernel for the share_quantile_ranges task.\n",
    "support_vector_machine_model.fit(X_train_q1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data for the share_quantile_ranges task.\n",
    "svm_y_pred_q1 = support_vector_machine_model.predict(X_test_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the share_quantile_ranges task.\n",
    "svm_accuracy_q1 = accuracy_score(y_test1, svm_y_pred_q1)\n",
    "svm_precision_q1 = precision_score(y_test1, svm_y_pred_q1, average='macro')\n",
    "svm_recall_q1 = recall_score(y_test1, svm_y_pred_q1, average='macro')\n",
    "svm_f1_score_q1 = f1_score(y_test1, svm_y_pred_q1, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the share_quantile_ranges task.\n",
    "svm_model_scores_q1 = np.array([svm_accuracy_q1, svm_precision_q1, svm_recall_q1, svm_f1_score_q1])\n",
    "print('SVM on share_quantile_range Task')\n",
    "print(svm_model_scores_q1)\n",
    "print('---------------------------------------------------------')\n",
    "print('Support vector machine accuracy:', svm_accuracy_q1)\n",
    "print('Support vector machine precision:', svm_precision_q1)\n",
    "print('Support vector machine recall:', svm_recall_q1)\n",
    "print('Support vector machine F1 score:', svm_f1_score_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### day_of_week task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the support vector machine model on the training set using a linear kernel for the day_of_week task.\n",
    "support_vector_machine_model.fit(X_train_q2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data for the day_of_week task.\n",
    "svm_y_pred_q2 = support_vector_machine_model.predict(X_test_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the day_of_week task.\n",
    "svm_accuracy_q2 = accuracy_score(y_test2, svm_y_pred_q2)\n",
    "svm_precision_q2 = precision_score(y_test2, svm_y_pred_q2, average='macro')\n",
    "svm_recall_q2 = recall_score(y_test2, svm_y_pred_q2, average='macro')\n",
    "svm_f1_score_q2 = f1_score(y_test2, svm_y_pred_q2, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the day_of_week task\n",
    "svm_model_scores_q2 = np.array([svm_accuracy_q2, svm_precision_q2, svm_recall_q2, svm_f1_score_q2])\n",
    "print('SVM on day_of_week Task')\n",
    "print(svm_model_scores_q2)\n",
    "print('---------------------------------------------------------')\n",
    "print('Support vector machine accuracy:', svm_accuracy_q2)\n",
    "print('Support vector machine precision:', svm_precision_q2)\n",
    "print('Support vector machine recall:', svm_recall_q2)\n",
    "print('Support vector machine F1 score:', svm_f1_score_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust parameters as appropriate to increase generalization performance using your chosen metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"ModelEval4\"></a>\n",
    "# Modeling and Evaluation 4 (10 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the results using your chosen method of evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use visualizations of the results to bolster the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Explain any visuals and analyze why they are interesting to someone that might use this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"ModelEval5\"></a>\n",
    "# Modeling and Evaluation 5 (10 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss the advantages of each model for each classification task, if any."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"TaskEval\"></a>\n",
    "### Is the difference significant with 95% confidence? Use proper statistical comparison methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"sqrTask\"></a>\n",
    "#### share_quantile_ranges task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantileTransformer transformed accuracy scores\n",
    "print(rf_model_scores_q1)\n",
    "print(knn_model_scores_q1)\n",
    "print(svm_model_scores_q1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model accuracy scores\n",
    "model1_scores = rf_model_scores_q1\n",
    "model2_scores = knn_model_scores_q1\n",
    "model3_scores = svm_model_scores_q1\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model2_scores)\n",
    "\n",
    "print(\"Random Forest vs. KNN\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model1_scores, model3_scores)\n",
    "\n",
    "print(\"Random Forest vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value3 = ttest_rel(model2_scores, model3_scores)\n",
    "\n",
    "print(\"KNN vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"dowTask\"></a>\n",
    "#### day_of_week task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantileTransformer transformed accuracy scores\n",
    "print(rf_model_scores_q2)\n",
    "print(knn_model_scores_q2)\n",
    "print(svm_model_scores_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model accuracy scores\n",
    "model1_scores = rf_model_scores_q2\n",
    "model2_scores = knn_model_scores_q2\n",
    "model3_scores = svm_model_scores_q2\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model2_scores)\n",
    "\n",
    "print(\"Random Forest vs. KNN\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model1_scores, model3_scores)\n",
    "\n",
    "print(\"Random Forest vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value3 = ttest_rel(model2_scores, model3_scores)\n",
    "\n",
    "print(\"KNN vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"ModelEval6\"></a>\n",
    "# Modeling and Evaluation 6 (10 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which attributes from your analysis are most important?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use proper methods discussed in class to evaluate the importance of different attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"Deployment\"></a>\n",
    "# Deployment (5 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How useful is your model for interested parties (i.e., the companies or organizations that might want to use it for prediction)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How would you measure the model's value if it was used by these parties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How would your deploy your model for interested parties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### What other data should be collected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### How often would the model need to be updated, etc.?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"Exceptional\"></a>\n",
    "# Exceptional Work (10 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional modeling.\n",
    "The StandardScalar transformed data was taken down the same path as the QuantileTransformer transformed data. There was a seperate running of each task; one using each scale transformer. This was done in order to compare the two and determine which, if either, is better than the other.\n",
    "\n",
    "An additional task was performed using the three models. The 'news_category' task is focused on determining category of news that the article falls under. The SVM model did not take to this task very well. The SVM model runtime is much too long for it to be useful, especially compared to the runtime of the other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StandardScalar Transformed on Random Forest model for each task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### share_quantile_ranges task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the share_quantile_ranges model\n",
    "rf.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for share_quantile_ranges task\n",
    "rf_y_pred1 = rf.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the share_quantile_ranges task.\n",
    "rf_accuracy1 = accuracy_score(y_test1, rf_y_pred1)\n",
    "rf_precision1 = precision_score(y_test1, rf_y_pred1, average='macro')\n",
    "rf_recall1 = recall_score(y_test1, rf_y_pred1, average='macro')\n",
    "rf_f1_score1 = f1_score(y_test1, rf_y_pred1, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics\n",
    "rf_model_scores1 = np.array([rf_accuracy1, rf_precision1, rf_recall1, rf_f1_score1])\n",
    "\n",
    "# Print the model evaluation metrics\n",
    "print('Random Forest on share_quantile_ranges Task')\n",
    "print(rf_model_scores1)\n",
    "print('---------------------------------------------------------')\n",
    "print('Random Forest accuracy:', rf_accuracy1)\n",
    "print('Random Forest precision:', rf_precision1)\n",
    "print('Random Forest recall:', rf_recall1)\n",
    "print('Random Forest F1 score:', rf_f1_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### day_of_week task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the day_of_week task\n",
    "rf.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for day_of_week task\n",
    "rf_y_pred2 = rf.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for day_of_week task.\n",
    "rf_accuracy2 = accuracy_score(y_test2, rf_y_pred2)\n",
    "rf_precision2 = precision_score(y_test2, rf_y_pred2, average='macro')\n",
    "rf_recall2 = recall_score(y_test2, rf_y_pred2, average='macro')\n",
    "rf_f1_score2 = f1_score(y_test2, rf_y_pred2, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics\n",
    "rf_model_scores2 = np.array([rf_accuracy2, rf_precision2, rf_recall2, rf_f1_score2])\n",
    "\n",
    "# Print the model evaluation metrics\n",
    "print('Random Forest on day_of_week Task')\n",
    "print(rf_model_scores2)\n",
    "print('---------------------------------------------------------')\n",
    "print('Random Forest accuracy:', rf_accuracy2)\n",
    "print('Random Forest precision:', rf_precision2)\n",
    "print('Random Forest recall:', rf_recall2)\n",
    "print('Random Forest F1 score:', rf_f1_score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the news_category task\n",
    "rf.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for news_category task\n",
    "rf_y_pred3 = rf.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for news_category task.\n",
    "rf_accuracy3 = accuracy_score(y_test3, rf_y_pred3)\n",
    "rf_precision3 = precision_score(y_test3, rf_y_pred3, average='macro')\n",
    "rf_recall3 = recall_score(y_test3, rf_y_pred3, average='macro')\n",
    "rf_f1_score3 = f1_score(y_test3, rf_y_pred3, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics\n",
    "rf_model_scores3 = np.array([rf_accuracy3, rf_precision3, rf_recall3, rf_f1_score3])\n",
    "\n",
    "# Print the model evaluation metrics\n",
    "print('Random Forest on news_category Task')\n",
    "print(rf_model_scores3)\n",
    "print('---------------------------------------------------------')\n",
    "print('Random Forest accuracy:', rf_accuracy3)\n",
    "print('Random Forest precision:', rf_precision3)\n",
    "print('Random Forest recall:', rf_recall3)\n",
    "print('Random Forest F1 score:', rf_f1_score3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "##### StandardScalar Transformed on KNN model for each task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### share_quantile_ranges task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for the share_quantile_ranges task.\n",
    "knn.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for the share_quantile_ranges task.\n",
    "knn_y_pred1 = knn.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the share_quantile_ranges task.\n",
    "knn_accuracy1 = accuracy_score(y_test1, knn_y_pred1)\n",
    "knn_precision1 = precision_score(y_test1, knn_y_pred1, average='macro')\n",
    "knn_recall1 = recall_score(y_test1, knn_y_pred1, average='macro')\n",
    "knn_f1_score1 = f1_score(y_test1, knn_y_pred1, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the share_quantile_ranges task.\n",
    "knn_model_scores1 = np.array([knn_accuracy1, knn_precision1, knn_recall1, knn_f1_score1])\n",
    "print('KNN on share_quantile_ranges Task')\n",
    "print(knn_model_scores1)\n",
    "print('---------------------------------------------------------')\n",
    "print('KNN accuracy:', knn_accuracy1)\n",
    "print('KNN precision:', knn_precision1)\n",
    "print('KNN recall:', knn_recall1)\n",
    "print('KNN F1 score:', knn_f1_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### day_of_week task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for the day_of_week task.\n",
    "knn.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for the day_of_week task.\n",
    "knn_y_pred2 = knn.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the day_of_week task.\n",
    "knn_accuracy2 = accuracy_score(y_test2, knn_y_pred2)\n",
    "knn_precision2 = precision_score(y_test2, knn_y_pred2, average='macro')\n",
    "knn_recall2 = recall_score(y_test2, knn_y_pred2, average='macro')\n",
    "knn_f1_score2 = f1_score(y_test2, knn_y_pred2, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics the day_of_week task.\n",
    "knn_model_scores2 = np.array([knn_accuracy2, knn_precision2, knn_recall2, knn_f1_score2])\n",
    "print('KNN on day_of_week Task')\n",
    "print(knn_model_scores2)\n",
    "print('---------------------------------------------------------')\n",
    "print('KNN accuracy:', knn_accuracy2)\n",
    "print('KNN precision:', knn_precision2)\n",
    "print('KNN recall:', knn_recall2)\n",
    "print('KNN F1 score:', knn_f1_score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for the news_category task.\n",
    "knn.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for the news_category task.\n",
    "knn_y_pred3 = knn.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the news_category task.\n",
    "knn_accuracy3 = accuracy_score(y_test3, knn_y_pred3)\n",
    "knn_precision3 = precision_score(y_test3, knn_y_pred3, average='macro')\n",
    "knn_recall3 = recall_score(y_test3, knn_y_pred3, average='macro')\n",
    "knn_f1_score3 = f1_score(y_test3, knn_y_pred3, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the news_category task.\n",
    "knn_model_scores3 = np.array([knn_accuracy3, knn_precision3, knn_recall3, knn_f1_score3])\n",
    "print('KNN on news_category Task')\n",
    "print(knn_model_scores3)\n",
    "print('---------------------------------------------------------')\n",
    "print('KNN accuracy:', knn_accuracy3)\n",
    "print('KNN precision:', knn_precision3)\n",
    "print('KNN recall:', knn_recall3)\n",
    "print('KNN F1 score:', knn_f1_score3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "##### StandardScalar Transformed on SVM model for each task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### share_quantile_ranges task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the support vector machine model on the training set using a linear kernel for the share_quantile_ranges task.\n",
    "support_vector_machine_model.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data for the share_quantile_ranges task.\n",
    "svm_y_pred1 = support_vector_machine_model.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the share_quantile_ranges task.\n",
    "svm_accuracy1 = accuracy_score(y_test1, svm_y_pred1)\n",
    "svm_precision1 = precision_score(y_test1, svm_y_pred1, average='macro')\n",
    "svm_recall1 = recall_score(y_test1, svm_y_pred1, average='macro')\n",
    "svm_f1_score1 = f1_score(y_test1, svm_y_pred1, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the share_quantile_ranges task.\n",
    "svm_model_scores1 = np.array([svm_accuracy1, svm_precision1, svm_recall1, svm_f1_score1])\n",
    "print('SVM on share_quantile_range Task')\n",
    "print(svm_model_scores1)\n",
    "print('---------------------------------------------------------')\n",
    "print('Support vector machine accuracy:', svm_accuracy1)\n",
    "print('Support vector machine precision:', svm_precision1)\n",
    "print('Support vector machine recall:', svm_recall1)\n",
    "print('Support vector machine F1 score:', svm_f1_score1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### day_of_week task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the support vector machine model on the training set using a linear kernel for the day_of_week task.\n",
    "support_vector_machine_model.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data for the day_of_week task.\n",
    "svm_y_pred2 = support_vector_machine_model.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the day_of_week task.\n",
    "svm_accuracy2 = accuracy_score(y_test2, svm_y_pred2)\n",
    "svm_precision2 = precision_score(y_test2, svm_y_pred2, average='macro')\n",
    "svm_recall2 = recall_score(y_test2, svm_y_pred2, average='macro')\n",
    "svm_f1_score2 = f1_score(y_test2, svm_y_pred2, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the day_of_week task\n",
    "svm_model_scores2 = np.array([svm_accuracy2, svm_precision2, svm_recall2, svm_f1_score2])\n",
    "print('SVM on day_of_week Task')\n",
    "print(svm_model_scores2)\n",
    "print('---------------------------------------------------------')\n",
    "print('Support vector machine accuracy:', svm_accuracy2)\n",
    "print('Support vector machine precision:', svm_precision2)\n",
    "print('Support vector machine recall:', svm_recall2)\n",
    "print('Support vector machine F1 score:', svm_f1_score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "news_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the support vector machine model on the training set using a linear kernel for the news_category task.\n",
    "# support_vector_machine_model.fit(X_train3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data for the news_category task.\n",
    "# svm_y_pred3 = support_vector_machine_model.predict(X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the news_category task.\n",
    "# svm_accuracy3 = accuracy_score(y_test3, svm_y_pred3)\n",
    "# svm_precision3 = precision_score(y_test3, svm_y_pred3, average='macro')\n",
    "# svm_recall3 = recall_score(y_test3, svm_y_pred3, average='macro')\n",
    "# svm_f1_score3 = f1_score(y_test3, svm_y_pred3, average='macro')\n",
    "\n",
    "# # Create an array to store the model evaluation metrics for the news_category task\n",
    "# svm_model_scores3 = np.array([svm_accuracy3, svm_precision3, svm_recall3, svm_f1_score3])\n",
    "# print('SVM on news_category Task')\n",
    "# print(svm_model_scores3)\n",
    "# print('---------------------------------------------------------')\n",
    "# print('Support vector machine accuracy:', svm_accuracy3)\n",
    "# print('Support vector machine precision:', svm_precision3)\n",
    "# print('Support vector machine recall:', svm_recall3)\n",
    "# print('Support vector machine F1 score:', svm_f1_score3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"#ncTask\"></a>\n",
    "##### QuantileTransformer transformed for each model on 'news_category' task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the news_category task\n",
    "rf.fit(X_train_q3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for news_category task\n",
    "rf_y_pred_q3 = rf.predict(X_test_q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for news_category task.\n",
    "rf_accuracy_q3 = accuracy_score(y_test3, rf_y_pred_q3)\n",
    "rf_precision_q3 = precision_score(y_test3, rf_y_pred_q3, average='macro')\n",
    "rf_recall_q3 = recall_score(y_test3, rf_y_pred_q3, average='macro')\n",
    "rf_f1_score_q3 = f1_score(y_test3, rf_y_pred_q3, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics\n",
    "rf_model_scores_q3 = np.array([rf_accuracy_q3, rf_precision_q3, rf_recall_q3, rf_f1_score_q3])\n",
    "\n",
    "# Print the model evaluation metrics\n",
    "print('Random Forest on news_category Task')\n",
    "print(rf_model_scores_q3)\n",
    "print('---------------------------------------------------------')\n",
    "print('Random Forest accuracy:', rf_accuracy_q3)\n",
    "print('Random Forest precision:', rf_precision_q3)\n",
    "print('Random Forest recall:', rf_recall_q3)\n",
    "print('Random Forest F1 score:', rf_f1_score_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for the news_category task.\n",
    "knn.fit(X_train_q3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set for the news_category task.\n",
    "knn_y_pred_q3 = knn.predict(X_test_q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the news_category task.\n",
    "knn_accuracy_q3 = accuracy_score(y_test3, knn_y_pred_q3)\n",
    "knn_precision_q3 = precision_score(y_test3, knn_y_pred_q3, average='macro')\n",
    "knn_recall_q3 = recall_score(y_test3, knn_y_pred_q3, average='macro')\n",
    "knn_f1_score_q3 = f1_score(y_test3, knn_y_pred_q3, average='macro')\n",
    "\n",
    "# Create an array to store the model evaluation metrics for the news_category task.\n",
    "knn_model_scores_q3 = np.array([knn_accuracy_q3, knn_precision_q3, knn_recall_q3, knn_f1_score_q3])\n",
    "print('KNN on news_category Task')\n",
    "print(knn_model_scores_q3)\n",
    "print('---------------------------------------------------------')\n",
    "print('KNN accuracy:', knn_accuracy_q3)\n",
    "print('KNN precision:', knn_precision_q3)\n",
    "print('KNN recall:', knn_recall_q3)\n",
    "print('KNN F1 score:', knn_f1_score_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM\n",
    "This model did not run in a timely manner and was dropped. As mentioned before the SVM models for the news_category tasks took markedly longer than their counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the support vector machine model on the training set using a linear kernel for the news_category task.\n",
    "# support_vector_machine_model.fit(X_train_q3, y_train3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make predictions on the test data for the news_category task.\n",
    "# svm_y_pred_q3 = support_vector_machine_model.predict(X_test_q3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the accuracy, precision, recall, and F1 score of the model on the test data for the news_category task.\n",
    "# svm_accuracy_q3 = accuracy_score(y_test3, svm_y_pred_q3)\n",
    "# svm_precision_q3 = precision_score(y_test3, svm_y_pred_q3, average='macro')\n",
    "# svm_recall_q3 = recall_score(y_test3, svm_y_pred_q3, average='macro')\n",
    "# svm_f1_score_q3 = f1_score(y_test3, svm_y_pred_q3, average='macro')\n",
    "\n",
    "# # Create an array to store the model evaluation metrics for the news_category task\n",
    "# svm_model_scores_q3 = np.array([svm_accuracy_q3, svm_precision_q3, svm_recall_q3, svm_f1_score_q3])\n",
    "# print('SVM on news_category Task')\n",
    "# print(svm_model_scores_q3)\n",
    "# print('---------------------------------------------------------')\n",
    "# print('Support vector machine accuracy:', svm_accuracy_q3)\n",
    "# print('Support vector machine precision:', svm_precision_q3)\n",
    "# print('Support vector machine recall:', svm_recall_q3)\n",
    "# print('Support vector machine F1 score:', svm_f1_score_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "Comparing the StandardScalar transformed models' performance on each task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### share_quantile_ranges task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScalar transformed accuracy scores\n",
    "print(rf_model_scores1)\n",
    "print(knn_model_scores1)\n",
    "print(svm_model_scores1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model accuracy scores\n",
    "model1_scores = rf_model_scores1\n",
    "model2_scores = knn_model_scores1\n",
    "model3_scores = svm_model_scores1\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model2_scores)\n",
    "\n",
    "print(\"Random Forest vs. KNN\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model1_scores, model3_scores)\n",
    "\n",
    "print(\"Random Forest vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value3 = ttest_rel(model2_scores, model3_scores)\n",
    "\n",
    "print(\"KNN vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### day_of_week task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScalar transformed accuracy scores\n",
    "print(rf_model_scores2)\n",
    "print(knn_model_scores2)\n",
    "print(svm_model_scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model accuracy scores\n",
    "model1_scores = rf_model_scores2\n",
    "model2_scores = knn_model_scores2\n",
    "model3_scores = svm_model_scores2\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model2_scores)\n",
    "\n",
    "print(\"Random Forest vs. KNN\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model1_scores, model3_scores)\n",
    "\n",
    "print(\"Random Forest vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value3 = ttest_rel(model2_scores, model3_scores)\n",
    "\n",
    "print(\"KNN vs. SVM\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScalar transformed accuracy scores\n",
    "print(rf_model_scores3)\n",
    "print(knn_model_scores3)\n",
    "# print(svm_model_scores_q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model accuracy scores\n",
    "model1_scores = rf_model_scores3\n",
    "model2_scores = knn_model_scores3\n",
    "# model3_scores = svm_model_scores3\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model2_scores)\n",
    "\n",
    "print(\"Random Forest vs. KNN\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# # Perform a paired t-test\n",
    "# t_statistic, p_value2 = ttest_rel(model1_scores, model3_scores)\n",
    "\n",
    "# print(\"Random Forest vs. SVM\")\n",
    "# # Check if the difference is significant with 95% confidence\n",
    "# if p_value2 < 0.05:\n",
    "#   print(\"The difference between the two models is statistically significant.\")\n",
    "# else:\n",
    "#   print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "# print('---------------------------------------------------------')\n",
    "\n",
    "# # Perform a paired t-test\n",
    "# t_statistic, p_value3 = ttest_rel(model2_scores, model3_scores)\n",
    "\n",
    "# print(\"KNN vs. SVM\")\n",
    "# # Check if the difference is significant with 95% confidence\n",
    "# if p_value2 < 0.05:\n",
    "#   print(\"The difference between the two models is statistically significant.\")\n",
    "# else:\n",
    "#   print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "Comparing the removed news_category task QuantileTransformer Transformed models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QuantileTransformer transformed accuracy scores\n",
    "print(rf_model_scores_q3) # share_quantile_ranges task\n",
    "print(knn_model_scores_q3) # day_of_week task\n",
    "# print(svm_model_scores_q3) # news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model accuracy scores\n",
    "model1_scores = rf_model_scores_q3\n",
    "model2_scores = knn_model_scores_q3\n",
    "# model3_scores = svm_model_scores_q3\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model2_scores)\n",
    "\n",
    "print(\"Random Forest vs. KNN\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# # Perform a paired t-test\n",
    "# t_statistic, p_value2 = ttest_rel(model1_scores, model3_scores)\n",
    "\n",
    "# print(\"Random Forest vs. SVM\")\n",
    "# # Check if the difference is significant with 95% confidence\n",
    "# if p_value2 < 0.05:\n",
    "#   print(\"The difference between the two models is statistically significant.\")\n",
    "# else:\n",
    "#   print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "# print('---------------------------------------------------------')\n",
    "\n",
    "# # Perform a paired t-test\n",
    "# t_statistic, p_value3 = ttest_rel(model2_scores, model3_scores)\n",
    "\n",
    "# print(\"KNN vs. SVM\")\n",
    "# # Check if the difference is significant with 95% confidence\n",
    "# if p_value2 < 0.05:\n",
    "#   print(\"The difference between the two models is statistically significant.\")\n",
    "# else:\n",
    "#   print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "Comparing the two scaling methods in Modeling and Evaluation 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"ScalerEval\"></a>\n",
    "##### Now for determining if there is a statistically significant difference between transformation types StandardScalar & QuantileTransformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"RFEval\"></a>\n",
    "Random Forest Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScalar transformed accuracy scores\n",
    "print(rf_model_scores1) # share_quantile_ranges task\n",
    "print(rf_model_scores2) # day_of_week task\n",
    "print(rf_model_scores3) # news_category task\n",
    "\n",
    "# QuantileTransformer transformed accuracy scores\n",
    "print(rf_model_scores_q1) # share_quantile_ranges task\n",
    "print(rf_model_scores_q2) # day_of_week task\n",
    "print(rf_model_scores_q3) # news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model accuracy scores for StandardScalar transformed\n",
    "model1_scores = rf_model_scores1\n",
    "model2_scores = rf_model_scores2\n",
    "model3_scores = rf_model_scores3\n",
    "\n",
    "# Load the model accuracy scores for QuantileTransformer transformed\n",
    "model4_scores = rf_model_scores_q1\n",
    "model5_scores = rf_model_scores_q2\n",
    "model6_scores = rf_model_scores_q3\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model4_scores)\n",
    "print(\"Random Forest Models\")\n",
    "print(\"StandardScalar vs. QuantileTransformation on share_quantile_ranges task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model2_scores, model5_scores)\n",
    "\n",
    "print(\"StandardScalar vs. QuantileTransformation on day_of_week task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value3 = ttest_rel(model3_scores, model6_scores)\n",
    "\n",
    "print(\"StandardScalar vs. QuantileTransformation on news_category task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"KNNEval\"></a>\n",
    "\n",
    "KNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScalar transformed accuracy scores\n",
    "print(knn_model_scores1) # share_quantile_ranges task\n",
    "print(knn_model_scores2) # day_of_week task\n",
    "print(knn_model_scores3) # news_category task\n",
    "\n",
    "# QuantileTransformer transformed accuracy scores\n",
    "print(knn_model_scores_q1) # share_quantile_ranges task\n",
    "print(knn_model_scores_q2) # day_of_week task\n",
    "print(knn_model_scores_q3) # news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model accuracy scores for StandardScalar transformed\n",
    "model1_scores = knn_model_scores1\n",
    "model2_scores = knn_model_scores2\n",
    "model3_scores = knn_model_scores3\n",
    "\n",
    "# Load the model accuracy scores for QuantileTransformer transformed\n",
    "model4_scores = knn_model_scores_q1\n",
    "model5_scores = knn_model_scores_q2\n",
    "model6_scores = knn_model_scores_q3\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model4_scores)\n",
    "print(\"Random Forest Models\")\n",
    "print(\"StandardScalar vs. QuantileTransformation on share_quantile_ranges task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model2_scores, model5_scores)\n",
    "\n",
    "print(\"StandardScalar vs. QuantileTransformation on day_of_week task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value3 = ttest_rel(model3_scores, model6_scores)\n",
    "\n",
    "print(\"StandardScalar vs. QuantileTransformation on news_category task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"#top\">Back to Top</a>\n",
    "<a id=\"SVMEval\"></a>\n",
    "\n",
    "SVM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScalar transformed accuracy scores\n",
    "print(svm_model_scores1) # share_quantile_ranges task\n",
    "print(svm_model_scores2) # day_of_week task\n",
    "# print(svm_model_scores1) # news_category task\n",
    "\n",
    "# QuantileTransformer transformed accuracy scores\n",
    "print(svm_model_scores_q1) # share_quantile_ranges task\n",
    "print(svm_model_scores_q2) # day_of_week task\n",
    "# print(svm_model_scores_q3) # news_category task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model accuracy scores for StandardScalar transformed\n",
    "model1_scores = svm_model_scores1\n",
    "model2_scores = svm_model_scores2\n",
    "# model3_scores = svm_model_scores3\n",
    "\n",
    "# Load the model accuracy scores for QuantileTransformer transformed\n",
    "model4_scores = svm_model_scores_q1\n",
    "model5_scores = svm_model_scores_q2\n",
    "# model6_scores = svm_model_scores_q3\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value1 = ttest_rel(model1_scores, model4_scores)\n",
    "print(\"Random Forest Models\")\n",
    "print(\"StandardScalar vs. QuantileTransformation on share_quantile_ranges task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value1 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Perform a paired t-test\n",
    "t_statistic, p_value2 = ttest_rel(model2_scores, model5_scores)\n",
    "\n",
    "print(\"StandardScalar vs. QuantileTransformation on day_of_week task\")\n",
    "# Check if the difference is significant with 95% confidence\n",
    "if p_value2 < 0.05:\n",
    "  print(\"The difference between the two models is statistically significant.\")\n",
    "else:\n",
    "  print(\"The difference between the two models is not statistically significant.\")\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# # Perform a paired t-test\n",
    "# t_statistic, p_value3 = ttest_rel(model3_scores, model6_scores)\n",
    "\n",
    "# print(\"StandardScalar vs. QuantileTransformation on news_category task\")\n",
    "# # Check if the difference is significant with 95% confidence\n",
    "# if p_value2 < 0.05:\n",
    "#   print(\"The difference between the two models is statistically significant.\")\n",
    "# else:\n",
    "#   print(\"The difference between the two models is not statistically significant.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One idea: grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
