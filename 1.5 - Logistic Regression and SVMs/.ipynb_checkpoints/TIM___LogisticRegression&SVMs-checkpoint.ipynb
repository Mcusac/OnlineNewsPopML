{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca3a5118",
   "metadata": {},
   "source": [
    "### In this mini-project the team decided to reuse the Mashable dataset from Project 1. As you recall the data was gathered between 2013-2014 in order to learn more about their readers in the hopes of creating more ad revenue. The data collected monitored a range quantified features such as sentiment, polarity, and number of shares per article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "813cff82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# this import allows you train and test you test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# this import allows you to standardize your data, scaling so that all features have a mean of zero and a standard deviation of 1. \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# this import allows you to create a logistic regression model; type of machine learning model that can be used for classification tasks \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# this import allows you to create a support vector machine SVM model, a type of ML model that can be used for classification tasks. \n",
    "from sklearn.svm import SVC\n",
    "# this import allows you to perform CV on your model, a technique for evaluating the performance of a ML on unseen data\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# these imports allow you to calculate various evaluation metrics for your ML model. Eval metrics are used to asses the performance of a ML on held-out test set. \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8f4a6b",
   "metadata": {},
   "source": [
    "### Here we load in our data and drop a few other columns that are categorical that will not be used in our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43ed2738",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url_name</th>\n",
       "      <th>date</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>news_category</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>log_shares</th>\n",
       "      <th>log_n_tokens_content</th>\n",
       "      <th>log_num_hrefs</th>\n",
       "      <th>log_num_self_hrefs</th>\n",
       "      <th>log_num_imgs</th>\n",
       "      <th>log_num_videos</th>\n",
       "      <th>log_kw_max_min</th>\n",
       "      <th>log_kw_min_max</th>\n",
       "      <th>log_kw_avg_avg</th>\n",
       "      <th>log_self_reference_min_shares</th>\n",
       "      <th>log_self_reference_max_shares</th>\n",
       "      <th>log_self_reference_avg_sharess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon-instant-video-browser/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>593</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.386879</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reeddit-reddit/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>4.546154</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.022276</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.681548</td>\n",
       "      <td>0.381987</td>\n",
       "      <td>0.152189</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.353939</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>1300</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rage-comics-dying/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>4.759494</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.199626</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.714611</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.542580</td>\n",
       "      <td>0.122370</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357269</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.338889</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1100</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.003974</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>power-matters-alliance-organization/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.535390</td>\n",
       "      <td>5.147748</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.117255</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>0.425089</td>\n",
       "      <td>0.128515</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.337965</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.225794</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1600</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.378384</td>\n",
       "      <td>6.320768</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>polaroid-android-camera/</td>\n",
       "      <td>2013-01-07</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.424132</td>\n",
       "      <td>4.631390</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.327017</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>0.506520</td>\n",
       "      <td>0.279769</td>\n",
       "      <td>0.071749</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.417055</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.212354</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2400</td>\n",
       "      <td>Monday</td>\n",
       "      <td>Tech</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.783641</td>\n",
       "      <td>7.017506</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.302619</td>\n",
       "      <td>9.680406</td>\n",
       "      <td>8.140199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               url_name        date  timedelta  \\\n",
       "0         amazon-instant-video-browser/  2013-01-07      731.0   \n",
       "1                       reeddit-reddit/  2013-01-07      731.0   \n",
       "2                    rage-comics-dying/  2013-01-07      731.0   \n",
       "3  power-matters-alliance-organization/  2013-01-07      731.0   \n",
       "4              polaroid-android-camera/  2013-01-07      731.0   \n",
       "\n",
       "   n_tokens_title  n_unique_tokens  average_token_length  num_keywords  \\\n",
       "0            12.0         0.663594              4.680365           5.0   \n",
       "1             8.0         0.821705              4.546154           9.0   \n",
       "2             9.0         0.608602              4.759494           7.0   \n",
       "3            10.0         0.535390              5.147748          10.0   \n",
       "4             9.0         0.424132              4.631390           8.0   \n",
       "\n",
       "   kw_min_min  kw_avg_min  kw_max_max  kw_avg_max  kw_min_avg  kw_max_avg  \\\n",
       "0         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4         0.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   is_weekend    LDA_00    LDA_01    LDA_02    LDA_03    LDA_04  \\\n",
       "0         0.0  0.500331  0.378279  0.040005  0.041263  0.040123   \n",
       "1         0.0  0.022265  0.022446  0.022276  0.251465  0.681548   \n",
       "2         0.0  0.028575  0.199626  0.028615  0.714611  0.028572   \n",
       "3         0.0  0.020011  0.020317  0.117255  0.020007  0.822410   \n",
       "4         0.0  0.025001  0.327017  0.025001  0.025001  0.597981   \n",
       "\n",
       "   global_subjectivity  global_sentiment_polarity  global_rate_positive_words  \\\n",
       "0             0.521617                   0.092562                    0.045662   \n",
       "1             0.381987                   0.152189                    0.038462   \n",
       "2             0.542580                   0.122370                    0.063291   \n",
       "3             0.425089                   0.128515                    0.039640   \n",
       "4             0.506520                   0.279769                    0.071749   \n",
       "\n",
       "   global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "0                    0.013699             0.769231             0.230769   \n",
       "1                    0.007692             0.833333             0.166667   \n",
       "2                    0.025316             0.714286             0.285714   \n",
       "3                    0.012613             0.758621             0.241379   \n",
       "4                    0.013453             0.842105             0.157895   \n",
       "\n",
       "   avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "0               0.378636               0.100000                    0.7   \n",
       "1               0.353939               0.033333                    0.7   \n",
       "2               0.357269               0.050000                    0.6   \n",
       "3               0.337965               0.050000                    0.7   \n",
       "4               0.417055               0.100000                    1.0   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.350000                   -0.6                 -0.200   \n",
       "1              -0.400000                   -0.4                 -0.400   \n",
       "2              -0.338889                   -1.0                 -0.050   \n",
       "3              -0.225794                   -0.4                 -0.125   \n",
       "4              -0.212354                   -0.5                 -0.050   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.500000                   -0.1875                0.000000   \n",
       "1            0.250000                    0.2000                0.250000   \n",
       "2            0.650000                   -0.5000                0.150000   \n",
       "3            0.500000                   -0.1000                0.000000   \n",
       "4            0.333333                    0.2500                0.166667   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares day_of_week  news_category  year  \\\n",
       "0                        0.1875     593      Monday  Entertainment  2013   \n",
       "1                        0.2000    1300      Monday           Tech  2013   \n",
       "2                        0.5000    1100      Monday  Uncategorized  2013   \n",
       "3                        0.1000    1600      Monday           Tech  2013   \n",
       "4                        0.2500    2400      Monday           Tech  2013   \n",
       "\n",
       "   month  log_shares  log_n_tokens_content  log_num_hrefs  log_num_self_hrefs  \\\n",
       "0      1    6.386879              5.393628       1.609438            1.098612   \n",
       "1      1    7.170888              4.875197       2.079442            1.609438   \n",
       "2      1    7.003974              6.163315       2.484907            0.000000   \n",
       "3      1    7.378384              6.320768       2.079442            1.945910   \n",
       "4      1    7.783641              7.017506       3.091042            3.091042   \n",
       "\n",
       "   log_num_imgs  log_num_videos  log_kw_max_min  log_kw_min_max  \\\n",
       "0      0.693147             0.0             0.0             0.0   \n",
       "1      0.000000             0.0             0.0             0.0   \n",
       "2      0.693147             0.0             0.0             0.0   \n",
       "3      0.693147             0.0             0.0             0.0   \n",
       "4      3.044522             0.0             0.0             0.0   \n",
       "\n",
       "   log_kw_avg_avg  log_self_reference_min_shares  \\\n",
       "0             0.0                       6.208590   \n",
       "1             0.0                       7.170888   \n",
       "2             0.0                       0.000000   \n",
       "3             0.0                       7.550135   \n",
       "4             0.0                       6.302619   \n",
       "\n",
       "   log_self_reference_max_shares  log_self_reference_avg_sharess  \n",
       "0                       6.208590                        6.208590  \n",
       "1                       7.170888                        7.170888  \n",
       "2                       0.000000                        0.000000  \n",
       "3                       7.550135                        7.550135  \n",
       "4                       9.680406                        8.140199  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file path\n",
    "filepath = \"../1 - Visualization and Data Preprocessing/Data/ONPClean2.csv\"\n",
    "# Load the dataset\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# Set the maximum number of columns to display to None\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0a27abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>log_shares</th>\n",
       "      <th>log_n_tokens_content</th>\n",
       "      <th>log_num_hrefs</th>\n",
       "      <th>log_num_self_hrefs</th>\n",
       "      <th>log_num_imgs</th>\n",
       "      <th>log_num_videos</th>\n",
       "      <th>log_kw_max_min</th>\n",
       "      <th>log_kw_min_max</th>\n",
       "      <th>log_kw_avg_avg</th>\n",
       "      <th>log_self_reference_min_shares</th>\n",
       "      <th>log_self_reference_max_shares</th>\n",
       "      <th>log_self_reference_avg_sharess</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>593</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.386879</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>4.546154</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.022276</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.681548</td>\n",
       "      <td>0.381987</td>\n",
       "      <td>0.152189</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.353939</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>1300</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>4.759494</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.199626</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.714611</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.542580</td>\n",
       "      <td>0.122370</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357269</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.338889</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1100</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.003974</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.535390</td>\n",
       "      <td>5.147748</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.117255</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>0.425089</td>\n",
       "      <td>0.128515</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.337965</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.225794</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1600</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.378384</td>\n",
       "      <td>6.320768</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.424132</td>\n",
       "      <td>4.631390</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.327017</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>0.506520</td>\n",
       "      <td>0.279769</td>\n",
       "      <td>0.071749</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.417055</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.212354</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2400</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.783641</td>\n",
       "      <td>7.017506</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.302619</td>\n",
       "      <td>9.680406</td>\n",
       "      <td>8.140199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timedelta  n_tokens_title  n_unique_tokens  average_token_length  \\\n",
       "0      731.0            12.0         0.663594              4.680365   \n",
       "1      731.0             8.0         0.821705              4.546154   \n",
       "2      731.0             9.0         0.608602              4.759494   \n",
       "3      731.0            10.0         0.535390              5.147748   \n",
       "4      731.0             9.0         0.424132              4.631390   \n",
       "\n",
       "   num_keywords  kw_min_min  kw_avg_min  kw_max_max  kw_avg_max  kw_min_avg  \\\n",
       "0           5.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1           9.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2           7.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3          10.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4           8.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   kw_max_avg  is_weekend    LDA_00    LDA_01    LDA_02    LDA_03    LDA_04  \\\n",
       "0         0.0         0.0  0.500331  0.378279  0.040005  0.041263  0.040123   \n",
       "1         0.0         0.0  0.022265  0.022446  0.022276  0.251465  0.681548   \n",
       "2         0.0         0.0  0.028575  0.199626  0.028615  0.714611  0.028572   \n",
       "3         0.0         0.0  0.020011  0.020317  0.117255  0.020007  0.822410   \n",
       "4         0.0         0.0  0.025001  0.327017  0.025001  0.025001  0.597981   \n",
       "\n",
       "   global_subjectivity  global_sentiment_polarity  global_rate_positive_words  \\\n",
       "0             0.521617                   0.092562                    0.045662   \n",
       "1             0.381987                   0.152189                    0.038462   \n",
       "2             0.542580                   0.122370                    0.063291   \n",
       "3             0.425089                   0.128515                    0.039640   \n",
       "4             0.506520                   0.279769                    0.071749   \n",
       "\n",
       "   global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "0                    0.013699             0.769231             0.230769   \n",
       "1                    0.007692             0.833333             0.166667   \n",
       "2                    0.025316             0.714286             0.285714   \n",
       "3                    0.012613             0.758621             0.241379   \n",
       "4                    0.013453             0.842105             0.157895   \n",
       "\n",
       "   avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "0               0.378636               0.100000                    0.7   \n",
       "1               0.353939               0.033333                    0.7   \n",
       "2               0.357269               0.050000                    0.6   \n",
       "3               0.337965               0.050000                    0.7   \n",
       "4               0.417055               0.100000                    1.0   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.350000                   -0.6                 -0.200   \n",
       "1              -0.400000                   -0.4                 -0.400   \n",
       "2              -0.338889                   -1.0                 -0.050   \n",
       "3              -0.225794                   -0.4                 -0.125   \n",
       "4              -0.212354                   -0.5                 -0.050   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.500000                   -0.1875                0.000000   \n",
       "1            0.250000                    0.2000                0.250000   \n",
       "2            0.650000                   -0.5000                0.150000   \n",
       "3            0.500000                   -0.1000                0.000000   \n",
       "4            0.333333                    0.2500                0.166667   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares  year  month  log_shares  \\\n",
       "0                        0.1875     593  2013      1    6.386879   \n",
       "1                        0.2000    1300  2013      1    7.170888   \n",
       "2                        0.5000    1100  2013      1    7.003974   \n",
       "3                        0.1000    1600  2013      1    7.378384   \n",
       "4                        0.2500    2400  2013      1    7.783641   \n",
       "\n",
       "   log_n_tokens_content  log_num_hrefs  log_num_self_hrefs  log_num_imgs  \\\n",
       "0              5.393628       1.609438            1.098612      0.693147   \n",
       "1              4.875197       2.079442            1.609438      0.000000   \n",
       "2              6.163315       2.484907            0.000000      0.693147   \n",
       "3              6.320768       2.079442            1.945910      0.693147   \n",
       "4              7.017506       3.091042            3.091042      3.044522   \n",
       "\n",
       "   log_num_videos  log_kw_max_min  log_kw_min_max  log_kw_avg_avg  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   log_self_reference_min_shares  log_self_reference_max_shares  \\\n",
       "0                       6.208590                       6.208590   \n",
       "1                       7.170888                       7.170888   \n",
       "2                       0.000000                       0.000000   \n",
       "3                       7.550135                       7.550135   \n",
       "4                       6.302619                       9.680406   \n",
       "\n",
       "   log_self_reference_avg_sharess  \n",
       "0                        6.208590  \n",
       "1                        7.170888  \n",
       "2                        0.000000  \n",
       "3                        7.550135  \n",
       "4                        8.140199  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop certain columns\n",
    "df1 = df.drop('url_name', axis=1) # was a string\n",
    "df1 = df1.drop('date', axis=1) # datetime change didnt even work.\n",
    "df1 = df1.drop('day_of_week', axis=1) # other categorical variable\n",
    "df1 = df1.drop('news_category', axis=1) # other categorical variable\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef80c32",
   "metadata": {},
   "source": [
    "### For the logistic regression model we used the number of shares per online news article as the response variable and categorize the number of shares into ranges. The number of shares of a news article equate to popularity. Understanding which article features are needed to create an accurate model that can predict whether an article is popular or not and may provide some insight into what about the article features are driving popularity. Mashable could then share these findings with their journalists in order to adjust to their readers and hopefully create more ad revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6186be1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     39644.000000\n",
       "mean       3395.380184\n",
       "std       11626.950749\n",
       "min           1.000000\n",
       "25%         946.000000\n",
       "50%        1400.000000\n",
       "75%        2800.000000\n",
       "max      843300.000000\n",
       "Name: shares, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the `shares` column\n",
    "df1['shares'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2003c16e",
   "metadata": {},
   "source": [
    "### A new variable called 'share_ranges' was created by seperating shares into seven levels denoting the range at which the 'shares' value would be found for that article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2463189b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>log_shares</th>\n",
       "      <th>log_n_tokens_content</th>\n",
       "      <th>log_num_hrefs</th>\n",
       "      <th>log_num_self_hrefs</th>\n",
       "      <th>log_num_imgs</th>\n",
       "      <th>log_num_videos</th>\n",
       "      <th>log_kw_max_min</th>\n",
       "      <th>log_kw_min_max</th>\n",
       "      <th>log_kw_avg_avg</th>\n",
       "      <th>log_self_reference_min_shares</th>\n",
       "      <th>log_self_reference_max_shares</th>\n",
       "      <th>log_self_reference_avg_sharess</th>\n",
       "      <th>share_ranges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.200</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>593</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.386879</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>4.546154</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.022276</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.681548</td>\n",
       "      <td>0.381987</td>\n",
       "      <td>0.152189</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.353939</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>1300</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>4.759494</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.199626</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.714611</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.542580</td>\n",
       "      <td>0.122370</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357269</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.338889</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1100</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.003974</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.535390</td>\n",
       "      <td>5.147748</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.117255</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>0.425089</td>\n",
       "      <td>0.128515</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.337965</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.225794</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1600</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.378384</td>\n",
       "      <td>6.320768</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.424132</td>\n",
       "      <td>4.631390</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.327017</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>0.506520</td>\n",
       "      <td>0.279769</td>\n",
       "      <td>0.071749</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.417055</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.212354</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.050</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2400</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.783641</td>\n",
       "      <td>7.017506</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.302619</td>\n",
       "      <td>9.680406</td>\n",
       "      <td>8.140199</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timedelta  n_tokens_title  n_unique_tokens  average_token_length  \\\n",
       "0      731.0            12.0         0.663594              4.680365   \n",
       "1      731.0             8.0         0.821705              4.546154   \n",
       "2      731.0             9.0         0.608602              4.759494   \n",
       "3      731.0            10.0         0.535390              5.147748   \n",
       "4      731.0             9.0         0.424132              4.631390   \n",
       "\n",
       "   num_keywords  kw_min_min  kw_avg_min  kw_max_max  kw_avg_max  kw_min_avg  \\\n",
       "0           5.0         0.0         0.0         0.0         0.0         0.0   \n",
       "1           9.0         0.0         0.0         0.0         0.0         0.0   \n",
       "2           7.0         0.0         0.0         0.0         0.0         0.0   \n",
       "3          10.0         0.0         0.0         0.0         0.0         0.0   \n",
       "4           8.0         0.0         0.0         0.0         0.0         0.0   \n",
       "\n",
       "   kw_max_avg  is_weekend    LDA_00    LDA_01    LDA_02    LDA_03    LDA_04  \\\n",
       "0         0.0         0.0  0.500331  0.378279  0.040005  0.041263  0.040123   \n",
       "1         0.0         0.0  0.022265  0.022446  0.022276  0.251465  0.681548   \n",
       "2         0.0         0.0  0.028575  0.199626  0.028615  0.714611  0.028572   \n",
       "3         0.0         0.0  0.020011  0.020317  0.117255  0.020007  0.822410   \n",
       "4         0.0         0.0  0.025001  0.327017  0.025001  0.025001  0.597981   \n",
       "\n",
       "   global_subjectivity  global_sentiment_polarity  global_rate_positive_words  \\\n",
       "0             0.521617                   0.092562                    0.045662   \n",
       "1             0.381987                   0.152189                    0.038462   \n",
       "2             0.542580                   0.122370                    0.063291   \n",
       "3             0.425089                   0.128515                    0.039640   \n",
       "4             0.506520                   0.279769                    0.071749   \n",
       "\n",
       "   global_rate_negative_words  rate_positive_words  rate_negative_words  \\\n",
       "0                    0.013699             0.769231             0.230769   \n",
       "1                    0.007692             0.833333             0.166667   \n",
       "2                    0.025316             0.714286             0.285714   \n",
       "3                    0.012613             0.758621             0.241379   \n",
       "4                    0.013453             0.842105             0.157895   \n",
       "\n",
       "   avg_positive_polarity  min_positive_polarity  max_positive_polarity  \\\n",
       "0               0.378636               0.100000                    0.7   \n",
       "1               0.353939               0.033333                    0.7   \n",
       "2               0.357269               0.050000                    0.6   \n",
       "3               0.337965               0.050000                    0.7   \n",
       "4               0.417055               0.100000                    1.0   \n",
       "\n",
       "   avg_negative_polarity  min_negative_polarity  max_negative_polarity  \\\n",
       "0              -0.350000                   -0.6                 -0.200   \n",
       "1              -0.400000                   -0.4                 -0.400   \n",
       "2              -0.338889                   -1.0                 -0.050   \n",
       "3              -0.225794                   -0.4                 -0.125   \n",
       "4              -0.212354                   -0.5                 -0.050   \n",
       "\n",
       "   title_subjectivity  title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0            0.500000                   -0.1875                0.000000   \n",
       "1            0.250000                    0.2000                0.250000   \n",
       "2            0.650000                   -0.5000                0.150000   \n",
       "3            0.500000                   -0.1000                0.000000   \n",
       "4            0.333333                    0.2500                0.166667   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares  year  month  log_shares  \\\n",
       "0                        0.1875     593  2013      1    6.386879   \n",
       "1                        0.2000    1300  2013      1    7.170888   \n",
       "2                        0.5000    1100  2013      1    7.003974   \n",
       "3                        0.1000    1600  2013      1    7.378384   \n",
       "4                        0.2500    2400  2013      1    7.783641   \n",
       "\n",
       "   log_n_tokens_content  log_num_hrefs  log_num_self_hrefs  log_num_imgs  \\\n",
       "0              5.393628       1.609438            1.098612      0.693147   \n",
       "1              4.875197       2.079442            1.609438      0.000000   \n",
       "2              6.163315       2.484907            0.000000      0.693147   \n",
       "3              6.320768       2.079442            1.945910      0.693147   \n",
       "4              7.017506       3.091042            3.091042      3.044522   \n",
       "\n",
       "   log_num_videos  log_kw_max_min  log_kw_min_max  log_kw_avg_avg  \\\n",
       "0             0.0             0.0             0.0             0.0   \n",
       "1             0.0             0.0             0.0             0.0   \n",
       "2             0.0             0.0             0.0             0.0   \n",
       "3             0.0             0.0             0.0             0.0   \n",
       "4             0.0             0.0             0.0             0.0   \n",
       "\n",
       "   log_self_reference_min_shares  log_self_reference_max_shares  \\\n",
       "0                       6.208590                       6.208590   \n",
       "1                       7.170888                       7.170888   \n",
       "2                       0.000000                       0.000000   \n",
       "3                       7.550135                       7.550135   \n",
       "4                       6.302619                       9.680406   \n",
       "\n",
       "   log_self_reference_avg_sharess share_ranges  \n",
       "0                        6.208590        <2500  \n",
       "1                        7.170888        <2500  \n",
       "2                        0.000000        <2500  \n",
       "3                        7.550135        <2500  \n",
       "4                        8.140199        <2500  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column called `share_ranges` with categorical levels\n",
    "df1['share_ranges'] = pd.cut(df1['shares'], bins=[0, 2500, 5000, 7500, 10000, 20000, 100000, 1000000], labels=['<2500', '>2500 & <5000', '>5000 & <7500', '>7500 & <10000', '>10000 & <20000', '>20000 & <100000', '>100000'])\n",
    "\n",
    "# Print the DataFrame\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced8ac52",
   "metadata": {},
   "source": [
    "### Now that we have our new categorical column 'share_ranges' some data cleaning and sanity checks are done to avoid errors as we create our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5572a458",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<2500               28778\n",
      ">2500 & <5000        5794\n",
      ">5000 & <7500        1920\n",
      ">10000 & <20000      1367\n",
      ">7500 & <10000        967\n",
      ">20000 & <100000      760\n",
      ">100000                58\n",
      "Name: share_ranges, dtype: int64\n",
      "category\n"
     ]
    }
   ],
   "source": [
    "# Describe the `shares` column\n",
    "print(df1['share_ranges'].value_counts())\n",
    "print(df1['share_ranges'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c0b05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>log_shares</th>\n",
       "      <th>log_n_tokens_content</th>\n",
       "      <th>log_num_hrefs</th>\n",
       "      <th>log_num_self_hrefs</th>\n",
       "      <th>log_num_imgs</th>\n",
       "      <th>log_num_videos</th>\n",
       "      <th>log_kw_max_min</th>\n",
       "      <th>log_kw_min_max</th>\n",
       "      <th>log_kw_avg_avg</th>\n",
       "      <th>log_self_reference_min_shares</th>\n",
       "      <th>log_self_reference_max_shares</th>\n",
       "      <th>log_self_reference_avg_sharess</th>\n",
       "      <th>share_ranges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>593</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>6.386879</td>\n",
       "      <td>5.393628</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.208590</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>731.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>4.546154</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022265</td>\n",
       "      <td>0.022446</td>\n",
       "      <td>0.022276</td>\n",
       "      <td>0.251465</td>\n",
       "      <td>0.681548</td>\n",
       "      <td>0.381987</td>\n",
       "      <td>0.152189</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.353939</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>1300</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>4.875197</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.608602</td>\n",
       "      <td>4.759494</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028575</td>\n",
       "      <td>0.199626</td>\n",
       "      <td>0.028615</td>\n",
       "      <td>0.714611</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.542580</td>\n",
       "      <td>0.122370</td>\n",
       "      <td>0.063291</td>\n",
       "      <td>0.025316</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.357269</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-0.338889</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1100</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.003974</td>\n",
       "      <td>6.163315</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>731.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.535390</td>\n",
       "      <td>5.147748</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.020317</td>\n",
       "      <td>0.117255</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>0.425089</td>\n",
       "      <td>0.128515</td>\n",
       "      <td>0.039640</td>\n",
       "      <td>0.012613</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>0.337965</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.225794</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.1000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>1600</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.378384</td>\n",
       "      <td>6.320768</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.424132</td>\n",
       "      <td>4.631390</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.327017</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.025001</td>\n",
       "      <td>0.597981</td>\n",
       "      <td>0.506520</td>\n",
       "      <td>0.279769</td>\n",
       "      <td>0.071749</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.157895</td>\n",
       "      <td>0.417055</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.212354</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>2400</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>7.783641</td>\n",
       "      <td>7.017506</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.091042</td>\n",
       "      <td>3.044522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.302619</td>\n",
       "      <td>9.680406</td>\n",
       "      <td>8.140199</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39639</th>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.567227</td>\n",
       "      <td>4.313253</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>42.600000</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>571200.000000</td>\n",
       "      <td>2170.324903</td>\n",
       "      <td>3385.393320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.040020</td>\n",
       "      <td>0.040004</td>\n",
       "      <td>0.040008</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.839967</td>\n",
       "      <td>0.440992</td>\n",
       "      <td>0.266721</td>\n",
       "      <td>0.040161</td>\n",
       "      <td>0.008032</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.385909</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.145833</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1100</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>7.003974</td>\n",
       "      <td>5.521461</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>5.384495</td>\n",
       "      <td>9.994288</td>\n",
       "      <td>7.967156</td>\n",
       "      <td>8.071219</td>\n",
       "      <td>9.179984</td>\n",
       "      <td>8.771990</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39640</th>\n",
       "      <td>9.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.570136</td>\n",
       "      <td>4.589286</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>511.000000</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>310130.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>3900.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020009</td>\n",
       "      <td>0.219217</td>\n",
       "      <td>0.020005</td>\n",
       "      <td>0.118088</td>\n",
       "      <td>0.622681</td>\n",
       "      <td>0.384271</td>\n",
       "      <td>0.197662</td>\n",
       "      <td>0.044643</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.348636</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>1400</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>7.244942</td>\n",
       "      <td>5.416100</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>8.131825</td>\n",
       "      <td>7.313887</td>\n",
       "      <td>7.946497</td>\n",
       "      <td>7.003974</td>\n",
       "      <td>7.550135</td>\n",
       "      <td>7.424165</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39641</th>\n",
       "      <td>9.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.514925</td>\n",
       "      <td>4.263403</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>224885.714286</td>\n",
       "      <td>1880.000000</td>\n",
       "      <td>6433.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.172060</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.742224</td>\n",
       "      <td>0.434468</td>\n",
       "      <td>0.169252</td>\n",
       "      <td>0.039627</td>\n",
       "      <td>0.016317</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.391176</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.179847</td>\n",
       "      <td>-0.312500</td>\n",
       "      <td>-0.025000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3200</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>8.071219</td>\n",
       "      <td>6.063785</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.378384</td>\n",
       "      <td>8.366603</td>\n",
       "      <td>8.083845</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>&gt;2500 &amp; &lt;5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39642</th>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.506261</td>\n",
       "      <td>5.005172</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>88.857143</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>266628.571429</td>\n",
       "      <td>1558.755814</td>\n",
       "      <td>4966.668990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.028579</td>\n",
       "      <td>0.028573</td>\n",
       "      <td>0.792900</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.121376</td>\n",
       "      <td>0.428833</td>\n",
       "      <td>0.188667</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.008621</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.407333</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.115000</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>1700</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>7.438972</td>\n",
       "      <td>6.364751</td>\n",
       "      <td>2.772589</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.104793</td>\n",
       "      <td>9.752723</td>\n",
       "      <td>7.912769</td>\n",
       "      <td>7.244942</td>\n",
       "      <td>7.244942</td>\n",
       "      <td>7.244942</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39643</th>\n",
       "      <td>8.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.701987</td>\n",
       "      <td>4.471338</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>843300.0</td>\n",
       "      <td>366200.000000</td>\n",
       "      <td>3035.080555</td>\n",
       "      <td>3613.512953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.799339</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.050659</td>\n",
       "      <td>0.050001</td>\n",
       "      <td>0.517893</td>\n",
       "      <td>0.104892</td>\n",
       "      <td>0.063694</td>\n",
       "      <td>0.012739</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.247338</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>1300</td>\n",
       "      <td>2014</td>\n",
       "      <td>12</td>\n",
       "      <td>7.170888</td>\n",
       "      <td>5.062595</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>4.584967</td>\n",
       "      <td>12.233693</td>\n",
       "      <td>8.101044</td>\n",
       "      <td>7.650169</td>\n",
       "      <td>7.650169</td>\n",
       "      <td>7.650169</td>\n",
       "      <td>&lt;2500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39644 rows  49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       timedelta  n_tokens_title  n_unique_tokens  average_token_length  \\\n",
       "0          731.0            12.0         0.663594              4.680365   \n",
       "1          731.0             8.0         0.821705              4.546154   \n",
       "2          731.0             9.0         0.608602              4.759494   \n",
       "3          731.0            10.0         0.535390              5.147748   \n",
       "4          731.0             9.0         0.424132              4.631390   \n",
       "...          ...             ...              ...                   ...   \n",
       "39639        9.0            12.0         0.567227              4.313253   \n",
       "39640        9.0            13.0         0.570136              4.589286   \n",
       "39641        9.0            12.0         0.514925              4.263403   \n",
       "39642        9.0            15.0         0.506261              5.005172   \n",
       "39643        8.0            10.0         0.701987              4.471338   \n",
       "\n",
       "       num_keywords  kw_min_min  kw_avg_min  kw_max_max     kw_avg_max  \\\n",
       "0               5.0         0.0    0.000000         0.0       0.000000   \n",
       "1               9.0         0.0    0.000000         0.0       0.000000   \n",
       "2               7.0         0.0    0.000000         0.0       0.000000   \n",
       "3              10.0         0.0    0.000000         0.0       0.000000   \n",
       "4               8.0         0.0    0.000000         0.0       0.000000   \n",
       "...             ...         ...         ...         ...            ...   \n",
       "39639           5.0        -1.0   42.600000    843300.0  571200.000000   \n",
       "39640          10.0        -1.0  511.000000    843300.0  310130.000000   \n",
       "39641           7.0        -1.0  525.000000    843300.0  224885.714286   \n",
       "39642           7.0        -1.0   88.857143    843300.0  266628.571429   \n",
       "39643           4.0        -1.0   23.500000    843300.0  366200.000000   \n",
       "\n",
       "        kw_min_avg   kw_max_avg  is_weekend    LDA_00    LDA_01    LDA_02  \\\n",
       "0         0.000000     0.000000         0.0  0.500331  0.378279  0.040005   \n",
       "1         0.000000     0.000000         0.0  0.022265  0.022446  0.022276   \n",
       "2         0.000000     0.000000         0.0  0.028575  0.199626  0.028615   \n",
       "3         0.000000     0.000000         0.0  0.020011  0.020317  0.117255   \n",
       "4         0.000000     0.000000         0.0  0.025001  0.327017  0.025001   \n",
       "...            ...          ...         ...       ...       ...       ...   \n",
       "39639  2170.324903  3385.393320         0.0  0.040020  0.040004  0.040008   \n",
       "39640  1500.000000  3900.000000         0.0  0.020009  0.219217  0.020005   \n",
       "39641  1880.000000  6433.333333         0.0  0.028572  0.172060  0.028572   \n",
       "39642  1558.755814  4966.668990         0.0  0.028579  0.028573  0.792900   \n",
       "39643  3035.080555  3613.512953         0.0  0.050001  0.799339  0.050000   \n",
       "\n",
       "         LDA_03    LDA_04  global_subjectivity  global_sentiment_polarity  \\\n",
       "0      0.041263  0.040123             0.521617                   0.092562   \n",
       "1      0.251465  0.681548             0.381987                   0.152189   \n",
       "2      0.714611  0.028572             0.542580                   0.122370   \n",
       "3      0.020007  0.822410             0.425089                   0.128515   \n",
       "4      0.025001  0.597981             0.506520                   0.279769   \n",
       "...         ...       ...                  ...                        ...   \n",
       "39639  0.040000  0.839967             0.440992                   0.266721   \n",
       "39640  0.118088  0.622681             0.384271                   0.197662   \n",
       "39641  0.028572  0.742224             0.434468                   0.169252   \n",
       "39642  0.028571  0.121376             0.428833                   0.188667   \n",
       "39643  0.050659  0.050001             0.517893                   0.104892   \n",
       "\n",
       "       global_rate_positive_words  global_rate_negative_words  \\\n",
       "0                        0.045662                    0.013699   \n",
       "1                        0.038462                    0.007692   \n",
       "2                        0.063291                    0.025316   \n",
       "3                        0.039640                    0.012613   \n",
       "4                        0.071749                    0.013453   \n",
       "...                           ...                         ...   \n",
       "39639                    0.040161                    0.008032   \n",
       "39640                    0.044643                    0.004464   \n",
       "39641                    0.039627                    0.016317   \n",
       "39642                    0.025862                    0.008621   \n",
       "39643                    0.063694                    0.012739   \n",
       "\n",
       "       rate_positive_words  rate_negative_words  avg_positive_polarity  \\\n",
       "0                 0.769231             0.230769               0.378636   \n",
       "1                 0.833333             0.166667               0.353939   \n",
       "2                 0.714286             0.285714               0.357269   \n",
       "3                 0.758621             0.241379               0.337965   \n",
       "4                 0.842105             0.157895               0.417055   \n",
       "...                    ...                  ...                    ...   \n",
       "39639             0.833333             0.166667               0.385909   \n",
       "39640             0.909091             0.090909               0.348636   \n",
       "39641             0.708333             0.291667               0.391176   \n",
       "39642             0.750000             0.250000               0.407333   \n",
       "39643             0.833333             0.166667               0.247338   \n",
       "\n",
       "       min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "0                   0.100000                   0.70              -0.350000   \n",
       "1                   0.033333                   0.70              -0.400000   \n",
       "2                   0.050000                   0.60              -0.338889   \n",
       "3                   0.050000                   0.70              -0.225794   \n",
       "4                   0.100000                   1.00              -0.212354   \n",
       "...                      ...                    ...                    ...   \n",
       "39639               0.136364                   1.00              -0.145833   \n",
       "39640               0.100000                   0.50              -0.071429   \n",
       "39641               0.166667                   0.75              -0.179847   \n",
       "39642               0.100000                   0.80              -0.115000   \n",
       "39643               0.100000                   0.50              -0.200000   \n",
       "\n",
       "       min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                  -0.600000              -0.200000            0.500000   \n",
       "1                  -0.400000              -0.400000            0.250000   \n",
       "2                  -1.000000              -0.050000            0.650000   \n",
       "3                  -0.400000              -0.125000            0.500000   \n",
       "4                  -0.500000              -0.050000            0.333333   \n",
       "...                      ...                    ...                 ...   \n",
       "39639              -0.166667              -0.125000            0.000000   \n",
       "39640              -0.071429              -0.071429            0.800000   \n",
       "39641              -0.312500              -0.025000            0.000000   \n",
       "39642              -0.125000              -0.100000            0.500000   \n",
       "39643              -0.200000              -0.200000            0.333333   \n",
       "\n",
       "       title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                       -0.1875                0.000000   \n",
       "1                        0.2000                0.250000   \n",
       "2                       -0.5000                0.150000   \n",
       "3                       -0.1000                0.000000   \n",
       "4                        0.2500                0.166667   \n",
       "...                         ...                     ...   \n",
       "39639                    0.0000                0.500000   \n",
       "39640                    0.4000                0.300000   \n",
       "39641                    0.0000                0.500000   \n",
       "39642                    0.5000                0.000000   \n",
       "39643                    0.2500                0.166667   \n",
       "\n",
       "       abs_title_sentiment_polarity  shares  year  month  log_shares  \\\n",
       "0                            0.1875     593  2013      1    6.386879   \n",
       "1                            0.2000    1300  2013      1    7.170888   \n",
       "2                            0.5000    1100  2013      1    7.003974   \n",
       "3                            0.1000    1600  2013      1    7.378384   \n",
       "4                            0.2500    2400  2013      1    7.783641   \n",
       "...                             ...     ...   ...    ...         ...   \n",
       "39639                        0.0000    1100  2014     12    7.003974   \n",
       "39640                        0.4000    1400  2014     12    7.244942   \n",
       "39641                        0.0000    3200  2014     12    8.071219   \n",
       "39642                        0.5000    1700  2014     12    7.438972   \n",
       "39643                        0.2500    1300  2014     12    7.170888   \n",
       "\n",
       "       log_n_tokens_content  log_num_hrefs  log_num_self_hrefs  log_num_imgs  \\\n",
       "0                  5.393628       1.609438            1.098612      0.693147   \n",
       "1                  4.875197       2.079442            1.609438      0.000000   \n",
       "2                  6.163315       2.484907            0.000000      0.693147   \n",
       "3                  6.320768       2.079442            1.945910      0.693147   \n",
       "4                  7.017506       3.091042            3.091042      3.044522   \n",
       "...                     ...            ...                 ...           ...   \n",
       "39639              5.521461       1.609438            1.098612      0.693147   \n",
       "39640              5.416100       2.079442            2.079442      0.693147   \n",
       "39641              6.063785       1.386294            1.386294      1.386294   \n",
       "39642              6.364751       2.772589            1.098612      1.386294   \n",
       "39643              5.062595       0.693147            0.693147      0.000000   \n",
       "\n",
       "       log_num_videos  log_kw_max_min  log_kw_min_max  log_kw_avg_avg  \\\n",
       "0            0.000000        0.000000        0.000000        0.000000   \n",
       "1            0.000000        0.000000        0.000000        0.000000   \n",
       "2            0.000000        0.000000        0.000000        0.000000   \n",
       "3            0.000000        0.000000        0.000000        0.000000   \n",
       "4            0.000000        0.000000        0.000000        0.000000   \n",
       "...               ...             ...             ...             ...   \n",
       "39639        0.693147        5.384495        9.994288        7.967156   \n",
       "39640        0.693147        8.131825        7.313887        7.946497   \n",
       "39641        0.000000        7.378384        8.366603        8.083845   \n",
       "39642        0.000000        6.104793        9.752723        7.912769   \n",
       "39643        1.098612        4.584967       12.233693        8.101044   \n",
       "\n",
       "       log_self_reference_min_shares  log_self_reference_max_shares  \\\n",
       "0                           6.208590                       6.208590   \n",
       "1                           7.170888                       7.170888   \n",
       "2                           0.000000                       0.000000   \n",
       "3                           7.550135                       7.550135   \n",
       "4                           6.302619                       9.680406   \n",
       "...                              ...                            ...   \n",
       "39639                       8.071219                       9.179984   \n",
       "39640                       7.003974                       7.550135   \n",
       "39641                       7.170888                       7.170888   \n",
       "39642                       7.244942                       7.244942   \n",
       "39643                       7.650169                       7.650169   \n",
       "\n",
       "       log_self_reference_avg_sharess   share_ranges  \n",
       "0                            6.208590          <2500  \n",
       "1                            7.170888          <2500  \n",
       "2                            0.000000          <2500  \n",
       "3                            7.550135          <2500  \n",
       "4                            8.140199          <2500  \n",
       "...                               ...            ...  \n",
       "39639                        8.771990          <2500  \n",
       "39640                        7.424165          <2500  \n",
       "39641                        7.170888  >2500 & <5000  \n",
       "39642                        7.244942          <2500  \n",
       "39643                        7.650169          <2500  \n",
       "\n",
       "[39644 rows x 49 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "820f849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39644 entries, 0 to 39643\n",
      "Data columns (total 49 columns):\n",
      " #   Column                          Non-Null Count  Dtype   \n",
      "---  ------                          --------------  -----   \n",
      " 0   timedelta                       39644 non-null  float64 \n",
      " 1   n_tokens_title                  39644 non-null  float64 \n",
      " 2   n_unique_tokens                 39644 non-null  float64 \n",
      " 3   average_token_length            39644 non-null  float64 \n",
      " 4   num_keywords                    39644 non-null  float64 \n",
      " 5   kw_min_min                      39644 non-null  float64 \n",
      " 6   kw_avg_min                      39644 non-null  float64 \n",
      " 7   kw_max_max                      39644 non-null  float64 \n",
      " 8   kw_avg_max                      39644 non-null  float64 \n",
      " 9   kw_min_avg                      39644 non-null  float64 \n",
      " 10  kw_max_avg                      39644 non-null  float64 \n",
      " 11  is_weekend                      39644 non-null  float64 \n",
      " 12  LDA_00                          39644 non-null  float64 \n",
      " 13  LDA_01                          39644 non-null  float64 \n",
      " 14  LDA_02                          39644 non-null  float64 \n",
      " 15  LDA_03                          39644 non-null  float64 \n",
      " 16  LDA_04                          39644 non-null  float64 \n",
      " 17  global_subjectivity             39644 non-null  float64 \n",
      " 18  global_sentiment_polarity       39644 non-null  float64 \n",
      " 19  global_rate_positive_words      39644 non-null  float64 \n",
      " 20  global_rate_negative_words      39644 non-null  float64 \n",
      " 21  rate_positive_words             39644 non-null  float64 \n",
      " 22  rate_negative_words             39644 non-null  float64 \n",
      " 23  avg_positive_polarity           39644 non-null  float64 \n",
      " 24  min_positive_polarity           39644 non-null  float64 \n",
      " 25  max_positive_polarity           39644 non-null  float64 \n",
      " 26  avg_negative_polarity           39644 non-null  float64 \n",
      " 27  min_negative_polarity           39644 non-null  float64 \n",
      " 28  max_negative_polarity           39644 non-null  float64 \n",
      " 29  title_subjectivity              39644 non-null  float64 \n",
      " 30  title_sentiment_polarity        39644 non-null  float64 \n",
      " 31  abs_title_subjectivity          39644 non-null  float64 \n",
      " 32  abs_title_sentiment_polarity    39644 non-null  float64 \n",
      " 33  shares                          39644 non-null  int64   \n",
      " 34  year                            39644 non-null  int64   \n",
      " 35  month                           39644 non-null  int64   \n",
      " 36  log_shares                      39644 non-null  float64 \n",
      " 37  log_n_tokens_content            39644 non-null  float64 \n",
      " 38  log_num_hrefs                   39644 non-null  float64 \n",
      " 39  log_num_self_hrefs              39644 non-null  float64 \n",
      " 40  log_num_imgs                    39644 non-null  float64 \n",
      " 41  log_num_videos                  39644 non-null  float64 \n",
      " 42  log_kw_max_min                  39644 non-null  float64 \n",
      " 43  log_kw_min_max                  39644 non-null  float64 \n",
      " 44  log_kw_avg_avg                  39644 non-null  float64 \n",
      " 45  log_self_reference_min_shares   39644 non-null  float64 \n",
      " 46  log_self_reference_max_shares   39644 non-null  float64 \n",
      " 47  log_self_reference_avg_sharess  39644 non-null  float64 \n",
      " 48  share_ranges                    39644 non-null  category\n",
      "dtypes: category(1), float64(45), int64(3)\n",
      "memory usage: 14.6 MB\n"
     ]
    }
   ],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fdc77d",
   "metadata": {},
   "source": [
    "# Create Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988b3a87",
   "metadata": {},
   "source": [
    "### For the machine learning models, the 80/20 split was chosen as it is a common practice in machine learning and is used to prevent overfitting.\n",
    "\n",
    "### To avoid redundancy, the response variable 'share_ranges' and 'log shares' were removed from the dataset before the split. The log_shares variable was created during the EDA process however it was determined that using the share_ranges was a more appropriate response variable for SVM and Logistic Regression modeling. To improve the performance of the models, the data splits were scaled so that features were normalized to reduce the impact of those features with large ranges of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a20bc638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timedelta', 'n_tokens_title', 'n_unique_tokens',\n",
      "       'average_token_length', 'num_keywords', 'kw_min_min', 'kw_avg_min',\n",
      "       'kw_max_max', 'kw_avg_max', 'kw_min_avg', 'kw_max_avg', 'is_weekend',\n",
      "       'LDA_00', 'LDA_01', 'LDA_02', 'LDA_03', 'LDA_04', 'global_subjectivity',\n",
      "       'global_sentiment_polarity', 'global_rate_positive_words',\n",
      "       'global_rate_negative_words', 'rate_positive_words',\n",
      "       'rate_negative_words', 'avg_positive_polarity', 'min_positive_polarity',\n",
      "       'max_positive_polarity', 'avg_negative_polarity',\n",
      "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
      "       'title_sentiment_polarity', 'abs_title_subjectivity',\n",
      "       'abs_title_sentiment_polarity', 'shares', 'year', 'month',\n",
      "       'log_n_tokens_content', 'log_num_hrefs', 'log_num_self_hrefs',\n",
      "       'log_num_imgs', 'log_num_videos', 'log_kw_max_min', 'log_kw_min_max',\n",
      "       'log_kw_avg_avg', 'log_self_reference_min_shares',\n",
      "       'log_self_reference_max_shares', 'log_self_reference_avg_sharess'],\n",
      "      dtype='object')\n",
      "0                <2500\n",
      "1                <2500\n",
      "2                <2500\n",
      "3                <2500\n",
      "4                <2500\n",
      "             ...      \n",
      "39639            <2500\n",
      "39640            <2500\n",
      "39641    >2500 & <5000\n",
      "39642            <2500\n",
      "39643            <2500\n",
      "Name: share_ranges, Length: 39644, dtype: category\n",
      "Categories (7, object): ['<2500' < '>2500 & <5000' < '>5000 & <7500' < '>7500 & <10000' < '>10000 & <20000' < '>20000 & <100000' < '>100000']\n"
     ]
    }
   ],
   "source": [
    "X = df1.drop(['share_ranges', 'log_shares'], axis=1)\n",
    "y = df1['share_ranges']\n",
    "print(X.columns)\n",
    "print(y)\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "# Scale the features in the training and testing sets.\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fa24e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check:  Check for NaNs in the dataset\n",
    "for column in df1.columns:\n",
    "    if df1[column].isnull().any():\n",
    "        print('NaNs found in column:', column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c7dc8",
   "metadata": {},
   "source": [
    "## Logistic Regression Model: \n",
    "\n",
    "### Tuning: \n",
    "For the logistic regression model, the regularization strength parameter was set at 0.5.  The Lasso method was used for the ability to robustly handle noisy data and penalize absolute values.  The Stochastic Average Gradient Aggregation (SAGA) was used to optimize the training of the logistic regression model.  The SAGA provided faster convergence and improved stability for the training of the model.  The maximum number of iterations chosen for the model was 10000, to ensure an optimal outcome from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5310427a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.05, max_iter=10000, penalty='l1', solver='saga')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model parameters.\n",
    "C = 0.05\n",
    "penalty = 'l1'\n",
    "solver = 'saga'\n",
    "max_iter=10000\n",
    "\n",
    "# Create a logistic regression model.\n",
    "logistic_regression_model = LogisticRegression(C=C, penalty=penalty, solver=solver, max_iter=max_iter)\n",
    "# Fit the model to the training data.\n",
    "logistic_regression_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8bcf26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression cross-validated accuracy: 0.9194387513794734\n"
     ]
    }
   ],
   "source": [
    "# Calculate the cross-validated accuracy of the model.\n",
    "logistic_regression_cross_validated_accuracy = cross_val_score(logistic_regression_model, X_train, y_train, cv=5)\n",
    "\n",
    "# Print the cross-validated accuracy of the model.\n",
    "print('Logistic regression cross-validated accuracy:', logistic_regression_cross_validated_accuracy.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9616f7",
   "metadata": {},
   "source": [
    "##### A high cross validation score indicates that we should move on with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23cccd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data.\n",
    "y_pred = logistic_regression_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94eecf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy: 0.9255896077689494\n",
      "Logistic regression precision: 0.8494424067493583\n",
      "Logistic regression recall: 0.6890616004467248\n",
      "Logistic regression F1 score: 0.7164776184440045\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_accuracy = accuracy_score(y_test, y_pred)\n",
    "logistic_regression_precision = precision_score(y_test, y_pred, average='macro')\n",
    "logistic_regression_recall = recall_score(y_test, y_pred, average='macro')\n",
    "logistic_regression_f1_score = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print the metrics.\n",
    "print('Logistic regression accuracy:', logistic_regression_accuracy)\n",
    "print('Logistic regression precision:', logistic_regression_precision)\n",
    "print('Logistic regression recall:', logistic_regression_recall)\n",
    "print('Logistic regression F1 score:', logistic_regression_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86202381",
   "metadata": {},
   "source": [
    "## Support Vector Machine Model:\n",
    "\n",
    "### Tuning: \n",
    "For the support vector machine model (SVM), the regularization strength (C) was set to 1.0.  This setting strikes a balance, allowing the SVM to exhibit flexibility in fitting the data while simultaneously preserving a well-defined margin.  A linear kernel function was used represent a decision boundary as a straight line in the feature space.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8035c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(kernel='linear')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the model parameters\n",
    "C = 1.0\n",
    "kernel = 'linear'\n",
    "\n",
    "# Create the support vector machine model\n",
    "support_vector_machine_model = SVC(C=C, kernel=kernel)\n",
    "\n",
    "# Train the support vector machine model on the training set using a linear kernel.\n",
    "support_vector_machine_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "487fb7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM cross-validated accuracy: 0.9860633769509697\n"
     ]
    }
   ],
   "source": [
    "# Do cross validation\n",
    "support_vector_machine_cross_validated_accuracy = cross_val_score(support_vector_machine_model, X_train, y_train, cv=5)\n",
    "\n",
    "# Check accuracy from cross validation\n",
    "print('SVM cross-validated accuracy:', support_vector_machine_cross_validated_accuracy.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c5069a",
   "metadata": {},
   "source": [
    "##### A high cross validation score indicates that we should move on with this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c3ab1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data.\n",
    "y_pred = support_vector_machine_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "426fec8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Support vector machine accuracy: 0.9904149325261697\n",
      "Support vector machine precision: 0.978781438391208\n",
      "Support vector machine recall: 0.9741562206299955\n",
      "Support vector machine F1 score: 0.9764193565401654\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data.\n",
    "support_vector_machine_accuracy = accuracy_score(y_test, y_pred)\n",
    "support_vector_machine_precision = precision_score(y_test, y_pred, average='macro')\n",
    "support_vector_machine_recall = recall_score(y_test, y_pred, average='macro')\n",
    "support_vector_machine_f1_score = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Print the metrics.\n",
    "print('Support vector machine accuracy:', support_vector_machine_accuracy)\n",
    "print('Support vector machine precision:', support_vector_machine_precision)\n",
    "print('Support vector machine recall:', support_vector_machine_recall)\n",
    "print('Support vector machine F1 score:', support_vector_machine_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d282b7",
   "metadata": {},
   "source": [
    "## Optimizing Using SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25042de",
   "metadata": {},
   "source": [
    "### SVM using SGD\n",
    "\n",
    "### Tuning: \n",
    "The SVM model is using SGDClassifier to fit a linear model. We chose a regularization penalty of 0.0001 which is an extremely low penalty but a higher alpha lead to a more regularized model and the model performed worse. Lasso was used and an optimal learning rate was selected to increase efficiency of the model. The loss parameter is hinge which gives a linear SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "467b0282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(l1_ratio=0.0, max_iter=10000, n_iter_no_change=1000, penalty='l1')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# Set the model parameters\n",
    "alpha = 0.0001\n",
    "fit_intercept = True\n",
    "l1_ratio = 0.0\n",
    "learning_rate = 'optimal'\n",
    "loss = 'hinge' # gives a linear SVM \n",
    "n_iter_no_change = 1000\n",
    "max_iter= 10000\n",
    "# Lasso \n",
    "penalty = 'l1'\n",
    "\n",
    "# Initialize the SVM model.\n",
    "support_vector_machine_model_sgd = SGDClassifier(alpha=alpha,fit_intercept=fit_intercept, l1_ratio=l1_ratio, learning_rate=learning_rate, loss=loss, n_iter_no_change=n_iter_no_change, penalty=penalty,max_iter=max_iter)\n",
    "\n",
    "# Train the support vector machine model on the training set using a linear kernel.\n",
    "support_vector_machine_model_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "01ff3733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data.\n",
    "y_pred = support_vector_machine_model_sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0ba6f3b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------\n",
      "Support vector machine using SGD accuracy: 0.8336486316054988\n",
      "Support vector machine using SGD precision: 0.5095303733523482\n",
      "Support vector machine using SGD recall: 0.5526900541561853\n",
      "Support vector machine using SGD F1 score: 0.5116293699248358\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy, precision, recall, and F1 score of the model on the test data.\n",
    "support_vector_machine_accuracy_sgd = accuracy_score(y_test, y_pred)\n",
    "support_vector_machine_precision_sgd = precision_score(y_test, y_pred, average='macro')\n",
    "support_vector_machine_recall_sgd = recall_score(y_test, y_pred, average='macro')\n",
    "support_vector_machine_f1_score_sgd= f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print('---------------------------------------------------------')\n",
    "\n",
    "# Print the metrics.\n",
    "print('Support vector machine using SGD accuracy:', support_vector_machine_accuracy_sgd)\n",
    "print('Support vector machine using SGD precision:', support_vector_machine_precision_sgd)\n",
    "print('Support vector machine using SGD recall:', support_vector_machine_recall_sgd)\n",
    "print('Support vector machine using SGD F1 score:', support_vector_machine_f1_score_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de88211a",
   "metadata": {},
   "source": [
    "##### SVM - SGD performed worse on metrics overall than the SVM model without using SGD. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9828cb",
   "metadata": {},
   "source": [
    "### Logistic Regression using SGD\n",
    "\n",
    "#### Tuning: \n",
    "The Logistical regression model used SGDClassifier to fit a linear model. We chose a regularization penalty of 0.0001 which is an  low penalty but a higher alpha lead to a more regularized model and the model performed worse. Lasso was used and an optimal learning rate was selected to increase efficiency of the model. The loss parameter is loss which gives a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38973207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "alpha = .0001\n",
    "fit_intercept = True\n",
    "l1_ratio = 0.0\n",
    "learning_rate = 'optimal'\n",
    "loss = 'log' #use log for logistic regression \n",
    "n_iter_no_change = 500\n",
    "# lasso \n",
    "penalty = 'l1'\n",
    "\n",
    "# Initialize the model.\n",
    "logistic_regression_log_sgd = SGDClassifier(alpha=alpha,fit_intercept=fit_intercept, l1_ratio=l1_ratio, learning_rate=learning_rate, loss=loss, n_iter_no_change=n_iter_no_change, penalty=penalty)\n",
    "\n",
    "# Train the support vector machine model on the training set using a linear kernel.\n",
    "logistic_regression_log_sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "875e7bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data.\n",
    "y_pred = logistic_regression_log_sgd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc2237ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression using SGD accuracy: 0.8842224744608399\n",
      "Logistic regression using SGD precision: 0.5728547894636675\n",
      "Logistic regression using SGD recall: 0.5214161314706651\n",
      "Logistic regression using SGD F1 score: 0.5003674339374432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bfo2f\\.conda\\envs\\ML1\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_accuracy_sgd = accuracy_score(y_test, y_pred)\n",
    "logistic_regression_precision_sgd = precision_score(y_test, y_pred, average='macro')\n",
    "logistic_regression_recall_sgd = recall_score(y_test, y_pred, average='macro')\n",
    "logistic_regression_f1_score_sgd = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print the metrics.\n",
    "print('Logistic regression using SGD accuracy:', logistic_regression_accuracy_sgd)\n",
    "print('Logistic regression using SGD precision:', logistic_regression_precision_sgd)\n",
    "print('Logistic regression using SGD recall:', logistic_regression_recall_sgd)\n",
    "print('Logistic regression using SGD F1 score:', logistic_regression_f1_score_sgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1bbd49",
   "metadata": {},
   "source": [
    "# Model Advantages\n",
    "\n",
    "## Discuss the advantages of each model for each classification task. Does one type of model offer superior performance over another in terms of prediction accuracy? In terms of training time or efficiency? Explain in detail.\n",
    "\n",
    "### Logistic Regression\n",
    "#### Model Advantages - Efficiency: \n",
    "The model had a convergence rate of approximately three minutes, suggesting that the model isn't particularly complex. The incorporation of the SAGA optimization method further facilitated this swift convergence.\n",
    "\n",
    "#### Model Advantages - Performance\n",
    "The logistic regression model demonstrated a strong performance by achieving an accuracy score of 0.9255896077689494. Although the accuracy of this model was strong, it was not as strong as the SVM model or the SVM (SVC) model. The model also exhibited a relatively high precision score of 0.8494424067493583, indicating minimal occurrences of false positives. The model also achieved a logistic regression recall of 0.6890616004467248, reflecting a good proportion of true positives. The logistic regression F1 score exhibited a high value of 0.7164776184440045, indicating the models overall strong capability to accurately differentiate between popular and non-popular online news articles. Overall the model was effective in categorizing the online news articles as being popular or not.\n",
    "\n",
    "### Logistic Regression with SGD\n",
    "#### Model Advantages - Efficiency: \n",
    "The models convergence rate was ~36.7 seconds. This is fast indicating our model is not complex.  \n",
    "\n",
    "#### Model Advantages - Performance\n",
    "The Logistic Regression with Stochastic Gradient Descent (SGD) model demonstrated a good performance by achieving an accuracy score of 0.8842224744608399. This accuracy of this model was one of the weakest of the models, with the SVM (SGD) being the worst in this report. The model had an adequate precision score of 0.5728547894636675, indicating that there is room for improvement in reducing the rate of falsely classified articles as popular when they are not.  The model also achieved a logistic regression (SGD) recall of 0.5214161314706651, indicating a fair model for predicting true positives. The models logistic regression (SGD) F1 score was 0.5003674339374432, suggests that the model possesses some capability in distinguishing between popular and non-popular online news articles. Overall the model may be suitable for accurately categorizing the online news articles as being popular or not.\n",
    "\n",
    "### Support Vector Machine\n",
    "#### Model Advantages - Efficiency: \n",
    "The model had a convergence rate of approximately 14.4 seconds, suggesting that the model isnt particularly complex.  \n",
    "\n",
    "#### Model Advantages - Performance\n",
    "The SVM model demonstrated a strong performance by achieving an accuracy score of 0.9904149325261697. This performance of this model exceeded that of all the models. The model also exhibited a relatively high precision score of 0.978781438391208, indicating minimal occurrences of false positives.  The SVM recall exhibited a relatively high score of 0.9741562206299955, reflecting a high proportion of true positives. The SVM F1 score also exhibited a relatively high score of 0.9764193565401654, indicating the models overall strong capability to accurately differentiate between popular and non-popular online news articles. Overall the model did extremely well with its ability to accurately classify online news articles as being popular or not.\n",
    "\n",
    "### Support Vector Machine with SGD\n",
    "#### Model Advantages - Efficiency: \n",
    "The models convergence rate was ~23.5 seconds. This is fast indicating our model is not complex.  \n",
    "\n",
    "#### Model Advantages - Performance\n",
    "The SVM with Stochastic Gradient Descent (SGD) model demonstrated a good performance by achieving an accuracy score of 0.8336486316054988. This accuracy of this model was the weakest of the models in this report. The model had an adequate precision score of 0.5095303733523482, indicating that there is room for improvement in reducing the rate of falsely classified articles as popular when they are not. The model also achieved a SVM (SDG) recall of 0.5526900541561853, indicating a decent model for predicting true positives. The models SVM (SGD) F1 score was 0.5116293699248358, suggests that the model possesses some capability in distinguishing between popular and non-popular online news articles. Overall the model may be suitable for accurately categorizing the online news articles as being popular or not.\n",
    "\n",
    "### SVM (SVC)\n",
    "#### Model Advantages - Performance\n",
    "The SVM (SVC) model demonstrated a strong performance by achieving an accuracy score of 0.987892546348846. This performance of this model exceeded that of all the models except for the SVM model, which had the highest performance. The model also exhibited a relatively high precision score of 0.9520786847783501, indicating minimal occurrences of false positives. The SVM (SVC) recall exhibited a relatively high score of 0.9636874071308031, reflecting a high proportion of true positives. The SVM(SVC) F1 score also exhibited a relatively high score of 0.9572471107774686, indicating the models overall strong capability to accurately differentiate between popular and non-popular online news articles. Overall the model did extremely well with its ability to accurately classify online news articles as being popular or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c908664",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "##### Our classifcation task was to predict what range of shares an article would belong to based off other features of the article. Overall, we found the best model to be the Support Vector Machine model. This model showed higher metrics in each category when compared to the other four models. The previously listed advantages on top of these metric comparison support the SVM model as the best for this classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e32da26",
   "metadata": {},
   "source": [
    "# Interpret Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bec6cd",
   "metadata": {},
   "source": [
    "## Use the weights from logistic regression to interpret the importance of different features for each classification task. Explain your interpretation in detail. Why do you think some variables are more important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4738ba",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37735d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url_name has weight of -0.007155539328913435\n",
      "date has weight of 0.0\n",
      "timedelta has weight of 0.0\n",
      "n_tokens_title has weight of 0.02969453884305824\n",
      "n_unique_tokens has weight of 0.0\n",
      "average_token_length has weight of 0.0\n",
      "num_keywords has weight of 0.0\n",
      "kw_min_min has weight of 0.0\n",
      "kw_avg_min has weight of 0.0\n",
      "kw_max_max has weight of -0.05578507163583383\n",
      "kw_avg_max has weight of -0.012538231082097456\n",
      "kw_min_avg has weight of 0.0\n",
      "kw_max_avg has weight of 0.0\n",
      "is_weekend has weight of 0.0\n",
      "LDA_00 has weight of 0.0004717852977236848\n",
      "LDA_01 has weight of 0.0\n",
      "LDA_02 has weight of 0.0\n",
      "LDA_03 has weight of -0.009093886045133769\n",
      "LDA_04 has weight of 0.0\n",
      "global_subjectivity has weight of 0.0\n",
      "global_sentiment_polarity has weight of 0.0\n",
      "global_rate_positive_words has weight of 0.0\n",
      "global_rate_negative_words has weight of 0.0\n",
      "rate_positive_words has weight of 0.0\n",
      "rate_negative_words has weight of 0.015619388717560386\n",
      "avg_positive_polarity has weight of 0.0\n",
      "min_positive_polarity has weight of 0.0\n",
      "max_positive_polarity has weight of 0.0047991513835828\n",
      "avg_negative_polarity has weight of 0.0\n",
      "min_negative_polarity has weight of 0.0\n",
      "max_negative_polarity has weight of 0.0\n",
      "title_subjectivity has weight of -0.015208314945134862\n",
      "title_sentiment_polarity has weight of -0.03485860510063452\n",
      "abs_title_subjectivity has weight of -40.20261357757811\n",
      "abs_title_sentiment_polarity has weight of 0.0018742808536748672\n",
      "shares has weight of 0.0\n",
      "day_of_week has weight of 0.0\n",
      "news_category has weight of -0.00045230483825831015\n",
      "year has weight of 0.0\n",
      "month has weight of 0.0\n",
      "log_shares has weight of 0.0\n",
      "log_n_tokens_content has weight of 0.0\n",
      "log_num_hrefs has weight of 0.05098574496406618\n",
      "log_num_self_hrefs has weight of -0.05038425077449816\n",
      "log_num_imgs has weight of -0.030674773098127314\n",
      "log_num_videos has weight of 0.0\n",
      "log_kw_max_min has weight of 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get the weights from the trained model.\n",
    "weights = logistic_regression_model.coef_.T\n",
    "\n",
    "# Print the weight of each variable.\n",
    "for weight, variable_name in zip(weights, df.columns[:-1]):\n",
    "    print(f'{variable_name} has weight of {weight[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894cfbbc",
   "metadata": {},
   "source": [
    "#### Interpret Feature Importance - Weighted Coefficients Explanation:\n",
    "The models weighted coefficients revealed valuable insight into the relationship between the features and the response variable.  Within the feature set there were a range of slight positive or negative correlations with the response variable.  One feature, absolute subjectivity level, exhibited a significant negative correlation with the response variable.  This finding implies that any online new article with higher levels of subjectivity are less likely to become popular.  The weight of absolute subjectivity level greatly exceeds that of other weighted coefficients, indicating how strong the correlation is with the response variable as compared to other factors.\n",
    "\n",
    "#### Weights:\n",
    "abs_title_subjectivity has weight of -40.20261357757811 -- significant negative correlation\n",
    "##### Absolute value subjectivity is the degree to which a statement or opinion is subjective, without regard to its direction (positive or negative). This variable is likely so much more important than others due to the title of the artice being the first things everyone reads. Very polarizing statements are less capable of convincing people to share. From this Mashle could surmise that writing less polarizing statements or opinions could increase the amount of shares an article receives. \n",
    "\n",
    "url_name has weight of -0.007155539328913435 - slight negative correlation\n",
    "\n",
    "n_tokens_title has weight of 0.02969453884305824 - slight positive correlation \n",
    "\n",
    "kw_max_max has weight of -0.05578507163583383 - slight negative correlation \n",
    "\n",
    "\n",
    "kw_avg_max has weight of -0.012538231082097456 - slight negative correlation \n",
    "\n",
    "\n",
    "LDA_00 has weight of 0.0004717852977236848 - slight positive correlation \n",
    "\n",
    "LDA_03 has weight of -0.009093886045133769 - slight negative correlation \n",
    "\n",
    "rate_negative_words has weight of 0.015619388717560386 - slight positive correlation \n",
    "\n",
    "max_positive_polarity has weight of 0.0047991513835828 - slight positive correlation \n",
    "\n",
    "title_subjectivity has weight of -0.015208314945134862 - slight negative correlation \n",
    "\n",
    "title_sentiment_polarity has weight of -0.03485860510063452 - slight negative correlation \n",
    "\n",
    "abs_title_sentiment_polarity has weight of 0.0018742808536748672 - slight positive correlation \n",
    "\n",
    "news_category has weight of -0.00045230483825831015 - slight negative correlation \n",
    "\n",
    "log_num_hrefs has weight of 0.05098574496406618 - slight positive correlation \n",
    "\n",
    "log_num_self_hrefs has weight of -0.05038425077449816 - slight negative correlation \n",
    "\n",
    "log_num_imgs has weight of -0.030674773098127314 - slight negative correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481fab46",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55d1df7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url_name has weight of 0.004224610225037617\n",
      "date has weight of -9.779298907781897e-05\n",
      "timedelta has weight of -0.012354444389279986\n",
      "n_tokens_title has weight of -1.963497733459274e-05\n",
      "n_unique_tokens has weight of 5.707727329928858e-05\n",
      "average_token_length has weight of -9.083277849442073e-05\n",
      "num_keywords has weight of 0.0003029711223826864\n",
      "kw_min_min has weight of -0.00016142625453019477\n",
      "kw_avg_min has weight of -5.0646105330148794e-05\n",
      "kw_max_max has weight of 0.0001513461180681075\n",
      "kw_avg_max has weight of -2.6099687903874402e-05\n",
      "kw_min_avg has weight of -3.680483164825166e-05\n",
      "kw_max_avg has weight of -2.8699910095131287e-06\n",
      "is_weekend has weight of -2.757023039885731e-05\n",
      "LDA_00 has weight of 2.3725277128461286e-05\n",
      "LDA_01 has weight of -2.4699526460780774e-05\n",
      "LDA_02 has weight of 2.5567487829544766e-05\n",
      "LDA_03 has weight of -0.00016522623532955194\n",
      "LDA_04 has weight of 0.0002605617903491675\n",
      "global_subjectivity has weight of 1.7256980799906785e-06\n",
      "global_sentiment_polarity has weight of -4.6340397138910916e-05\n",
      "global_rate_positive_words has weight of -0.00012171434485708232\n",
      "global_rate_negative_words has weight of 0.00014795287422664227\n",
      "rate_positive_words has weight of -0.0001896965250170468\n",
      "rate_negative_words has weight of 5.144430063874772e-05\n",
      "avg_positive_polarity has weight of 0.0001825759700648799\n",
      "min_positive_polarity has weight of 5.242195785537529e-05\n",
      "max_positive_polarity has weight of -0.00013678804865757677\n",
      "avg_negative_polarity has weight of -0.0001694947826806903\n",
      "min_negative_polarity has weight of 6.674635951098296e-05\n",
      "max_negative_polarity has weight of -3.713899601032189e-05\n",
      "title_subjectivity has weight of -0.0001338859984638674\n",
      "title_sentiment_polarity has weight of -7.676545513127242e-05\n",
      "abs_title_subjectivity has weight of -3.2466337893453776\n",
      "abs_title_sentiment_polarity has weight of 0.003706528882303717\n",
      "shares has weight of 0.0021782690302249996\n",
      "day_of_week has weight of -0.0006898725701414143\n",
      "news_category has weight of -1.874804875434677e-05\n",
      "year has weight of -4.082322600251764e-05\n",
      "month has weight of -0.0001462436108377041\n",
      "log_shares has weight of -2.3412961033031365e-05\n",
      "log_n_tokens_content has weight of -8.372188423644289e-05\n",
      "log_num_hrefs has weight of -0.00014290478374134885\n",
      "log_num_self_hrefs has weight of -0.00011891585306411656\n",
      "log_num_imgs has weight of -7.287393465560399e-05\n",
      "log_num_videos has weight of 0.0017717641650130123\n",
      "log_kw_max_min has weight of -0.0015840527506587598\n"
     ]
    }
   ],
   "source": [
    "weights = support_vector_machine_model.coef_.T\n",
    "\n",
    "# # Print the weight of each variable.\n",
    "for weight, variable_name in zip(weights, df.columns[:-1]):\n",
    "    print(f'{variable_name} has weight of {weight[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8731cd2",
   "metadata": {},
   "source": [
    "#### Interpret Feature Importance - Weighted Coefficients Explanation:\n",
    "Similar to the logistic regression model, the SVM models weighted coefficients indicated that several of the features with slight positive or negative correlations with the response variable.  One feature, absolute subjectivity level, exhibited a significant negative correlation with the response variable.  This finding implies that any new article with higher levels of subjectivity are less likely to be classified as popular. The weight of absolute subjectivity level greatly exceeds that of other weighted coefficients, indicating how strong the correlation is with the response variable as compared to other factors.\n",
    "\n",
    "#### Weights: \n",
    "url_name has weight of 0.004224610225037617 - slight positive correlation\n",
    "\n",
    "date has weight of -9.779298907781897e-05 - slight negative correlation\n",
    "\n",
    "timedelta has weight of -0.012354444389279986 - slight negative correlation\n",
    "\n",
    "n_tokens_title has weight of -1.963497733459274e-05 - slight negative correlation\n",
    "\n",
    "n_unique_tokens has weight of 5.707727329928858e-05 - slight positive correlation\n",
    "\n",
    "average_token_length has weight of -9.083277849442073e-05 - slight negative correlation\n",
    "\n",
    "num_keywords has weight of 0.0003029711223826864 - slight positive correlation\n",
    "\n",
    "kw_min_min has weight of -0.00016142625453019477 - slight negative correlation\n",
    "\n",
    "kw_avg_min has weight of -5.0646105330148794e-05 - slight negative correlation\n",
    "\n",
    "kw_max_max has weight of 0.0001513461180681075 - slight positive correlation\n",
    "\n",
    "kw_avg_max has weight of -2.6099687903874402e-05 - slight negative correlation\n",
    "\n",
    "kw_min_avg has weight of -3.680483164825166e-05 - slight negative correlation\n",
    "\n",
    "kw_max_avg has weight of -2.8699910095131287e-06 - slight negative correlation\n",
    "\n",
    "is_weekend has weight of -2.757023039885731e-05 - slight negative correlation\n",
    "\n",
    "LDA_00 has weight of 2.3725277128461286e-05 - slight positive correlation\n",
    "\n",
    "LDA_01 has weight of -2.4699526460780774e-05 - slight negative correlation\n",
    "\n",
    "LDA_02 has weight of 2.5567487829544766e-05 - slight positive correlation\n",
    "\n",
    "LDA_03 has weight of -0.00016522623532955194 - slight negative correlation\n",
    "\n",
    "LDA_04 has weight of 0.0002605617903491675 - slight positive correlation\n",
    "\n",
    "global_subjectivity has weight of 1.7256980799906785e-06 - slight positive correlation\n",
    "\n",
    "global_sentiment_polarity has weight of -4.6340397138910916e-05 - slight negative correlation\n",
    "\n",
    "global_rate_positive_words has weight of -0.00012171434485708232 - slight negative correlation\n",
    "\n",
    "global_rate_negative_words has weight of 0.00014795287422664227 - slight positive correlation\n",
    "\n",
    "rate_positive_words has weight of -0.0001896965250170468 - slight negative correlation\n",
    "\n",
    "rate_negative_words has weight of 5.144430063874772e-05 - slight positive correlation\n",
    "\n",
    "avg_positive_polarity has weight of 0.0001825759700648799 - slight positive correlation\n",
    "\n",
    "min_positive_polarity has weight of 5.242195785537529e-05 - slight positive correlation\n",
    "\n",
    "max_positive_polarity has weight of -0.00013678804865757677 - slight negative correlation\n",
    "\n",
    "avg_negative_polarity has weight of -0.0001694947826806903 - slight negative correlation\n",
    "\n",
    "min_negative_polarity has weight of 6.674635951098296e-05 - slight positive correlation\n",
    "\n",
    "max_negative_polarity has weight of -3.713899601032189e-05 - slight negative correlation\n",
    "\n",
    "title_subjectivity has weight of -0.0001338859984638674 - slight negative correlation\n",
    "\n",
    "title_sentiment_polarity has weight of -7.676545513127242e-05 - slight negative correlation\n",
    "\n",
    "abs_title_subjectivity has weight of -3.2466337893453776 - negative correlation\n",
    "\n",
    "abs_title_sentiment_polarity has weight of 0.003706528882303717 - slight positive correlation\n",
    "\n",
    "shares has weight of 0.0021782690302249996 - slight positive correlation\n",
    "\n",
    "day_of_week has weight of -0.0006898725701414143 - slight negative correlation\n",
    "\n",
    "news_category has weight of -1.874804875434677e-05 - slight negative correlation\n",
    "\n",
    "year has weight of -4.082322600251764e-05 - slight negative correlation\n",
    "\n",
    "month has weight of -0.0001462436108377041 - slight negative correlation\n",
    "\n",
    "log_shares has weight of -2.3412961033031365e-05 - slight negative correlation\n",
    "\n",
    "log_n_tokens_content has weight of -8.372188423644289e-05 - slight negative correlation\n",
    "\n",
    "log_num_hrefs has weight of -0.00014290478374134885 - slight negative correlation\n",
    "\n",
    "log_num_self_hrefs has weight of -0.00011891585306411656 - slight negative correlation\n",
    "\n",
    "log_num_imgs has weight of -7.287393465560399e-05 - slight negative correlation\n",
    "\n",
    "log_num_videos has weight of 0.0017717641650130123 - slight positive correlation\n",
    "\n",
    "log_kw_max_min has weight of -0.0015840527506587598 - slight negative correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0651b4a",
   "metadata": {},
   "source": [
    "### SVM with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf2176d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url_name has weight of 0.0\n",
      "date has weight of 0.0\n",
      "timedelta has weight of 0.0\n",
      "n_tokens_title has weight of 0.0\n",
      "n_unique_tokens has weight of 0.0\n",
      "average_token_length has weight of 0.0\n",
      "num_keywords has weight of 0.0\n",
      "kw_min_min has weight of 0.0\n",
      "kw_avg_min has weight of 0.0\n",
      "kw_max_max has weight of 0.0\n",
      "kw_avg_max has weight of 0.0\n",
      "kw_min_avg has weight of 0.0\n",
      "kw_max_avg has weight of 0.0\n",
      "is_weekend has weight of 0.0\n",
      "LDA_00 has weight of 0.0\n",
      "LDA_01 has weight of 0.0\n",
      "LDA_02 has weight of 0.0\n",
      "LDA_03 has weight of 0.0\n",
      "LDA_04 has weight of 0.0\n",
      "global_subjectivity has weight of 0.0\n",
      "global_sentiment_polarity has weight of 0.0\n",
      "global_rate_positive_words has weight of 0.0\n",
      "global_rate_negative_words has weight of 0.0\n",
      "rate_positive_words has weight of 0.0\n",
      "rate_negative_words has weight of 0.0\n",
      "avg_positive_polarity has weight of 0.0\n",
      "min_positive_polarity has weight of 0.0\n",
      "max_positive_polarity has weight of 0.0\n",
      "avg_negative_polarity has weight of 0.0\n",
      "min_negative_polarity has weight of 0.0\n",
      "max_negative_polarity has weight of 0.0\n",
      "title_subjectivity has weight of 0.0\n",
      "title_sentiment_polarity has weight of 0.0\n",
      "abs_title_subjectivity has weight of -454.5110527055157\n",
      "abs_title_sentiment_polarity has weight of 0.0\n",
      "shares has weight of 0.0\n",
      "day_of_week has weight of 0.0\n",
      "news_category has weight of 0.0\n",
      "year has weight of 0.0\n",
      "month has weight of 0.0\n",
      "log_shares has weight of 0.0\n",
      "log_n_tokens_content has weight of 0.0\n",
      "log_num_hrefs has weight of 0.0\n",
      "log_num_self_hrefs has weight of 0.0\n",
      "log_num_imgs has weight of 0.0\n",
      "log_num_videos has weight of 0.0\n",
      "log_kw_max_min has weight of 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get the weights from the trained model.\n",
    "weights = support_vector_machine_model_sgd.coef_.T\n",
    "\n",
    "# Print the weight of each variable.\n",
    "for weight, variable_name in zip(weights, df.columns[:-1]):\n",
    "    print(f'{variable_name} has weight of {weight[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c594e9",
   "metadata": {},
   "source": [
    "#### Interpret Feature Importance - Weighted Coefficients Explanation:\n",
    "Just as in the previous logistic regression model, the SVM weighted coefficients indicate that absolute subjectivity level stands out with a negative correlation with the response variable. This means that the more articles with higher subjective title rating are less likely to be popular. The weight for absolute subjectivity level is much larger than the other weighted coefficients which is another indication of how strong the correlation with the response variable is compared to that of other features. All other coeficients were pushed to a value of zero even with the low alpha of 0.0001 showing that this model did not give any weight to those coefficients.\n",
    "\n",
    "#### Weights: \n",
    "abs_title_subjectivity has weight of -454.5110527055157"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c489a8",
   "metadata": {},
   "source": [
    "### Logistic Regression with SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14f3846e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url_name has weight of 0.0\n",
      "date has weight of -0.02079030554402063\n",
      "timedelta has weight of 0.0\n",
      "n_tokens_title has weight of 0.0\n",
      "n_unique_tokens has weight of 0.0\n",
      "average_token_length has weight of 0.040770063005095454\n",
      "num_keywords has weight of 0.0\n",
      "kw_min_min has weight of 0.0\n",
      "kw_avg_min has weight of 0.0\n",
      "kw_max_max has weight of 0.0\n",
      "kw_avg_max has weight of 0.0\n",
      "kw_min_avg has weight of 0.0\n",
      "kw_max_avg has weight of 0.0\n",
      "is_weekend has weight of 0.0\n",
      "LDA_00 has weight of 0.0\n",
      "LDA_01 has weight of 0.0\n",
      "LDA_02 has weight of 0.0\n",
      "LDA_03 has weight of 0.0\n",
      "LDA_04 has weight of 0.0\n",
      "global_subjectivity has weight of 0.039893656132094266\n",
      "global_sentiment_polarity has weight of 0.0\n",
      "global_rate_positive_words has weight of 0.0\n",
      "global_rate_negative_words has weight of 0.0\n",
      "rate_positive_words has weight of 0.0\n",
      "rate_negative_words has weight of 0.0\n",
      "avg_positive_polarity has weight of 0.0\n",
      "min_positive_polarity has weight of 0.0\n",
      "max_positive_polarity has weight of 0.0\n",
      "avg_negative_polarity has weight of 0.0\n",
      "min_negative_polarity has weight of 0.0\n",
      "max_negative_polarity has weight of 0.0\n",
      "title_subjectivity has weight of -0.010592046044262716\n",
      "title_sentiment_polarity has weight of 0.0\n",
      "abs_title_subjectivity has weight of -457.6761187676259\n",
      "abs_title_sentiment_polarity has weight of 0.0\n",
      "shares has weight of 0.0\n",
      "day_of_week has weight of 0.0\n",
      "news_category has weight of 0.0\n",
      "year has weight of 0.0\n",
      "month has weight of 0.03356599350658822\n",
      "log_shares has weight of 0.0\n",
      "log_n_tokens_content has weight of 0.0\n",
      "log_num_hrefs has weight of 0.0\n",
      "log_num_self_hrefs has weight of 0.0\n",
      "log_num_imgs has weight of -0.08036933031863915\n",
      "log_num_videos has weight of 0.0\n",
      "log_kw_max_min has weight of 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get the weights from the trained model.\n",
    "weights = logistic_regression_log_sgd.coef_.T\n",
    "\n",
    "# Print the weight of each variable.\n",
    "for weight, variable_name in zip(weights, df.columns[:-1]):\n",
    "    print(f'{variable_name} has weight of {weight[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed05cb7",
   "metadata": {},
   "source": [
    "#### Interpret Feature Importance - Weighted Coefficients Explanation:\n",
    "Just as in the previous logistic regression model, the SVM weighted coefficients indicate that there are number of features with slight either negative or positive correlations with the response variable, however absolute subjectivity level stands out with a negative correlation with the response variable. This means that the more articles with higher subjective title rating are less likely to be popular. The weight for absolute subjectivity level is much larger than the other weighted coefficients which is another indication of how strong the correlation with the response variable is compared to that of other features.\n",
    "\n",
    "#### Weights: \n",
    "date has weight of -0.02079030554402063 - slight negative correlation\n",
    "\n",
    "average_token_length has weight of 0.040770063005095454 - slight positive correlation\n",
    "\n",
    "global_subjectivity has weight of 0.039893656132094266 - slight positive correlation\n",
    "\n",
    "title_subjectivity has weight of -0.010592046044262716 - slight negative correlation\n",
    "\n",
    "abs_title_subjectivity has weight of -457.6761187676259 - significant negative correlation\n",
    "\n",
    "month has weight of 0.03356599350658822 - slight positive correlation\n",
    "\n",
    "log_num_imgs has weight of -0.08036933031863915 - slight negative correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11145f6c",
   "metadata": {},
   "source": [
    "# Interpret Support Vectors\n",
    "## Look at the chosen support vectors for the classification task. Do these provide any insight into the data? Explain. If you used stochastic gradient descent (and therefore did not explicitly solve for support vectors), try subsampling your data to train the SVC model then analyze the support vectors from the subsampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0a6668f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "# Set the model parameters\n",
    "C = 1.0  # Regularization parameter\n",
    "kernel = 'linear'  # Use a linear kernel for SVM\n",
    "# You can adjust other SVM-specific parameters such as gamma, degree, etc., if needed.\n",
    "# Initialize the SVM model.\n",
    "support_vector_machine_model_svc = SVC(C=C, kernel=kernel)\n",
    "\n",
    "# Split the training data into a training subset and a validation subset.\n",
    "X_train_subset, X_val_subset, y_train_subset, y_val_subset = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
    "\n",
    "# Train the support vector machine model on the training subset.\n",
    "support_vector_machine_model_svc.fit(X_train_subset, y_train_subset)\n",
    "\n",
    "# Make predictions on the validation subset.\n",
    "y_val_pred = support_vector_machine_model_svc.predict(X_val_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1e88176",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM (SVC) accuracy: 0.987892546348846\n",
      "SVM (SVC) precision: 0.9520786847783501\n",
      "SVM (SVC) recall: 0.9636874071308031\n",
      "SVM (SVC) F1 score: 0.9572471107774686\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation subset.\n",
    "val_accuracy = accuracy_score(y_val_subset, y_val_pred)\n",
    "\n",
    "# If the model performs well on the validation subset, make predictions on the test data.\n",
    "if val_accuracy > 0.8:\n",
    "    y_pred = support_vector_machine_model_svc.predict(X_test)\n",
    "\n",
    "# Make predictions on the test data.\n",
    "y_pred = support_vector_machine_model_svc.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy, precision, and recall of the model.\n",
    "accuracy_svm_svc_subset = accuracy_score(y_test, y_pred)\n",
    "precision_svm_svc_subset = precision_score(y_test, y_pred, average='macro')\n",
    "recall_svm_svc_subset = recall_score(y_test, y_pred, average='macro')\n",
    "F1Score_svm_svc_subset = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# Print the coefficients and metrics.\n",
    "print('SVM (SVC) accuracy:', accuracy_svm_svc_subset)\n",
    "print('SVM (SVC) precision:', precision_svm_svc_subset)\n",
    "print('SVM (SVC) recall:', recall_svm_svc_subset)\n",
    "print('SVM (SVC) F1 score:', F1Score_svm_svc_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ef7945",
   "metadata": {},
   "source": [
    "##### After performing the subsampling to train the data on a SVC model the the metrics are slightly less than those of the SVM recorded previously (and again below).\n",
    "\n",
    "Support vector machine accuracy: 0.9904149325261697\n",
    "\n",
    "Support vector machine precision: 0.978781438391208\n",
    "\n",
    "Support vector machine recall: 0.9741562206299955\n",
    "\n",
    "Support vector machine F1 score: 0.9764193565401654\n",
    "\n",
    "##### However, these scores were markedly better than when we ran the SVM model using SGD. The values are listed again below for reference.\n",
    "\n",
    "Support vector machine using SGD accuracy: 0.8508008576113003\n",
    "\n",
    "Support vector machine using SGD precision: 0.5179097283954291\n",
    "\n",
    "Support vector machine using SGD recall: 0.5855183900729812\n",
    "\n",
    "Support vector machine using SGD F1 score: 0.5278048979282167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6c81cfd6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  113   114   121 ... 23564 23605 23649]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3898</th>\n",
       "      <td>23417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3899</th>\n",
       "      <td>23530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3900</th>\n",
       "      <td>23564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3901</th>\n",
       "      <td>23605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3902</th>\n",
       "      <td>23649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3903 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0       113\n",
       "1       114\n",
       "2       121\n",
       "3       158\n",
       "4       172\n",
       "...     ...\n",
       "3898  23417\n",
       "3899  23530\n",
       "3900  23564\n",
       "3901  23605\n",
       "3902  23649\n",
       "\n",
       "[3903 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the support vectors\n",
    "support_vectors = support_vector_machine_model_svc.support_\n",
    "print(support_vectors)\n",
    "\n",
    "sp_df = pd.DataFrame(support_vectors)\n",
    "display(sp_df)\n",
    "# # Get the actual support vectors from the training data\n",
    "# support_vectors_data = X_train_subset[support_vectors]\n",
    "# print(support_vectors_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1deb509",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the indices of the support vectors\n",
    "support_vector_indices = support_vector_machine_model_svc.support_\n",
    "\n",
    "# Create a histogram of the support vector indices\n",
    "plt.hist(support_vector_indices, bins=len(support_vector_indices), edgecolor='k')\n",
    "plt.xlabel('Data Point Index')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Support Vector Indices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a986962",
   "metadata": {},
   "source": [
    "The Support Vectors are disributed quite evenly across the data. It is in a multimodal distribution as there are many peaks that can be seen. There are three outliers with a frequency of 5. The data seems to be well seperated between the clusters of frequency levels with 2 having the largest amount."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
