{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression, SVMs, and Gradient Optimization\n",
    "\n",
    "*Please address questions to Professor Eric Larson, eclarson@smu.edu*\n",
    "\n",
    "In this notebook we will explore methods of using logistic regression in `scikit-learn` and we will also investigate methods for gradient descent. Finally we will look at using support vector machines and investigate parameters of kernel functions. A basic understanding of `scikit-learn` is required to complete this notebook, but we start very basic. Note also that there are more efficient methods of separating testing and training data, but we will leave that for a later lecture. \n",
    "\n",
    "First let's load a dataset and prepare it for analysis. We will use pandas to load in data, and then prepare it for classification. We will be using the titanic dataset (a very modest sized data set of about 1000 instances) before loading a larger, more complicated dataset for gradient descent methods.\n",
    "\n",
    "______\n",
    "The imputation methods used here are discussed in a previous notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jt -t onedork\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 882 entries, 0 to 890\n",
      "Data columns (total 9 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Survived   882 non-null    int64  \n",
      " 1   Age        882 non-null    float64\n",
      " 2   age_range  882 non-null    int64  \n",
      " 3   Sex        882 non-null    object \n",
      " 4   Parch      882 non-null    int64  \n",
      " 5   SibSp      882 non-null    int64  \n",
      " 6   Pclass     882 non-null    int64  \n",
      " 7   Fare       882 non-null    float64\n",
      " 8   Embarked   882 non-null    object \n",
      "dtypes: float64(2), int64(5), object(2)\n",
      "memory usage: 68.9+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/m6ntk1l94mj0xc_5wxrc4dc40000gn/T/ipykernel_37544/2944901244.py:24: FutureWarning: Dropping invalid columns in DataFrameGroupBy.transform is deprecated. In a future version, a TypeError will be raised. Before calling .transform, select only columns which should be valid for the transforming function.\n",
      "  df_imputed = df_grouped.transform(lambda grp: grp.fillna(grp.median()))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/titanic.csv') # read in the csv file\n",
    "\n",
    "# 1. Remove attributes that just arent useful for us\n",
    "del df['PassengerId']\n",
    "del df['Name']\n",
    "del df['Cabin']\n",
    "del df['Ticket']\n",
    "\n",
    "# 2. Impute some missing values, grouped by their Pclass and SibSp numbers\n",
    "#specifies that we want to group the DataFrame based on the unique combinations of values in the 'Pclass' and 'SibSp' columns.\n",
    "df_grouped = df.groupby(by=['Pclass','SibSp'])\n",
    "\n",
    "# # now use this grouping to fill the data set in each group, then transform back\n",
    "# fill in the numeric values\n",
    "# After running this code, df_imputed will be a DataFrame with missing values filled in\n",
    "# for each group based on the median value of that group. This is a common imputation \n",
    "# technique in data analysis, as it replaces missing values with a central tendency\n",
    "# measure specific to each group, reducing potential bias in the imputation process.\n",
    "df_imputed = df_grouped.transform(lambda grp: grp.fillna(grp.median()))\n",
    "# fill in the categorical values\n",
    "df_imputed[['Sex','Embarked']] = df_grouped[['Sex','Embarked']].apply(lambda grp: grp.fillna(grp.mode()))\n",
    "# fillin the grouped variables from original data frame\n",
    "df_imputed[['Pclass','SibSp']] = df[['Pclass','SibSp']]\n",
    "\n",
    "# 3. Computed discrete features agains now with the newest values\n",
    "df_imputed['age_range'] = pd.cut(df_imputed.Age,[0,16,24,65,1e6],4,labels=[0,1,2,3]) # this creates a new variable\n",
    "# 4. drop rows that still had missing values after grouped imputation\n",
    "df_imputed.dropna(inplace=True)\n",
    "df_imputed.age_range = df_imputed.age_range.astype(int)\n",
    "# 5. Rearrange the columns\n",
    "df_imputed = df_imputed[['Survived','Age','age_range','Sex','Parch','SibSp','Pclass','Fare','Embarked']]\n",
    "\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________\n",
    "Now let's look a little further at each of the categorical objects. Note that age range has already been saved as an ordinal. We need to look at `Sex` and `Embarked` objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>882</td>\n",
       "      <td>2</td>\n",
       "      <td>male</td>\n",
       "      <td>573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>882</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         count unique   top freq\n",
       "Sex        882      2  male  573\n",
       "Embarked   882      3     S  637"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: calling describe when not all the data is categorical will cause the \n",
    "# categorical variables to be removed\n",
    "df_imputed[['Sex','Embarked']].describe().transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the `Sex`  attribute binary, there is no need to encode it using OneHotEncoding. We can just convert it to an integer. However, we should transform the `Embarked` attribute to take on three different values--one for each possible variable outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 882 entries, 0 to 890\n",
      "Data columns (total 13 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Survived    882 non-null    int64  \n",
      " 1   Age         882 non-null    float64\n",
      " 2   age_range   882 non-null    int64  \n",
      " 3   Sex         882 non-null    object \n",
      " 4   Parch       882 non-null    int64  \n",
      " 5   SibSp       882 non-null    int64  \n",
      " 6   Pclass      882 non-null    int64  \n",
      " 7   Fare        882 non-null    float64\n",
      " 8   Embarked    882 non-null    object \n",
      " 9   Embarked_C  882 non-null    uint8  \n",
      " 10  Embarked_Q  882 non-null    uint8  \n",
      " 11  Embarked_S  882 non-null    uint8  \n",
      " 12  IsMale      882 non-null    int64  \n",
      "dtypes: float64(2), int64(6), object(2), uint8(3)\n",
      "memory usage: 78.4+ KB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/m6ntk1l94mj0xc_5wxrc4dc40000gn/T/ipykernel_37544/695271333.py:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  df_imputed.IsMale = df_imputed.IsMale.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "# perform one-hot encoding of the categorical data \"embarked\"\n",
    "tmp_df = pd.get_dummies(df_imputed.Embarked,prefix='Embarked')\n",
    "df_imputed = pd.concat((df_imputed,tmp_df),axis=1) # add back into the dataframe\n",
    "\n",
    "# replace the current Sex atribute with something slightly more intuitive and readable\n",
    "df_imputed['IsMale'] = df_imputed.Sex=='male' \n",
    "df_imputed.IsMale = df_imputed.IsMale.astype(np.int)\n",
    "\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 882 entries, 0 to 890\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Survived    882 non-null    int64  \n",
      " 1   Age         882 non-null    float64\n",
      " 2   age_range   882 non-null    int64  \n",
      " 3   Parch       882 non-null    int64  \n",
      " 4   SibSp       882 non-null    int64  \n",
      " 5   Pclass      882 non-null    int64  \n",
      " 6   Fare        882 non-null    float64\n",
      " 7   Embarked_C  882 non-null    uint8  \n",
      " 8   Embarked_Q  882 non-null    uint8  \n",
      " 9   Embarked_S  882 non-null    uint8  \n",
      " 10  IsMale      882 non-null    int64  \n",
      "dtypes: float64(2), int64(6), uint8(3)\n",
      "memory usage: 64.6 KB\n"
     ]
    }
   ],
   "source": [
    "# Now let's clean up the dataset\n",
    "if 'Sex' in df_imputed:\n",
    "    del df_imputed['Sex'] # if 'Sex' column still exists, delete it (as we created an ismale column)\n",
    "    \n",
    "if 'Embarked' in df_imputed:    \n",
    "    del df_imputed['Embarked'] # get reid of the original category as it is now one-hot encoded\n",
    "    \n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 882 entries, 0 to 890\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Survived    882 non-null    int64  \n",
      " 1   Age         882 non-null    float64\n",
      " 2   age_range   882 non-null    int64  \n",
      " 3   Parch       882 non-null    int64  \n",
      " 4   SibSp       882 non-null    int64  \n",
      " 5   Pclass      882 non-null    int64  \n",
      " 6   Fare        882 non-null    float64\n",
      " 7   Embarked_C  882 non-null    uint8  \n",
      " 8   Embarked_Q  882 non-null    uint8  \n",
      " 9   Embarked_S  882 non-null    uint8  \n",
      " 10  IsMale      882 non-null    int64  \n",
      " 11  FamilySize  882 non-null    int64  \n",
      "dtypes: float64(2), int64(7), uint8(3)\n",
      "memory usage: 71.5 KB\n"
     ]
    }
   ],
   "source": [
    "# Finally, let's create a new variable based on the number of family members\n",
    "# traveling with the passenger\n",
    "\n",
    "# notice that this new column did not exist before this line of code--we use the pandas \n",
    "#    syntax to add it in \n",
    "# this line of code calculates the total family size for each passenger by adding the number \n",
    "# of parents/children ('Parch') and the number of siblings/spouses ('SibSp') they have. \n",
    "df_imputed['FamilySize'] = df_imputed.Parch + df_imputed.SibSp\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Training and Testing Split\n",
    "For training and testing purposes, let's gather the data we have and grab 80% of the instances for training and the remaining 20% for testing. Moreover, let's repeat this process of separating the testing and training data three times. We will use the hold out cross validation method built into `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShuffleSplit(n_splits=3, random_state=None, test_size=0.2, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "# we want to predict the X and y data as follows:\n",
    "if 'Survived' in df_imputed:\n",
    "    y = df_imputed['Survived'].values # get the labels we want, our target labels that we want ot predict \n",
    "    del df_imputed['Survived'] # get rid of the class label\n",
    "#     this line converts the remaining data in the df_imputed DataFrame into a NumPy array and \n",
    "#     assigns it to the variable X. These are the features or input variables that will be used\n",
    "#     to make predictions.\n",
    "    X = df_imputed.values # use everything else to predict!\n",
    "\n",
    "    ## X and y are now numpy matrices, by calling 'values' on the pandas data frames we\n",
    "    #    have converted them into simple matrices to use with scikit learn\n",
    "    \n",
    "    \n",
    "# to use the cross validation object in scikit learn, we need to grab an instance\n",
    "#    of the object and set it up. This object will be able to split our data into \n",
    "#    training and testing splits\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "# Here, it's set to 0.2, which means that 20% of the data will be used for testing, \n",
    "# and the remaining 80% will be used for training in each iteration.\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,\n",
    "                         test_size  = 0.2)\n",
    "\n",
    "# This line prints the details of the cv_object, which will show you the settings and \n",
    "# configuration of the cross-validation object, including the number of splits and the test size.\n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Logistic Regression\n",
    "Now let's use Logistic Regression from `scikit-learn`. The documentation can be found here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.7966101694915254\n",
      "confusion matrix\n",
      " [[97 18]\n",
      " [18 44]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.807909604519774\n",
      "confusion matrix\n",
      " [[94 22]\n",
      " [12 49]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.7909604519774012\n",
      "confusion matrix\n",
      " [[83 16]\n",
      " [21 57]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression and vary some parameters\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "\n",
    "# first we create a reusable logisitic regression object\n",
    "#   here we can setup the object with different learning parameters and constants\n",
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None, solver='liblinear' ) # get object\n",
    "\n",
    "# now we can use the cv_object that we setup before to iterate through the \n",
    "#    different training and testing sets. Each time we will reuse the logisitic regression \n",
    "#    object, but it gets trained on different data each time we use it.\n",
    "\n",
    "iter_num=0\n",
    "# the indices are the rows used for training and testing in each iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1\n",
    "    \n",
    "# Also note that every time you run the above code\n",
    "#   it randomly creates a new training and testing set, \n",
    "#   so accuracy will be different each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.8022598870056498\n",
      "confusion matrix\n",
      " [[91 14]\n",
      " [21 51]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.8135593220338984\n",
      "confusion matrix\n",
      " [[103  13]\n",
      " [ 20  41]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.8361581920903954\n",
      "confusion matrix\n",
      " [[95 15]\n",
      " [14 53]]\n"
     ]
    }
   ],
   "source": [
    "# this does the exact same thing as the above block of code, but with shorter syntax\n",
    "\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    lr_clf.fit(X[train_indices],y[train_indices])  # train object\n",
    "    y_hat = lr_clf.predict(X[test_indices]) # get test set precitions\n",
    "\n",
    "    # print the accuracy and confusion matrix \n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", mt.accuracy_score(y[test_indices],y_hat)) \n",
    "    print(\"confusion matrix\\n\",mt.confusion_matrix(y[test_indices],y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80225989 0.82485876 0.76836158]\n"
     ]
    }
   ],
   "source": [
    "# and here is an even shorter way of getting the accuracies for each training and test set\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(lr_clf, X, y=y, cv=cv_object) # this also can help with parallelism\n",
    "print(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897902acf45e4b6399134fc0f02ddbe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=2.451, description='cost', max=5.0, min=0.001, step=0.05), Output()), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.lr_explor(cost)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we can change some of the parameters interactively\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def lr_explor(cost):\n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight=None,solver='liblinear') # get object\n",
    "    accuracies = cross_val_score(lr_clf,X,y=y,cv=cv_object) # this also can help with parallelism\n",
    "    print(accuracies)\n",
    "\n",
    "wd.interact(lr_explor,cost=(0.001,5.0,0.05),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretting weights\n",
    "Okay, so now lets take the last trained model for logistic regression and try to interpret the weights for the model. Is there something about the weights that makes this model more interpretable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age has weight of -0.033236723069031665\n",
      "age_range has weight of 0.04765644741724133\n",
      "Parch has weight of -0.0013474056469710187\n",
      "SibSp has weight of -0.08827120710500887\n",
      "Pclass has weight of -0.8329475390528553\n",
      "Fare has weight of 0.003996039093635039\n",
      "Embarked_C has weight of 1.1416001305841397\n",
      "Embarked_Q has weight of 0.8130781150277734\n",
      "Embarked_S has weight of 0.9104282502835718\n",
      "IsMale has weight of -2.4066998941003535\n",
      "FamilySize has weight of -0.08961861275199738\n"
     ]
    }
   ],
   "source": [
    "# interpret the weights\n",
    "\n",
    "# iterate over the coefficients\n",
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = df_imputed.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])\n",
    "    \n",
    "# does this look correct? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These weight interpretations **are not neccessarily interpretable** because of the values we had. Very large attribute values could just as easily be assigned a higher weight. Instead, let's normalize the feature values so that all the attributes are on the same dynamic range. Once we normalize the attributes, the weights should have magnitudes that reflect their poredictive power in the logistic regression model.\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.7966101694915254\n",
      "[[83 16]\n",
      " [20 58]]\n",
      "IsMale has weight of -1.0510462217394771\n",
      "Pclass has weight of -0.6117498098204632\n",
      "Age has weight of -0.30751799174781047\n",
      "SibSp has weight of -0.16407320842166065\n",
      "FamilySize has weight of -0.09593768067462637\n",
      "age_range has weight of -0.09521135327813347\n",
      "Embarked_S has weight of -0.07575788825815184\n",
      "Embarked_Q has weight of -0.05168792345068599\n",
      "Parch has weight of 0.019096655256103788\n",
      "Fare has weight of 0.11096774456741514\n",
      "Embarked_C has weight of 0.12129515556408124\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "# to Make things easier, let's start by just using whatever was last stored in the variables:\n",
    "##    X_train , y_train , X_test, y_test (they were set in a for loop above)\n",
    "\n",
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train) # find scalings for each column that make this zero mean and unit std\n",
    "# the line of code above only looks at training data to get mean and std and we can use it \n",
    "# to transform new feature data\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05, solver='liblinear') # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )\n",
    "\n",
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,df_imputed.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars)\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEtCAYAAAALNduYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAqeklEQVR4nO3df1wUdf4H8NeyiMTPFlA4UONETFTEFM0fGYmb9zjrCqtTz8sfWd51dsdF6Skaep6RpIkdZVlmaHfled7l5pU9qlXDBFNMi8TUvMS+iAjLLiDiisvu9w8eu8e6IMrsZwd3Xs/Ho8ejnZnd98yyvuYzM5/5jMpms9lARERez0fuFSAiIs9g4BMRKQQDn4hIIRj4REQKwcAnIlIIBj4RkUL4yr0CHamoqOjU+yIiImAwGNy8Nl23rpy1uc3eX1fO2tzmGxMdHd3uPLbwiYgUgoFPRKQQDHwiIoVg4BMRKQQDn4hIIRj4REQKwcAnIlIIBj4RkUJ0+RuviJSmee4D7c4738F71Rt2uHdlyKuwhU9EpBBs4RO14VqtbIAtbbo5sYVPRKQQDHwiIoVg4BMRKQQDn4hIIRj4REQKwcAnIlIIBj4RkUIw8ImIFIKBT0SkEAx8IiKFYOATESkEA5+ISCEY+ERECsHAJyJSCA6PfBORMmQvh+slIrcE/tdff438/HxYrVZMmDABaWlpTvNtNhvy8/Nx5MgRdO/eHfPmzUPfvn3dUZqIiK6T5FM6VqsVGzduxOLFi7F27VoUFhaivLzcaZkjR46gsrISeXl5+M1vfoO33npLalkiIrpBkgP/1KlTiIqKQmRkJHx9fTFmzBgUFxc7LXPo0CHcfffdUKlU6N+/Py5evAiTySS1NBER3QDJp3SMRiPCw8Mdr8PDw/H999+7LBMREeG0jNFohEajcfk8vV4PvV4PAMjJyXF6X2vnJ4+55npd63x25Paia763I9eq3dGj7yTV7uC9vr6+sFgsnf/8dkj5rgFp2yzb31mm77qj2iLrKvHflNK2WXLg22w2l2kqleqGl7HTarXQarWO1waDQeIauhLxmV2hdkREhKzb1h651skbv2v+jbtO7a66zdHR0e3Ok3xKJzw8HDU1NY7XNTU1Li338PBwpxVsaxkiIhJLcuDHxcXh3LlzqKqqgsViQVFREZKTk52WSU5Oxt69e2Gz2XDy5EkEBAQw8ImIPEzyKR21Wo05c+YgOzsbVqsV48ePR+/evfHpp58CACZOnIg77rgDhw8fRnp6Ovz8/DBv3jzJK05ERDfGLf3whw0bhmHDhjlNmzhxouP/VSoVnnjiCXeUIiKiTuLQCkRECsHAJyJSCI6lQ0QAOh5vqat2CaXrxxY+EZFCMPCJiBSCgU9EpBA8h09EsrvW9QNeO3AftvCJiBSCgU9EpBAMfCIiheA5/E7g+UYikkqOHGELn4hIIRj4REQKwcAnIlIIBj4RkUIw8ImIFIKBT0SkEAx8IiKFYOATESkEA5+ISCEY+ERECsGhFahDfPQdkXdgC5+ISCEY+ERECiHplE5DQwPWrl2L6upq9OjRAxkZGQgKCnJaxmAwYN26daitrYVKpYJWq8WkSZMkrTQREd04SYGv0+mQmJiItLQ06HQ66HQ6PProo07LqNVqzJgxA3379sWlS5ewaNEiDBkyBL169ZK04kREdGMkndIpLi5GSkoKACAlJQXFxcUuy2g0GvTt2xcAcMsttyAmJgZGo1FKWSIi6gRJLfy6ujpoNBoALcFeX19/zeWrqqpw+vRp9OvXr91l9Ho99Ho9ACAnJwcRERFtLne+k+sMoN3PdAdfX1+hn98Va4usy79z16grZ23+vtz4uR0tsGLFCtTW1rpMnzZt2g0VMpvNWLNmDWbPno2AgIB2l9NqtdBqtY7XIrr7iexCKGcXRblqd9Vumd74d+bvq+voqr+v6Ojodud1GPhZWVntzgsNDYXJZIJGo4HJZEJISEiby1ksFqxZswbjxo3DnXfeeR2rTERE7ibpHH5ycjIKCgoAAAUFBRgxYoTLMjabDevXr0dMTAzuv/9+KeWIiEgCSYGflpaGkpISpKeno6SkBGlpaQAAo9GIlStXAgBOnDiBvXv34ujRo1iwYAEWLFiAw4cPS15xIiK6MZIu2gYHB2Pp0qUu08PCwpCZmQkAGDBgAP75z39KKUNERG7AO22JiBSCgU9EpBAMfCIihWDgExEpBAOfiEghGPhERArBwCciUggGPhGRQjDwiYgUgoFPRKQQDHwiIoVg4BMRKQQDn4hIISSNlikn9YYd15zfVZ+SQ0QkF7bwiYgUgoFPRKQQDHwiIoVg4BMRKQQDn4hIIRj4REQKwcAnIlIIBj4RkUIw8ImIFIKBT0SkEAx8IiKFkDSWTkNDA9auXYvq6mr06NEDGRkZCAoKanNZq9WKRYsWISwsDIsWLZJSloiIOkFSC1+n0yExMRF5eXlITEyETqdrd9mdO3ciJiZGSjkiIpJAUuAXFxcjJSUFAJCSkoLi4uI2l6upqcHhw4cxYcIEKeWIiEgCSad06urqoNFoAAAajQb19fVtLrdp0yY8+uijuHTpUoefqdfrodfrAQA5OTmIiIjo1Lr5+vp2+r1SyFVXztoi656X8F6R34U3ftddtTZ/X2783I4WWLFiBWpra12mT5s27boKfPXVVwgNDUXfvn1RWlra4fJarRZardbxurNj2ss1Hr6c4/ArcZuvReQ6KfG7VuI2X0tX/X1FR0e3O6/DwM/Kymp3XmhoKEwmEzQaDUwmE0JCQlyWOXHiBA4dOoQjR46gqakJly5dQl5eHtLT069z9YmIyB0kndJJTk5GQUEB0tLSUFBQgBEjRrgsM336dEyfPh0AUFpaiv/85z8MeyIiGUi6aJuWloaSkhKkp6ejpKQEaWlpAACj0YiVK1e6Y/2IiMhNJLXwg4ODsXTpUpfpYWFhyMzMdJk+aNAgDBo0SEpJIiLqJN5pS0SkEAx8IiKFYOATESkEA5+ISCEY+ERECsHAJyJSCAY+EZFCMPCJiBSCgU9EpBAMfCIihWDgExEpBAOfiEghGPhERArBwCciUggGPhGRQjDwiYgUgoFPRKQQDHwiIoVg4BMRKQQDn4hIIRj4REQKwcAnIlIIBj4RkUIw8ImIFIKBT0SkEL5S3tzQ0IC1a9eiuroaPXr0QEZGBoKCglyWu3jxItavX4//+7//g0qlwu9+9zv0799fSmkiIrpBkgJfp9MhMTERaWlp0Ol00Ol0ePTRR12Wy8/Px9ChQ/Hss8/CYrHg8uXLUsoSEVEnSDqlU1xcjJSUFABASkoKiouLXZZpbGzEd999h9TUVACAr68vAgMDpZQlIqJOkNTCr6urg0ajAQBoNBrU19e7LFNVVYWQkBC89tprOHPmDPr27YvZs2fD39+/zc/U6/XQ6/UAgJycHERERHRq3Xx9fTv9XinkqitnbZF1z0t4r8jvwhu/665am78vN35uRwusWLECtbW1LtOnTZt2XQWam5tx+vRpzJkzB/Hx8cjPz4dOp2v3/VqtFlqt1vHaYDBcV52rRUREdPq9UshVV87acm7ztYhcJyV+10rc5mvpqr+v6Ojodud1GPhZWVntzgsNDYXJZIJGo4HJZEJISIjLMuHh4QgPD0d8fDwAYNSoUdDpdNex2kSAesOOa87vqmFA1BVJOoefnJyMgoICAEBBQQFGjBjhssytt96K8PBwVFRUAAC+/fZb9OrVS0pZIiLqBEmBn5aWhpKSEqSnp6OkpARpaWkAAKPRiJUrVzqWmzNnDvLy8jB//nyUlZVh8uTJklaaiIhunKSLtsHBwVi6dKnL9LCwMGRmZjpex8bGIicnR0opIiKSiHfaEhEpBAOfiEghGPhERArBwCciUggGPhGRQjDwiYgUgoFPRKQQDHwiIoVg4BMRKQQDn4hIIRj4REQKwcAnIlIIBj4RkUIw8ImIFIKBT0SkEAx8IiKFYOATESkEA5+ISCEY+ERECsHAJyJSCAY+EZFCMPCJiBSCgU9EpBAMfCIihfCV8uaGhgasXbsW1dXV6NGjBzIyMhAUFOSy3Icffojdu3dDpVKhd+/emDdvHvz8/KSUJiKiGySpha/T6ZCYmIi8vDwkJiZCp9O5LGM0GvHxxx8jJycHa9asgdVqRVFRkZSyRETUCZICv7i4GCkpKQCAlJQUFBcXt7mc1WpFU1MTmpub0dTUBI1GI6UsERF1gqRTOnV1dY7w1mg0qK+vd1kmLCwMv/jFL/C73/0Ofn5+SEpKQlJSkpSyRETUCR0G/ooVK1BbW+syfdq0addVoKGhAcXFxVi3bh0CAgKQm5uLvXv34u67725zeb1eD71eDwDIyclBRETEddW5mq+vb6ffK4VcdeWszW32/rpy1hZZ97yE94r8LkRtc4eBn5WV1e680NBQmEwmaDQamEwmhISEuCzz7bffomfPno55d955J06ePNlu4Gu1Wmi1Wsdrg8HQ4Ua0JSIiotPvlUKuunLW5jZ7f105a8u5zdcicp2kbHN0dHS78ySdw09OTkZBQQEAoKCgACNGjHBZJiIiAt9//z0uX74Mm82Gb7/9FjExMVLKEhFRJ0gK/LS0NJSUlCA9PR0lJSVIS0sD0NIzZ+XKlQCA+Ph4jBo1CgsXLsT8+fNhs9mcWvBEROQZki7aBgcHY+nSpS7Tw8LCkJmZ6Xg9ZcoUTJkyRUopIiKSiHfaEhEpBAOfiEghGPhERArBwCciUggGPhGRQjDwiYgUgoFPRKQQDHwiIoVg4BMRKQQDn4hIIRj4REQKwcAnIlIIBj4RkUIw8ImIFIKBT0SkEAx8IiKFYOATESkEA5+ISCEY+ERECsHAJyJSCAY+EZFCMPCJiBSCgU9EpBAMfCIihWDgExEphK+UN+/fvx/btm3D2bNn8cILLyAuLq7N5b7++mvk5+fDarViwoQJSEtLk1KWiIg6QVILv3fv3pg/fz4SEhLaXcZqtWLjxo1YvHgx1q5di8LCQpSXl0spS0REnSCphd+rV68Olzl16hSioqIQGRkJABgzZgyKi4uv671EROQ+kgL/ehiNRoSHhzteh4eH4/vvv293eb1eD71eDwDIyclBREREp+r6+vp2+r1SyFVXztrcZu+vK2dtkXXPS3ivyO9C1DZ3GPgrVqxAbW2ty/Rp06ZhxIgRHRaw2Wwu01QqVbvLa7VaaLVax2uDwdBhjbZERER0+r1SyFVXztrcZu+vK2dtObf5WkSuk5Rtjo6Obndeh4GflZXVqaJ24eHhqKmpcbyuqamBRqOR9JlERHTjhHfLjIuLw7lz51BVVQWLxYKioiIkJyeLLktERFeRFPgHDx7Ek08+iZMnTyInJwfZ2dkAWs7br1y5EgCgVqsxZ84cZGdnIyMjA6NHj0bv3r2lrzkREd0QSRdtR44ciZEjR7pMDwsLQ2ZmpuP1sGHDMGzYMCmliIhIIt5pS0SkEAx8IiKFYOATESmE8BuviIi6KvWGHdec31XvAegstvCJiBSCgU9EpBAMfCIihWDgExEpBAOfiEghGPhERArBwCciUggGPhGRQjDwiYgUQmVr65FURETkdby2hb9o0SJF1ZWzNrfZ++vKWZvb7D5eG/hEROSMgU9EpBBeG/harVZRdeWszW32/rpy1uY2uw8v2hIRKYTXtvCJiMgZA5+ISCEY+ERECsHAd4OmpiZUVFTIvRpE5AXMZrOwz/aaZ9rW1tZiy5YtMJlMWLx4McrLy3Hy5EmkpqYKrXvo0CH87W9/g8Viwbp161BWVoatW7di4cKFQuvaXblyBQcOHEBVVRWsVqtj+iOPPCK8dm1tLU6dOgUA6NevH2699VbhNQGgsrIS4eHh6NatG0pLS3HmzBmkpKQgMDBQeO3jx4/j3LlzGD9+POrr62E2m9GzZ09h9b7++muYzWaMGjXKafoXX3yB0NBQDBkyRFhtACgpKUF5eTkAIC4uDrfffrvQegCg1+sxaNAg/OQnP4HNZsPrr7+OAwcOoEePHpg3bx769u0rfB2qq6tx7tw5DBkyBE1NTWhubsYtt9witOaJEyewfv16mM1mvP766ygrK4Ner8cTTzzhthpe08J/7bXXkJSUBJPJBAD4yU9+go8++kh43W3btmHlypWOsImNjUV1dbXwunarVq1CcXEx1Go1unfv7vhPtF27diEzMxMHDhzAl19+iSVLlmD37t3C6wLAmjVr4OPjg8rKSqxfvx5VVVXIy8sTXnfbtm3Q6XTQ6XQAAIvFgldeeUV4zYEDB7pMT0xMxNatW4XVNRgMWLhwIf7973+juroaVVVVeO+995CdnY0rV65g165dwmp//PHH6NGjBwCgsLAQZ86cwauvvoqZM2di06ZNwura6fV65ObmYsOGDQCAmpoarF69WnjdzZs3Y8mSJQgODgbQkiXfffedW2t4TQv/woULGDNmjOMfo1qtho+P+P2ZWq1GQECA8DrtMRqNWLJkicfr7tixA6tWrXL8OC9cuIDnnntO+BEVAPj4+ECtVuPgwYOYNGkSfv7zn+NPf/qT8LoHDx7EqlWrHEdvYWFhuHTpktCaly9fRkhIiMv0W2+9FZcvXxZWd+PGjfj5z3+Oe+65x2l6QUEBnnvuOQDAhAkThNT28fGBr29LNH311VdISUlBcHAwhgwZgnfffVdIzdY++eQTrFy5EosXLwbQ0nisq6sTXhcAIiIinF67O8O8poXfvXt3XLhwASqVCgBw8uRJjwRx7969sW/fPlitVpw7dw5vv/02+vfvL7yuXf/+/fHjjz96rJ5deHi40yHuLbfc4vJjFUWtVmPfvn0oKCjA8OHDAQDNzc3C6/r6+kKlUjl+YyLPtdpduXKlzW2zWCxoamoSVreiosIl7AEgJSUFdXV1jjAUwcfHByaTCU1NTTh69CgSExMd80Rus123bt0cOxyg5bdl/5uLFB4ejhMnTkClUsFisWDHjh2IiYlxaw2vaeHPnDkTq1atQmVlJbKyslBfX49nnnlGeN05c+bg/fffR7du3fDXv/4VSUlJePjhh4XXffbZZ6FSqdDc3IzPP/8cPXv2RLdu3WCz2aBSqfDSSy8JrR8WFobFixcjOTkZKpUKhw4dQlxcHD788EMAwP333y+s9rx58/Dpp59i8uTJ6NmzJ6qqqjBu3Dhh9exGjx6NN998ExcvXoRer8eePXuEtXLtRo4ciTfeeANz5syBv78/gJYdTX5+PkaOHCmsbuvrQVdP9/PzQ2hoqLDaU6ZMwaJFi2C1WjF8+HD07t0bAHDs2DGh10vsBg4ciPfffx9NTU0oKSnBJ5984mhYiDR37lxs2rQJRqMRTz75JIYMGYLHH3/crTW86k7b5uZmVFRUwGazITo62mkv7W06uk5gPwcqyrZt2645/5e//KXQ+nYNDQ2oqanBbbfdJrSOzWZDTU0NKioq8M0338Bms2Ho0KHCL5o2NzfjH//4B3bv3u04gjIYDEhNTcXUqVOF/cY3bdoEs9mM2bNnO+1oNm/eDD8/Pzz22GNC6to1Nzfj0qVLCAoKckyzH1HZ16ekpETI92+1WrF7926UlJTAZrMhKSkJEyZMEN7KNxgMLkfJtbW1bu0M4TWBf+DAAZdpAQEB6NOnj9DWSE5OjssPISAgAHFxcdBqtfDz8xNWG2g5ddW7d2/H6ZVLly6hvLwc8fHxQuu21tDQgMDAQI8c9gLAn//8Z/zpT3+C1WrFggULEBISgoEDB2LWrFlC6y5cuBAvvvii0BrtaWpqQmVlJQAgKirK5Xfl7vCzWCx47733UFBQgIiICKhUKlRXVyMlJQXTp0/vEo0pOf8eIkybNg2jRo3CvHnzHH9fd2+j/H81N9m9ezdOnjyJQYMGAWg5/IuPj8e5c+fwyCOP4O677xZSNzIyEvX19Rg7diwAoKioCKGhoaioqMAbb7yBP/zhD0Lq2r311ltOP4ju3bu7THOnf/3rXxg9ejRiYmJw5coVvPDCCygrK4NarUZ6errwFi8ANDY2IiAgALt27cL48eMxZcoUzJ8/X3jd+Ph4nDp1Cv369RNe62p+fn7o06dPu/Pfffddt373vr6+mDlzJqZNm4bKykrYbDZERUW59AAT1cq+Hu5uq9pPk7ZH9GnSPn36ICEhAVlZWcjIyEBUVJTbt9FrAl+lUmHt2rWOw5/a2lq89dZbeOGFF7Bs2TJhgV9WVobly5c7XicnJ2PZsmVYvny5R64h2M/Z2/n4+Ai9gFlUVOS4RlFQUACbzYaNGzeioqIC69at88g//ubmZphMJuzfvx/Tpk0TXs+utLQUer0ePXr0QPfu3T12veR6iDpQ9/SO5ka4+4hSzgetAC3b87Of/Qy33XYbXnzxRfz61792+zZ6TeBXV1c7nesKDQ3FuXPnEBQUBLVaLaxufX2907k3g8GA+vp6APDIYW9kZCR27tyJiRMnAgA+/fRToRe27D1VgJabgsaOHQsfHx/06tWr3Qt97vbII48gOzsbAwYMQL9+/XD+/HlERUUJryuyZ4pUnjqddjUvOSMMQPx1r47Yv8sBAwZg6dKlePnll3H27Fm31vCawE9ISEBOTo7jjsQDBw4gISEBZrNZ6B2YM2bMQFZWluPwq6qqCk888QTMZjNSUlKE1bWbO3cu8vPz8f7770OlUmHw4MH47W9/K6xet27d8OOPP+LWW29FaWkpZs6c6Zgnsl94a6NHj8bo0aMdryMjIz1ySsceCHV1dbhy5YrwejcDuXY0gLiAPnnyJPLz81FeXg6LxQKr1Qp/f39s3rxZSD27zMxMx/9rNBosXboUJ06ccGsNrwn8xx9/HAcOHMDx48cBtNzqbzKZ4O/vj2XLlgmrO2zYMOTl5Tn2xNHR0Y4LLvfdd5+wukBLb4LNmzfj6aefFlqntdmzZyM3Nxf19fW47777HEcThw8fRmxsrEfWoampCbt370Z5eblTv+x58+YJrXvo0CG88847MJlMCAkJgcFgQExMDHJzc4XWvR5yt07dqa0OGK3deeedACBsJ//222/j6aefRm5uLnJyclBQUOC4YC7C3r17cffdd6OwsLDN+W3dad1ZXhP4KpUKUVFROHXqFPbv34+ePXs6fhii/fDDD6iurkZzczPOnDkDAB5p3fv4+ODChQuwWCwe6zURHx+Pl19+2WX6sGHDMGzYMI+sw6uvvoro6Gh88803ePjhh7Fv3z6336DSlq1btyI7OxsrVqzAqlWrcPTo0Xb/kbqL3OHXERE7mq+++gpAy5FU644YpaWlGDRokEf+XUdFRcFqtcLHxwfjx4933F0sgv3IWPRd24AXBH5FRQWKiopQWFiIoKAgjBkzBjabTWirvrVXXnkF58+fR2xsrNNt0J4IfKDlH1xWVhaGDx/u6J8MiL3xCWgZSmHbtm2OQ84BAwbgkUcecQy1IFJlZSWeeeYZHDp0CPfccw/uuusuZGdnC6+rVqsRHBwMm80Gq9WKwYMHC7/VX67wk3NHYz9Sy8nJQW5uLjQaDQDAZDJh48aNbq93te7du8NisSA2NhZ///vfhQ9jce+99wLwzL0rN33gZ2RkYMCAAVi4cKHjwp0nBk2z++GHH5CbmyvbuUyNRgONRgObzeaRFoLdyy+/jISEBDz77LMAWkZvfPnll5GVlSW8tv0ifGBgoON6gicGrAsMDITZbEZCQgLy8vIQGhoqtEMAIF/4dYVWdnV1tWN7gf91xBDt97//PaxWK+bMmYOPPvoINTU1jt+5CNcaHfSpp57CT3/6U7fVuukD/9lnn0VhYSGWL1+OpKQkjB071qM9B3r37o3a2lqnH6YneeqO1qs1NDQ4DcH88MMPo7i42CO1tVotGhoaMHXqVKxatQpmsxlTpkwRVs/eC2vBggXw8/PDrFmz8MUXX6CxsdEjw1ADng8/uVvZQMu56+zsbKd7XOw7HpHsp6n8/Pw88u/r448/doxb1Hp00NOnTyM/Px9/+ctf3Fbrpg/8kSNHYuTIkTCbzSguLsZHH32Euro6bNiwASNHjkRSUpLQ+hcuXMAzzzyDfv36OZ1H99R4+PX19fjggw9cLmCKPqU1aNAgFBYWOnrLfPnllx47h28fv2bgwIF49dVXhddbvXo1XnzxRfj7++Oll17C/Pnz2xxYTCS5wk+uVjbQ0hHj4MGDOHbsGICWHb3I8YM6Oj0l6n4LT44OetMHvp2/vz/GjRuHcePGoaGhAfv374dOpxMe+HK1sO3y8vIwZswYHD58GHPnzsXnn3/e5nC67jJz5kyoVCrYbDZ89NFHjsC1d10T2dK2D8zWHlHXLVofMVZVVQmp0RFPh5+dXDsau5/+9Kfw9/fHkCFDcPnyZVy6dEnYg0jsI6HeddddGD58uPBhUezso4MGBgbi6NGjeOihhxzz3D06qNcEfmtBQUG49957HRdDRHJnl6nOuHDhAlJTU7Fz504MHDgQAwcOFNq6f+edd4R9dkc8eY2itdbXZ+Tsd+7J8LOTa0cDtJzb3rVrFxoaGvDKK6/AaDRiw4YNWLp0qZB6q1evxtmzZ1FYWIi8vDzExMTgrrvuQlJSktBrNZ4cHdQrA9+T5LpJw85+KKjRaHD48GFoNBoYjUZh9c6ePYuYmBj88MMPbc4X+fg5uY6mysrKMGvWLNhsNjQ1NTkGabMPreCJv7Wnw681OXY0gDwPIomJicGUKVMwZcoUFBUVYd26dXjwwQfxwAMPCKs5fPhwvPbaay6jg/bt2xcZGRlurcXAl6itmzQ8dY4TAB566CE0NjZixowZyM/PR2Njo9BRIz/88EP89re/xd/+9rc253uiO+yrr76Kxx57zHEHdUNDA9555x1hN16JfJTg9ZLrKUxy7mjkeBCJ0WhEYWEhDh48iMDAQMyaNcsjRzRqtRrPP/88xo8fj7FjxyIoKMipm7W7MPDdwJM3adg1NTXhs88+Q2VlJYxGI1JTUz0SthMmTEBtba2j1ueff+7oQiby/H1rP/74o9NwGUFBQSgrK/NIbbnI9RQmOR/35+kHkSxbtgxmsxmjR4/GU0895WhtWywWNDQ0OLW+RXj66aexZ88eZGZmIi4uDvfccw+SkpLc+ndm4Evk6Zs07NatWwe1Wo2EhAQcOXIE5eXlwh9KAQAbNmxw9LU/duwYtmzZgsceewxlZWV44403hPZXtrPZbE7/ABsaGjzyiEM5yfUUJrl2NAAwffp07N69G3369MFnn32GO+64A1qtVlg9g8EAAPjss8+g1+sB/O+CvUqlEt4jLCoqCr/61a8wdepUHD58GK+//rqjETlp0iS37HAY+BJ5+iYNu/LycqxZswYAkJqa6rGRHK1Wq+OHV1RUhAkTJmDUqFEYNWoUFixY4JF1uP/++/Hcc89h1KhRUKlU2L9/v1PPBm/k6fCzk2tHA7Q8VW3q1KmO7bRarcjLy0N6erqQeuvWrRPyuTfizJkz2LNnD44cOYI777wT48aNw/Hjx7F8+XKsXr1a8ucz8CWwWq3YsmUL0tPTPXaThl3rVpfouz1bs1qtaG5uhlqtxtGjR/Gb3/zGaZ4npKSkIC4uDkePHoXNZsP8+fPRq1cvj9SWi6fDz06uHQ3Q0uLevn07Jk+eDIvFgtzcXI8M0Hf8+HHExsbC398fe/fuxenTp3Hfffe5PH7Q3RYuXIjAwECkpqbi17/+Nbp16wagZfwqd42aycCXQI7By+zsPUcAOPUeEd1zZOzYsfjzn/+M4OBg+Pn5ISEhAUDL+DYBAQFCatq1vm7Rp08f3HvvvR7d2clJrvCTa0cDtNztm5eXh+3bt6O0tBRDhw4VPkYU0PIUudWrV6OsrAw7duxAamoqXnnlFacHHYnwzDPPIDIyss157hqziIEvkVyDl8nVc+Shhx7C4MGDUVtbiyFDhjjO51qtVuHXEK6+bnH27FnMnj1baM2uQq7wk2NH07rL76RJk/Dmm2/i9ttvx8CBA/HDDz8I7foLtBwxq1QqHDp0CJMmTUJqaioKCgqE1fPkDYUMfInkGrxMTv3793eZFh0dLbyuXNct5CR3+Mmxo7m6y29QUBDOnj3rmC66N5q/vz+2b9+OL774AsuXL4fVaoXFYhFWz5O5obJ50zPKuqC3334bc+bMkXs1vMLChQudHs5+9Wtv1NFpBFHh13pH09zc7NjRpKamAhB7gx3QcsT45ZdfYsyYMULrtKW2thb79u1DXFwcEhISYDAYUFpa6rEhz0Vi4AumhFDylKlTpzpOm9mvW7R+mLin7m72NDnCT64dzdU1RJ837wo++OADPPjgg3j77bfbnO/OBiNP6dBNoyvc8SoHHx8ffPLJJx4N/GXLlsnaygaAxMRE7NixA2PGjHG6PibqBij7wIBXE92gsD+tTfRRE8DAJ7opeDr8AHl2NK3t2bMHQMvdvnYib4CSa2DA5ORkAPDIkNsMfMF4xozcwdPhZyfHjsauK9wI5Un//e9/8f7778NgMDjdOe7Ocfh5Dt9NzGZzm4Mdff755x5/WAaRuzz11FMu0zyxo7H78ccfUV5ejitXrjimecPF07b88Y9/xIwZM9CnTx+nU0vufFA8W/gSnThxAuvXr4fZbMbrr7+OsrIy6PV6PPHEEwA8c5hGyiBH+MnZyt62bRuOHTuG8vJy3HHHHThy5AgGDBjgtYEfEhLiOL0jCgNfos2bN2PJkiVYtWoVACA2NhbfffedzGtF3kbO8JOrlf3ll19i9erVWLhwIebNm4fa2lqsX79eeF25TJkyBevXr8fgwYMdwyoAcOsD4xn4bnD1GBs+Pj4yrQl5K7nCT84djZ+fH3x8fODj44PGxkaEhobK9ohJT9izZw8qKipgsVicMoSB34WEh4fjxIkTUKlUsFgs2Llzp6ObFZG7yBV+cray4+LicPHiRUyYMAGLFi2Cv78/+vXr55Hacjhz5ozjTnJRGPgSzZ07F5s2bYLRaMSTTz6JIUOG4PHHH5d7tcjLyBV+cray7dfBJk6ciKFDh+LSpUu47bbbPFJbDvHx8SgvLxc68it76RDdZKqqqjwWfm+99RZ+9atfobCwEB9++CH8/f0RGxsr7HGSVztw4ACOHz8OlUqFAQMGeOwB6nLIyMhAZWUlevbsiW7dujlu+GK3zC6krduhAwICEBcXhxEjRsiwRuSt5A4/T+5ogJadTWVlJcaOHQug5YE7kZGRjpa/t6murm5zOrtldiFXrlxBRUUFRo0aBaDlH2WvXr2we/dulJaWKmb4XhLr6vD77LPPUFJS4pHwu3pH46nAP3bsGNasWePok56SkuK2ceG7Inuw19XVOfWIcicGvkSVlZVYunSp40EcEydOxPPPP4+srCyPPOqQlEGu8JNzRxMdHQ2DweAIwpqaGvTp00d4XbkcOnQI77zzDkwmE0JCQmAwGBATE4Pc3Fy31WDgS2Q0GnH58mXH054uX74Mk8kEHx8fp760RFLIFX5y7GhycnKgUqnQ2NiIjIwM9OvXDyqVCt9//z1uv/12obXltHXrVmRnZ2PFihVYtWoVjh49isLCQrfWYOBL9OCDD2LBggUYNGgQbDYbvvvuO0yePBlmsxmJiYlyrx7d5OQOPzl2NA888IDQz++q1Go1goODYbPZYLVaMXjwYLz77rturcHAlyg1NRVDhw7F3r17ERMTg6SkJISFhcHf3x8zZsyQe/XoJidX+Mm5oxk4cKDT68bGRlitVqE1u4LAwECYzWYkJCQgLy8PoaGhbn9mM3vpSLRr1y7s3LkTRqMRsbGxOHnyJPr37++RB0SQ8lwdfqJGrTx27Ng1518dyiLo9Xps3boVfn5+UKlUjm6Knhq4zVMMBgMiIiJgNpvh5+cHm82GL774Ao2NjRg3bhyCg4PdVostfIl27tyJlStXYsmSJVi2bBnOnj2Lf/7zn3KvFnkZT4dfV2hl79ixA2vWrEFISIhH63ra6tWr8eKLL8Lf3x8vvfQS5s+fL2zQRQa+RH5+fvDz8wPQ0kUzJiYGFRUVMq8VeRu5wk/OVnZkZCS6d+8uvI7cWp9kEX0XMwNforCwMFy8eBEjRozA888/j8DAQISFhcm9WuRl5Ao/OVvZ06dPx3PPPYf4+Hj4+v4vqtz5jNeuoPXY9209YtGttXgO332OHTuGxsZGDB061OkHSiTV6dOn8dprr3k8/LKzszF//nxZdjaZmZkYMGCAywNBvO0ZE1OnToW/vz9sNhuampoc37WIZ+kyldzIExeySJnefPNNDB482CX8RJOzla1WqzFr1izhdeS2detWj9Vi4BPdBOQKP7l2NAAwaNAg6PV6DB8+3OkmRk88T9dbMfCJbgJyhZ+crex9+/YBALZv3+6Y5o3dMj2J5/CJbgJyPUx8y5Yt6NGjB1vZXoKBT0TtkmNH88EHH+DBBx8EAOzfvx+jR492zHvvvfcwffp0YbW9HU/pEHVhcoffunXrhH5+W4qKihzbrNPpnLb5m2++YeBLwKdtE3VhRUVFjv/X6XRO87755hthdT/44APH/+/fv99p3nvvvSesLuB8I9LVJyB4QkIaBj5RFyZX+Mm1owGufSOSp3sKeRue0iHqwuQKPzlb2WVlZZg1a5bjRiR7LyGbzSbsSVBKwcAn6sLkCj85W9mevBFJadhLh4hcXOt2/ytXrmDLli0yryF1BgOfiEgheNGWiEghGPhERArBwCciUggGPhGRQvw/ZzTmomhARMAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now let's make a pandas Series with the names and values, and plot them\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "weights = pd.Series(lr_clf.coef_[0],index=df_imputed.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For more improvement and guarding against overfitting:** At this point it would make sense to remove variables that are highly related to one another or ones that are irrelevant and keep going with the weights analysis. What variables would you remove?\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# we want to normalize the features based upon the mean and standard deviation of each column. \n",
    "# However, we do not want to accidentally use the testing data to find out the mean and std (this would be snooping)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# you can apply the StandardScaler function inside of the cross-validation loop \n",
    "#  but this requires the use of PipeLines in scikit. \n",
    "#  A pipeline can apply feature pre-processing and data fitting in one compact notation\n",
    "#  Here is an example!\n",
    "\n",
    "std_scl = StandardScaler()\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05, solver='liblinear') \n",
    "\n",
    "# create the pipline\n",
    "piped_object = Pipeline([('scale', std_scl),  # do this\n",
    "                         ('logit_model', lr_clf)]) # and then do this\n",
    "\n",
    "weights = []\n",
    "# run the pipline cross validated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(X,y)):\n",
    "    piped_object.fit(X[train_indices],y[train_indices])  # train object\n",
    "    # it is a little odd getting trained objects from a  pipeline:\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "\n",
    "weights = np.array(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b9/m6ntk1l94mj0xc_5wxrc4dc40000gn/T/ipykernel_37544/3449738070.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_notebook_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# run at the start of every notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m error_y=dict(\n\u001b[1;32m      5\u001b[0m             \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import plotly\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': df_imputed.columns,\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For more improvement and guarding against overfitting:** At this point it would make sense to remove variables that are highly related to one another or ones that are irrelevant and keep going with the weights analysis. What variables would you remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = df_imputed[['Age','Pclass','IsMale']].values\n",
    "\n",
    "weights = []\n",
    "# run the pipline corssvalidated\n",
    "for iter_num, (train_indices, test_indices) in enumerate(cv_object.split(Xnew,y)):\n",
    "    piped_object.fit(Xnew[train_indices],y[train_indices])  # train object\n",
    "    weights.append(piped_object.named_steps['logit_model'].coef_[0])\n",
    "    \n",
    "weights = np.array(weights)\n",
    "\n",
    "error_y=dict(\n",
    "            type='data',\n",
    "            array=np.std(weights,axis=0),\n",
    "            visible=True\n",
    "        )\n",
    "\n",
    "graph1 = {'x': ['Age','Pclass','IsMale'],\n",
    "          'y': np.mean(weights,axis=0),\n",
    "    'error_y':error_y,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "We can use the previous training and testing attributes (scaled) to investigate the weights and support vectors in the attributes. SVMs were first hypothesized by Vladmir Vapnik ~50 years ago, but did not gain popularity until the turn of the millenium. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# Here he is, in all his glory:\n",
    "Image(url='http://engineering.columbia.edu/files/engineering/vapnik.jpg')\n",
    "# Image(url='http://yann.lecun.com/ex/images/allyourbayes.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# okay, so run through the cross validation loop and set the training and testing variable for one single iteration\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets investigate SVMs on the data and play with the parameters and kernels\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# train the model just as before\n",
    "svm_clf = SVC(C=0.5, kernel='rbf', degree=3, gamma='auto') # get object\n",
    "svm_clf.fit(X_train_scaled, y_train)  # train object\n",
    "\n",
    "y_hat = svm_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the support vectors\n",
    "print(svm_clf.support_vectors_.shape)\n",
    "print(svm_clf.support_.shape)\n",
    "print(svm_clf.n_support_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using linear kernel, these make sense to look at (not otherwise, why?)\n",
    "print(svm_clf.coef_)\n",
    "weights = pd.Series(svm_clf.coef_[0],index=df_imputed.columns)\n",
    "weights.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do some different analysis with the SVM and look at the instances that were chosen as support vectors\n",
    "\n",
    "# now lets look at the support for the vectors and see if we they are indicative of anything\n",
    "# grabe the rows that were selected as support vectors (these are usually instances that are hard to classify)\n",
    "\n",
    "# make a dataframe of the training data\n",
    "df_tested_on = df_imputed.iloc[train_indices].copy() # saved from above, the indices chosen for training\n",
    "# now get the support vectors from the trained model\n",
    "df_support = df_tested_on.iloc[svm_clf.support_,:].copy()\n",
    "\n",
    "df_support['Survived'] = y[svm_clf.support_] # add back in the 'Survived' Column to the pandas dataframe\n",
    "df_imputed['Survived'] = y # also add it back in for the original data\n",
    "df_support.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets see the statistics of these attributes\n",
    "from pandas.plotting import boxplot\n",
    "\n",
    "# group the original data and the support vectors\n",
    "df_grouped_support = df_support.groupby(['Survived'])\n",
    "df_grouped = df_imputed.groupby(['Survived'])\n",
    "\n",
    "# plot KDE of Different variables\n",
    "vars_to_plot = ['Age','Pclass','IsMale','FamilySize']\n",
    "\n",
    "for v in vars_to_plot:\n",
    "    plt.figure(figsize=(10,4))\n",
    "    # plot support vector stats\n",
    "    plt.subplot(1,2,1)\n",
    "    ax = df_grouped_support[v].plot.kde() \n",
    "    plt.legend(['Perished','Survived'])\n",
    "    plt.title(v+' (Instances chosen as Support Vectors)')\n",
    "    \n",
    "    # plot original distributions\n",
    "    plt.subplot(1,2,2)\n",
    "    ax = df_grouped[v].plot.kde() \n",
    "    plt.legend(['Perished','Survived'])\n",
    "    plt.title(v+' (Original)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the analysis here is basically telling us what the original statistics of the data looked like, and also what the statistics of the support vectors looked like. We can see that the separation in distributions is not as great as the separation for the original data. This is because the support vectors tend to be instances on the edge of the class boundaries and also instances that are classified incorrectly in the training data. \n",
    "\n",
    "You can also look at joint plots of the data and see how relationships have changed. (**Hint hint for the min-lab assignment**--this would be a nice analysis of the support vectors.)\n",
    "\n",
    "That's mostly it for using these things! They are really nice analysis tools and provide human interpretable summaries of the data. \n",
    "___\n",
    "\n",
    "# Gradient Based Alternatives\n",
    "So now let's go and find out how we can use these when our data size gets bigger. Like a lot bigger. We will use a kaggle dataset that attempts to classify plankton. We will use some example code to get us started from the tutorial here:\n",
    "http://www.kaggle.com/c/datasciencebowl/details/tutorial \n",
    "\n",
    "You can download from Kaggle (login required): https://www.kaggle.com/c/datasciencebowl/data\n",
    "\n",
    "UPDATE: This problem was also solved using deep learning! Check out the blog here:\n",
    "http://benanne.github.io/2015/03/17/plankton.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(url='https://storage.googleapis.com/kaggle-competitions/kaggle/3978/media/Plankton-Diagram3-lg.png')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load another dataset (large) and train using various methods of gradient (and mini-batch)\n",
    "from __future__ import print_function\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "# change this to point to the dataset on your machine/cluster!!\n",
    "# For my students: please email me for a link to the data if you cannot get it from Kaggle\n",
    "directory_of_dataset = \"D:/SMU/Larson/2019 Data Mining 7331 Updates/Class Github/kaggle_plank/\"\n",
    "\n",
    "# get the classnames from the training data directory structure\n",
    "directory_names = list(set(glob.glob(os.path.join(directory_of_dataset,\"train\", \"*\"))\n",
    " ).difference(set(glob.glob(os.path.join(directory_of_dataset,\"train\",\"*.*\")))))\n",
    "\n",
    "print('number of classes:', len(directory_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using this code requires the use of skimage to process the images (you will need to install via pip)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Rescale the images and create the combined metrics and training labels\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imread\n",
    "\n",
    "#get the total training images\n",
    "numberofImages = 0\n",
    "for folder in directory_names:\n",
    "    for fileNameDir in os.walk(folder):   \n",
    "        for fileName in fileNameDir[2]:\n",
    "             # Only read in the images\n",
    "            if fileName[-4:] != \".jpg\":\n",
    "              continue\n",
    "            numberofImages += 1\n",
    "\n",
    "# We'll rescale the images to be 40x40\n",
    "maxPixel = 25\n",
    "imageSize = maxPixel * maxPixel\n",
    "num_rows = numberofImages # one row for each image in the training dataset\n",
    "num_features = imageSize # for our ratio\n",
    "\n",
    "# X is the feature vector with one row of features per image\n",
    "# consisting of the pixel values and our metric\n",
    "X = np.zeros((num_rows, num_features), dtype=float)\n",
    "# y is the numeric class label \n",
    "y = np.zeros((num_rows))\n",
    "\n",
    "files = []\n",
    "# Generate training data\n",
    "i = 0    \n",
    "label = 0\n",
    "# List of string of class names\n",
    "namesClasses = list()\n",
    "\n",
    "print(\"Reading images\")\n",
    "# Navigate through the list of directories\n",
    "for folder in directory_names:\n",
    "    # Append the string class name for each class\n",
    "    currentClass = folder.split(os.pathsep)[-1]\n",
    "    namesClasses.append(currentClass)\n",
    "    for fileNameDir in os.walk(folder):   \n",
    "        for fileName in fileNameDir[2]:\n",
    "            # Only read in the images\n",
    "            if fileName[-4:] != \".jpg\":\n",
    "              continue\n",
    "            \n",
    "            # Read in the images and create the features\n",
    "            nameFileImage = \"{0}{1}{2}\".format(fileNameDir[0], os.sep, fileName)            \n",
    "            image = imread(nameFileImage, as_gray=True)\n",
    "            files.append(nameFileImage)\n",
    "            #axisratio = getMinorMajorRatio(image)\n",
    "            image = resize(image, (maxPixel, maxPixel))\n",
    "            \n",
    "            # Store the rescaled image pixels and the axis ratio\n",
    "            X[i, 0:imageSize] = np.reshape(image, (1, imageSize))\n",
    "            #X[i, imageSize] = axisratio\n",
    "            \n",
    "            # Store the classlabel\n",
    "            y[i] = label\n",
    "            i += 1\n",
    "            # report progress for each 5% done  \n",
    "            report = [int((j+1)*num_rows/20.) for j in range(20)]\n",
    "            if i in report: print(np.ceil(i *100.0 / num_rows), \"% done\")\n",
    "    label += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is where the online tutorial code stops and my code starts\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now divide the data into test and train using scikit learn built-ins\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "\n",
    "cv = StratifiedShuffleSplit( n_splits=1,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# use some compact notation for creating a linear SVM classifier with stichastic descent\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "regularize_const = 0.1\n",
    "iterations = 5\n",
    "svm_sgd = SGDClassifier(alpha=regularize_const,\n",
    "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "        loss='hinge', n_iter_no_change=iterations, n_jobs=-1, penalty='l2')\n",
    "\n",
    "scl = StandardScaler()\n",
    "for train_idx, test_idx in cv.split(X,y):\n",
    "    svm_sgd.fit(scl.fit_transform(X[train_idx]),y[train_idx])\n",
    "    yhat = svm_sgd.predict(scl.transform(X[test_idx]))\n",
    "    \n",
    "    conf = mt.confusion_matrix(y[test_idx],yhat)\n",
    "    acc = mt.accuracy_score(y[test_idx],yhat)\n",
    "\n",
    "print('SVM:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# use some compact notation for creating a logistic regression classifier with stochastic descent\n",
    "log_sgd = SGDClassifier(alpha=regularize_const,\n",
    "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "        loss='log', n_iter_no_change=iterations, n_jobs=-1, penalty='l2')\n",
    "\n",
    "scl = StandardScaler()\n",
    "for train_idx, test_idx in cv.split(X,y):\n",
    "    log_sgd.fit(scl.fit_transform(X[train_idx]),y[train_idx])\n",
    "    yhat = log_sgd.predict(scl.transform(X[test_idx]))\n",
    "    \n",
    "    conf = mt.confusion_matrix(y[test_idx],yhat)\n",
    "    acc = mt.accuracy_score(y[test_idx],yhat)\n",
    "\n",
    "print('Logistic Regression:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# now lets use some of what we know from this class to reduce the dimensionality of the set\n",
    "from sklearn.decomposition import PCA\n",
    "n_components = 50\n",
    "\n",
    "pca = PCA(n_components=n_components, svd_solver='randomized')\n",
    "\n",
    "iterations = 150\n",
    "log_sgd = SGDClassifier(\n",
    "        fit_intercept=True, l1_ratio=0.0, learning_rate='optimal',\n",
    "        loss='log', n_iter=iterations, n_jobs=-1, penalty='l2')\n",
    "\n",
    "# you could also set this up in a pipeline\n",
    "for train_idx, test_idx in cv.split(X,y):\n",
    "    log_sgd.fit(pca.fit_transform(X[train_idx]),y[train_idx])\n",
    "    yhat = log_sgd.predict(pca.transform(X[test_idx]))\n",
    "    \n",
    "    conf = mt.confusion_matrix(y[test_idx],yhat)\n",
    "    acc = mt.accuracy_score(y[test_idx],yhat)\n",
    "\n",
    "print('Logistic Regression:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(8,8))\n",
    "plt.imshow(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition does not use \"accuracy\" as the evaluation of the best model; they use the log loss:\n",
    "$$logloss=-\\frac{1}{N}\\sum_{i=1}^m\\sum_{j=1}^C {\\bf 1}_{ij}\\ln(p_{ij})$$\n",
    "\n",
    "Where there are $m$ instances (images) in the dataset, and $C$ is the number of classes. The equation ${\\bf 1}_{ij}$ is an indicator function that ensures we only add log probabilities when the class is correct. That is, it is zero if the predicted class for the $i^{th}$ instance is not equal to $j$ and it is one when the class of the $i^{th}$ instance == $j$. To prevent extremities in the log function they also replace probabilities, $p$, with $p=\\max(\\min(p,1-10^{-15}),10^{-15})$\n",
    "\n",
    "Would this be easy to code in python? `scikit-learn` has an implementation for log loss, but it is not exactly what the competition uses and is only defined for binary classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the tutorial: http://www.kaggle.com/c/datasciencebowl/details/tutorial \n",
    "def multiclass_log_loss(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    https://www.kaggle.com/wiki/MultiClassLogLoss\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape = [n_samples]\n",
    "            true class, integers in [0, n_classes - 1)\n",
    "    y_pred : array, shape = [n_samples, n_classes]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "    \"\"\"\n",
    "    predictions = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "    # normalize row sums to 1\n",
    "    predictions /= predictions.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    actual = np.zeros(y_pred.shape)\n",
    "    n_samples = actual.shape[0]\n",
    "    actual[np.arange(n_samples), y_true.astype(int)] = 1\n",
    "    vectsum = np.sum(actual * np.log(predictions))\n",
    "    loss = -1.0 / n_samples * vectsum\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## How do you think you might increase the accuracy of the classifier(s)?\n",
    "- Search through parameters for models?\n",
    "- Try different classifiers?\n",
    "- Add more features (through better image processing)?\n",
    "\n",
    "___\n",
    "## How do you think we can make the algorithms more efficient for training/testing?\n",
    "- What about mini-batch training? \n",
    "- Sampling?\n",
    "- Map/Reduce (what are advantages/disadvantages)?\n",
    "- Buy a ton of memory on AWS virtual machines?\n",
    "\n",
    "**Note:** For mini-batch calculations (they are not really needed here because the dataset fits in memory) they can be accessed for a number of different classifiers (including SGDClassifier) by managing the sub-samples we send it, $X_{sub}$, and calling the function `partial_fit`."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "state": {
    "4c41474cfeee4f77959ec7a366881a57": {
     "views": [
      {
       "cell_index": 14
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
